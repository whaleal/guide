{"./":{"url":"./","title":"Whaleal","keywords":"","body":"Whaleal Jinmu Information is a professional IT data consulting and service provider, dedicated to offering users high-quality information products, consulting, and services. Founded in 2015 in Shanghai, Jinmu Information has established branch offices in Beijing, Shenzhen, and Guangzhou. Jinmu Information is a core partner for MongoDB in the North Asia region, as well as a core partner for Akamai, Zendesk, Vonage, and Splunk in the China region. This guide primarily introduces the self-developed products by the Whaleal team and provides documentation for their deployment and usage. Whaleal Community "},"whalelaPlatform/":{"url":"whalelaPlatform/","title":"Whaleal Platform","keywords":"","body":"Whaleal Platform Introduction Whaleal Platform (WAP) is an intelligent operations and hosting platform designed for continuous monitoring and management, operating 24/7. WAP is an open-source MongoDB database supervision and control solution, providing functionalities for troubleshooting and diagnosis of MongoDB. Architecture Introduction The architecture diagram illustrates the overall structure of the system, showcasing the relationships between various modules. The division of functional modules involves breaking down a complex system into smaller, single-functional modules. Through analysis of system functionality, specific functional modules for the data sharing and exchange system are summarized. WAP Features WAP allows highly customizable deployment of MongoDB nodes based on business, data, and security requirements. WAP offers global control over each MongoDB node, ensuring precise awareness. WAP ensures communication encryption to guarantee data security and privacy. WAP audits operational logs to provide historical traceability, effectively reducing troubleshooting time. WAP associates time-grouped logs to establish clear dependencies between operations. WAP links Host monitoring with MongoDB monitoring, enabling correlated investigation and in-depth analysis. WAP supports real-time diagnosis of MongoDB, Slowest Operation analysis, Explain functionality, and more. WAP incorporates years of experience in MongoDB usage and operations from our company. WAP Roadmap WAP will add an automatic inspection feature. WAP will introduce an intelligent diagnosis module. WAP will implement a data archiving module. WAP will include a real-time data migration module. WAP will gradually integrate with AWS, GCP, Azure, Alibaba Cloud, Huawei Cloud, Tencent Cloud, and more. WAP will progressively support other mainstream OS distributions. WAP will further optimize performance. WAP will address and fix bugs. "},"whalelaPlatform/00-Overview/01-Introduction.html":{"url":"whalelaPlatform/00-Overview/01-Introduction.html","title":"Introduction","keywords":"","body":"Whaleal Platform Introduction Whaleal Platform (WAP) is an intelligent operations and management platform that provides 24/7 continuous monitoring and management. WAP is an open-source solution for supervising and controlling MongoDB databases. It facilitates troubleshooting and diagnostics for MongoDB operations. Architecture Introduction The architecture diagram illustrates the overall structure of the system, depicting the relationships between various modules. The division of functional modules involves decomposing a complex system into multiple modules, each responsible for a specific function. Based on an analysis of the system's functionality, specific functional modules are defined for the data sharing and exchange system. WAP Features WAP enables highly customizable deployment of MongoDB nodes based on business, data, and security requirements. WAP provides global oversight of each MongoDB node, ensuring accurate awareness. WAP ensures secure communication with encryption, safeguarding data privacy. WAP audits operation logs, ensuring well-documented historical records and expediting troubleshooting. WAP associates time-grouped logs, clarifying the relationships and dependencies between operations. WAP links host monitoring with MongoDB monitoring, enabling comprehensive and informed investigation. WAP supports real-time diagnosis of MongoDB, Slowest Operation analysis, and Explain functionality. WAP benefits from years of experience in MongoDB usage and operations by the company. WAP Roadmap Adding automatic inspection functionality to WAP. Incorporating an intelligent diagnostics module into WAP. Integrating a data archiving module into WAP. Including a real-time data migration module in WAP. Expanding WAP's compatibility to include AWS, GCP, Azure, Alibaba Cloud, Huawei Cloud, Tencent Cloud, and more. Extending WAP's support to other mainstream OS distributions. Further optimizing the performance of WAP. Addressing and fixing bugs in WAP. "},"whalelaPlatform/00-Overview/02-Comparison.html":{"url":"whalelaPlatform/00-Overview/02-Comparison.html","title":"Comparison","keywords":"","body":"Popular Solutions MongoDB Ops Manager Server Safely, securely, and seamlessly manage MongoDB in your own environment. Available through the MongoDB Enterprise Advanced subscription, Ops Manager eliminates operational overhead by automating key administration tasks such as deployment, upgrades, and more. Monitoring Monitor, visualize, and alert on 100+ performance metrics Backup Capture continuous, incremental backups, with point-in-time recovery Automation Perform single-click installations, upgrades, and index maintenance, with zero downtime Query Optimization Seamlessly identify and address slow-running queries with the Visual Query Profiler, index suggestions, and automated index roll-outs Zabbix Zabbix is an open source monitoring software tool for diverse IT components, including networks, servers, virtual machines (VMs) and cloud services. Zabbix provides monitoring metrics, such as network utilization, CPU load and disk space consumption. collect from any source flexible metric collection agent/agent-less monitoring custom collection method Percona Monitoring and Management Percona Monitoring and Management Percona Monitoring and Management (PMM) is an open source database monitoring, management, and observability solution for MySQL, PostgreSQL, and MongoDB. It allows you to observe the health of your database systems, explore new patterns in their behavior, troubleshoot them and perform database management operations no matter where they are located - on-prem or in the cloud. PMM collects thousands of out-of-the-box performance metrics from databases and their hosts. The PMM web UI visualizes data in dashboards. Additional features include advisors for database health assessments. Homogeneous Comparison Based on the information provided, a comparison between similar solutions can be made. Ops Manager Zabbix PMM WAP Change Management ✓ × × ✓ Monitoring & Alerts Detailed General Detailed Detailed Backup & Recovery ✓ × Other solutions Other solutions Licensing Enterprise Edition Open Source Open Source Open Source Advantages Official tool, comprehensive platform Widely used enterprise monitoring platform with easy integration Open source MySQL monitoring platform, integrated with MongoDB Years of troubleshooting experience, tailored for Chinese users Disadvantages Requires good understanding of MongoDB, high learning curve; Metrics not as detailed, limited diagnostics; Overwhelming number of dashboards, complex navigation; "},"whalelaPlatform/01-Intstall/00-requirement.html":{"url":"whalelaPlatform/01-Intstall/00-requirement.html","title":"Requirement","keywords":"","body":"Pre-Flight Check Before installing Whaleal Platform (WAP), you need to review the following materials: Server Requirement Agent Requirement Server Requirement Hardware Requirement All hosts that install the following Whaleal Platform (WAP) components must meet RAM and Disk requirements: Whaleal Platform Application Whaleal Platform Application Databases Whaleal Platform Application Hardware Requirement All hosts deploying Whaleal Platform Application must meet the following hardware requirements: Number of Monitored Nodes CPU Memory Disk 50 4+ 8GB+ 10GB + logs storage 200 8+ 16GB+ 10GB + logs storage 200+ Contact Whaleal Team Contact Whaleal Team Contact Whaleal Team Whaleal Platform Application Database Hardware Requirement All hosts deploying Whaleal Platform Application Database must meet the following hardware requirements: Number of Monitored Nodes CPU Memory Disk 50 4+ 8GB+ 256GB 200 8+ 16GB+ 512GB 200+ Contact Whaleal Team Contact Whaleal Team Contact Whaleal Team For better performance, it is recommended to use: SSD for Application Database storage WiredTiger storage engine for Application Database Software Requirement Java Environment Requirement JAVA Version jdk 1.8.x open-jdk 1.8.x Operating System Compatibility Whaleal Platform Application must be deployed on a 64-bit operating system. Operating System Version Red Hat Enterprise Linux 6.x, 7.x, 8.x CentOS 6.x, 7.x, 8.x Network Security TCP Connection Requirement All Whaleal Platform Application services must be able to communicate properly with the following services: Whaleal Platform Application Database Whaleal Platform Application Agent Monitor MongoDB Hosts To ensure the principle of \"out-of-the-box\" use, Whaleal Platform Application uses the domain name cloud.whaleal.com to provide services externally. All hosts that access Whaleal Platform Application must configure host resolution: Whaleal_Platform_Application_IP cloud.whaleal.com Port Whaleal Platform Application must meet the following basic requirements: Users and Whaleal Platform Application Agent must be able to access via HTTP/HTTPS requests Whaleal Platform Application must be able to access Whaleal Platform Application Database All Whaleal Platform Application and Whaleal Platform Application Agent must be able to access the monitored MongoDB services Whaleal Platform Application must be able to send messages to users via email, SMS, and DingTalk Therefore, Whaleal Platform Application must have the following ports open: Service Default Port Transport Direction Description HTTP 8080 TCP Inbound HTTPS 8443 TCP Inbound Whaleal Platform 9600 TCP Inbound MongoDB 27017 TCP Outbound SMTP 587 TCP Outbound SMS TCP Outbound DingTalk TCP Outbound For custom ports, please open the specified ports. Ports on Host Whaleal Platform Application can complete most operations, but some processes require administrator access to the Whaleal Platform Application host to complete. Therefore, the following port must be open: Service Default Port Transport Direction Description ssh 22 TCP Inbound Agent Requirement Hardware Requirement All hosts that install the following Whaleal Platform (WAP) components must meet RAM and Disk requirements: Whaleal Platform Application Agent Whaleal Platform Application Agent Hardware Requirement All hosts deploying Whaleal Platform Application Agent must meet the following hardware requirements: Number of Managed/Monitored Nodes CPU Memory Disk 1 1+ 2GB+ 2GB + logs storage 5 2+ 4GB+ 2GB + logs storage 5+ Contact Whaleal Team Contact Whaleal Team Contact Whaleal Team Software Requirement Java Environment Requirement JAVA Version jdk 1.8.x open-jdk 1.8.x Operating System Compatibility Whaleal Platform Application Agent must be deployed on a 64-bit operating system. Operating System Version Red Hat Enterprise Linux 6.x, 7.x, 8.x CentOS 6.x, 7.x, 8.x Network Security TCP Connection Requirement All Whaleal Platform Application services must be able to communicate properly with the following services: Whaleal Platform Application Database Whaleal Platform Application Agent Monitor MongoDB Hosts To ensure the principle of \"out-of-the-box\" use, Whaleal Platform Application uses the domain name cloud.whaleal.com to provide services externally. All hosts that access Whaleal Platform Application must configure host resolution: Whaleal_Platform_Application_IP cloud.whaleal.com Port Whaleal Platform Application Agent must meet the following basic requirements: Users and Whaleal Platform Application must be able to access the server and MongoDB Therefore, Whaleal Platform Application Agent must have the following ports open: Service Default Port Transport Direction Description Whaleal Platform 9600 TCP Outbound MongoDB 27017 TCP Inbound, Outbound For custom ports, please open the specified ports. Ports on Host Whaleal Platform Application Agent can complete most operations, but some processes require administrator access to the Whaleal Platform Application host to complete. Therefore, the following port must be open: Service Default Port Transport Direction Description ssh 22 TCP Inbound "},"whalelaPlatform/01-Intstall/01-Installation.html":{"url":"whalelaPlatform/01-Intstall/01-Installation.html","title":"Installation","keywords":"","body":"Installation Whaleal Platform (WAP) supports the following two installation methods: VM Appliance Docker VM Appliance Step 1. Install JDK a. Download JDK Visit the Oracle official website to download an appropriate version of JDK for installation. Note: The following example uses jdk-8u151-linux-x64.tar.gz. If you download a different version, make sure the file extension is .tar.gz. b. Create a directory Execute the following command to create a java directory under the /usr/ directory. mkdir /usr/java cd /usr/java c. Copy the downloaded file jdk-8u151-linux-x64.tar.gz to the /usr/java/ directory and unpack it. tar -zxvf jdk-8u151-linux-x64.tar.gz d. Set environment variables Edit the /etc/profile file and add the following content. Save the file afterward. # Set Java environment JAVA_HOME=/usr/java/jdk1.8.0_151 JRE_HOME=/usr/java/jdk1.8.0_151/jre CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin export JAVA_HOME JRE_HOME CLASS_PATH PATH Note: Make sure to adjust the paths for JAVA_HOME and JRE_HOME according to your actual installation paths and JDK version. Apply the changes to the current session. source /etc/profile e. Test Test the JDK installation by running the following command. java -version If it displays Java version information, the JDK is successfully installed: java version \"1.8.0_151\" Java(TM) SE Runtime Environment (build 1.8.0_151-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode) Step 2. Install NACOS Installation Guide - NACOS version 1.4 or higher is required. Step 3. Install MongoDB Installation Guide Step 4. Install Whaleal a. Gateway Module Modify the project configuration file server/ops-gateway-dev.yml: spring: cloud: nacos: discovery: server-addr: ****** # Configure Nacos address Start the gateway module: nohup java -jar /root/whaleal/server/ops-gateway-1.0.0.jar --spring.config.location=ops-gateway-dev.yml > whaleal-geteway.log & b. Data Collection Module Modify the project configuration file server/data-collection-api-dev.yml: spring: data: mongodb: uri: mongodb://****** # Configure AppDB database address database: ****** application: name: data-os-collection cloud: nacos: discovery: server-addr: ****** # Configure Nacos address Start the data collection module: nohup java -jar /root/whaleal/server/data-collection-api-1.0.0.jar --spring.config.location=data-collection-api-dev.yml > data-collection-api.log & c. Web Module Modify the project configuration file server/ops-server-web-dev.yml: server: port: 9602 spring: cloud: nacos: discovery: server-addr: ****** # Configure Nacos address data: mongodb: uri: mongodb://****** # Configure AppDB database address database: ****** file: root: path: /home/whaleal/server/ # Whaleal Platform database medium package storage directory Start the web module: nohup java -jar /root/whaleal/server/ops-server-web-1.0.0.jar --spring.config.location=ops-server-web-dev.yml > ops-server-web.log & d. Alert Module Configure the project configuration file server/ops-alert-dev.yml: spring: cloud: nacos: discovery: server-addr: ****** # Configure Nacos address data: mongodb: uri: mongodb://****** # Configure AppDB database address database: ****** feign: url: http://******/ # Whaleal project gateway address (http://IP:Port/) Start the alert module: nohup java -jar /root/whaleal/server/ops-alert-1.0.0.jar --spring.config.location=ops-alert-dev.yml > ops-alert.log & e . Archive Module Configure the project configuration file server/ops-archive-dev.yml: spring: cloud: nacos: discovery: server-addr: ****** # Configure Nacos address data: mongodb: uri: mongodb://****** # Configure AppDB database address database: ****** Start the archive module: nohup java -jar /root/whaleal/server/ops-archive-1.0.0.jar --spring.config.location=ops-archive-dev.yml > ops-archive.log & f. Third-Party Module Configure the project configuration file server/ops-third-party-dev.yml: spring: cloud: nacos: discovery: server-addr: ******** # Configure Nacos address third: sms: host: http://****** # SMS platform address appcode: *** # AppCode from: *** # Sender's phone number mail: protocol: *** # Email service protocol port: ****** # Email server port host: ****** # Email platform address from: ****** # Email sender title: ****** # Email content title username: ****** # SMTP server account password: ****** # SMTP server password default-encoding: ****** properties.mail.smtp.ssl.enable: ****** # Enable SSL transmission properties.mail.smtp.ssl.required: ****** # Require SSL transmission properties.mail.smtp.port: ****** # SMTP server port number Start the third-party module: nohup java -jar /root/whaleal/server/ops-third-party-1.0.0.jar --spring.config.location=ops-third-party-dev.yml > ops-third-party.log & g. Agent Module Copy agent-collection-1.0.0.jar to the file.root.path directory of the ops-server-web module: cp /root/whaleal/server/agent-collection-1.0.0.jar /home/whaleal/server/ Step 5. Startup and Shutdown Commands for All Modules Start nohup java -jar /root/whaleal/server/ops-gateway-1.0.0.jar --spring.config.location=ops-gateway-dev.yml > whaleal-geteway.log & nohup java -jar /root/whaleal/server/data-collection-api-1.0.0.jar --spring.config.location=data-collection-api-dev.yml > data-collection-api.log & nohup java -jar /root/whaleal/server/ops-server-web-1.0.0.jar --spring.config.location=ops-server-web-dev.yml > ops-server-web.log & nohup java -jar /root/whaleal/server/ops-alert-1.0.0.jar --spring.config.location=ops-alert-dev.yml > ops-alert.log & nohup java -jar /root/whaleal/server/ops-archive-1.0.0.jar --spring.config.location=ops-archive-dev.yml > ops-archive.log & nohup java -jar /root/whaleal/server/ops-third-party-1.0.0.jar --spring.config.location=ops-third-party-dev.yml > ops-third-party.log & Shutdown ps -ef | grep java | grep whaleal-server-web-1.0 | cut -c 9-15 | xargs kill -9 ps -ef | grep java | grep data-collection-api-1.0 | cut -c 9-15 | xargs kill -9 ps -ef | grep java | grep whaleal-alert-1.0 | cut -c 9-15 | xargs kill -9 ps -ef | grep java | grep whaleal-third-party-1.0 | cut -c 9-15 | xargs kill -9 ps -ef | grep java | grep agent-collection-1. | cut -c 9-15 | xargs kill -9 ps -ef | grep java | grep whaleal-archive-1.0 | cut -c 9-15 | xargs kill -9 ps -ef | grep java | grep whaleal-gateway-1.0 | cut -c 9-15 | xargs kill -9 Step 6. Deploy Nginx for Front-End Modify the Nginx configuration file: server { listen ******; # External service port listen ******; # Backend service address server_name ******; location / { root /www/dist; # Static files directory index index.html index.htm; } } Restart Nginx: nginx -s reload Step 7. Access via Web Browser Access the URL in the web browser: http://cloud.whaleal.com:8080/ Docker Step 1. Install Docker curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun # Restart the Docker service systemctl restart docker Step 2. Pull Docker Image docker pull whaleal/whaleal:lstest TODO: Start AppDB separately Step 3. Run Docker Container docker run -d -p 8080:8080 -p 9600:9600 whaleal Step 4. Access via Web Browser Access the URL in the web browser: http://IP:8080/dist/ "},"whalelaPlatform/02-Usage/Account/AccountCenter.html":{"url":"whalelaPlatform/02-Usage/Account/AccountCenter.html","title":"AccountCenter","keywords":"","body":"Account Center Account Center allows you to configure the following: - Email - Phone Number - Dingding URL a. Navigate to the left sidebar on the page. b. Click on the \"Account\" option button and select the \"Personal Center\" option. c. Click on the \"Edit Profile\" option button to modify your personal information. Email Modify the email address in the personal information section while editing your profile. In Whaleal, after configuring alerts, notification emails will be sent to your personal email address. Phone Number Modify the phone number in the personal information section while editing your profile. In Whaleal, after configuring alerts, alert notifications will be sent to your phone as text messages. Dingding URL You can add a robot in your Dingding (DingTalk) group. After adding it, you will receive a Webhook URL that contains an access_token value. Configure this access_token value in the Whaleal Account Center page. Alert notifications will be sent to the Dingding group. "},"whalelaPlatform/02-Usage/Account/Config.html":{"url":"whalelaPlatform/02-Usage/Account/Config.html","title":"Config","keywords":"","body":"Config Config provides the following configuration options: - Alert - TimeZone Alert Users can configure whether to receive alert notifications. TimeZone Users can configure the time zone for displaying monitored data time. "},"whalelaPlatform/02-Usage/Account/Users.html":{"url":"whalelaPlatform/02-Usage/Account/Users.html","title":"Users","keywords":"","body":"Users Users provide the following operations: - Operation - Management - Server - MongoDB Operation Whaleal can configure administrator permissions for users, allowing administrators to perform user deletion operations. Management Whaleal can configure whether users have permissions to create Servers and MongoDB instances. Server Whaleal provides access restrictions for server resources. MongoDB Whaleal provides access restrictions for MongoDB resources. "},"whalelaPlatform/02-Usage/Host/AddHost.html":{"url":"whalelaPlatform/02-Usage/Host/AddHost.html","title":"AddHost","keywords":"","body":"Add Host The process of adding a host is divided into the following two parts: - Prerequisites - Procedure Adding a host in the Whaleal platform involves creating and managing MongoDB clusters on the platform. Prerequisites Before adding a host, make sure that you have installed the JAVA environment. If not, please install it. If you encounter access issues with the page, ensure that you have configured domain name resolution. Ensure that the host has the ioStat plugin installed; if not, please install it. Prerequisites Reference Install the JAVA environment Visit the Oracle official website and download the appropriate JDK version. In this example, we'll use jdk-8u151-linux-x64.tar.gz. If you download a different version, make sure the file extension is .tar.gz. Create a directory and unzip the JDK: mkdir /usr/java cd /usr/java tar -zxvf jdk-8u151-linux-x64.tar.gz Edit the /etc/profile file and add the following lines, then save: # Set Java environment variables JAVA_HOME={jdk_path} JRE_HOME={jre_path} CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin export JAVA_HOME JRE_HOME CLASS_PATH PATH # Make changes take effect source /etc/profile Configure domain name resolution For Linux hosts: Domain: cloud.whaleal.com Domain's corresponding IP: 192.168.3.200 Execute the command:echo \"192.168.3.200 cloud.whaleal.com\" >> /etc/hosts You can also replace cloud.whaleal.com with the IP address of the Server. Install the ioStat plugin yum install sysstat Procedure Step 1. Navigate to the page a. Click the \"Server\" navigation item to enter the host page. Click the \"Add Host\" button on the right to open the prompt page. The first three steps in the prompt are part of the \"Prerequisites\" section, the fourth step generates the agentId (which acts as a unique identifier for the host), and the remaining steps involve executing commands on the host's command line. Step 2. Download Agent for the Host b. After completing the prerequisites, click the \"Generate Agent\" button on the prompt page. The subsequent operations will be executed on the host's command line. Copy the command for downloading AGENT_JAR and execute it in the command line: curl -H whaleal-Token:\"{your_token}\" -O http://cloud.whaleal.com:9600/api/server/agent/downAgentFile/62d8e617239d00094230b3b2/agent-collection-1.0.0.jar Copy the command for starting the AGENT and execute it in the command line: nohup java -jar agent-collection-1.0.0.jar --foreign.url=http://cloud.whaleal.com:9600/ --agentId=62d8e617239d00094230b3b2 & After completion, the host will be displayed in the Server host list, and you can find the agent process by querying the java processes from the command line. "},"whalelaPlatform/02-Usage/Host/HostInfos.html":{"url":"whalelaPlatform/02-Usage/Host/HostInfos.html","title":"HostInfos","keywords":"","body":"Host Information Host Information includes the following sections: - Basic Host Information - Host Update and Removal - Host Details and Operations Basic Host Information Displays basic information about the host. a. Host Name b. System Information c. Host Kernel d. Agent Uptime e. Host Status f. Host Operations Host Update and Removal Actions for updating and removing hosts. a. Removing a host (detaching it from management), detailed steps can be found in RemoveHost. b. Updating host information involves retrieving the host's information again, and then updating the page's content. This includes the host's static information, monitoring data, and status. Note that if a host experiences an abnormal shutdown and is manually restarted, the frontend may not automatically update the host's status. Clicking the \"Update Host Information\" button will refresh the host's status. Host Details and Operations Clicking on the host name leads to the host information page, where you can view details and perform operations on the host. a. Host Information This section displays the host's basic static information. b. Monitoring The monitoring information displays graphical representations of data related to MEMORY, CPU, NET, and DISKIO. The \"1\" section allows you to select different time ranges for displaying the graphical data or different granularities within a time range. The \"2\" section for NET and DISKIO allows you to choose what content you want to display using the dropdown menu. The \"3\" section lets you hide or show data by clicking on the corresponding buttons. c. Logs The logs record the host's activities, including operator actions and scheduled tasks. It provides detailed information about the execution events, status, and specific content of each event. The \"1\" section allows you to filter logs based on criteria like time range, type, or content. The \"2\" section displays the selected log information on the frontend. d. Commands The commands section includes operations at the host level or operations on MongoDB clusters. It displays the operation's status, content, event, and result, along with the event's execution process. MSG: Displays the operation's function, including host-level operations and manual operations. Status: Displays the status at different stages of the operation (real-time updates). Content: Click \"View Details\" to see the details of the cluster or operation. Event: The events include both host operation events and operator operation events. Click \"View Details\" to see the detailed execution process of the event. (Events for frontend operations are grouped into logs. Click \"View Details\" to see the event's execution process.) The \"1\" section allows you to set filters, perform fuzzy searches for MSG content, and display commands within a specific time range. The \"2\" section allows you to click \"View Details\" to display detailed command information in JSON format. The \"3\" section allows you to click \"View Event Log\" to view detailed event execution information. e. Alerts Alerts involve setting thresholds for various indicators on the host. When a threshold is breached, alerts are triggered, and notifications are sent via DingTalk, email, or SMS to notify users of abnormal conditions on the host. This page allows users to set thresholds and conditions for triggering alerts based on their specific requirements. "},"whalelaPlatform/02-Usage/Host/RemoveHost.html":{"url":"whalelaPlatform/02-Usage/Host/RemoveHost.html","title":"RemoveHost","keywords":"","body":"Remove Host Removing a host involves the following sections: - Prerequisites - Procedure Removing a host from the platform means detaching it from management. It does not uninstall the agent from the host. Prerequisites Before removing a host, check if there are any tasks related to the host that haven't been completed on the platform. Procedure Step 1. Navigate to the Host Management Page a. Click on the \"Server\" navigation tab to enter the host management page. b. On the right side of the host entry, click on the \"Detach Management\" button (or directly terminate the agent process through the command line with caution). Step 2. Check for Agent Process Termination a. After detaching management, the agent process of the host will be terminated. The host will no longer be displayed in the Whaleal platform's host list. The status of MongoDB nodes associated with this host will be updated to \"Detached from Management.\" b. If the agent process is forcibly terminated, the host will be forcefully detached from management. The host will no longer be displayed in the platform. "},"whalelaPlatform/02-Usage/MongoDB/CreateDeployment/CreateReplicaSet.html":{"url":"whalelaPlatform/02-Usage/MongoDB/CreateDeployment/CreateReplicaSet.html","title":"CreateReplicaSet","keywords":"","body":"Create ReplicaSet Creating a ReplicaSet involves the following sections: - Prerequisites - Procedure ReplicaSet deployment provides high availability mechanisms. It is recommended for production use. Using Whaleal Platform, you can create a ReplicaSet, add ReplicaSet nodes, and scale up or down. Prerequisites Before deploying a ReplicaSet, ensure that the host has been managed by the Whaleal Platform. If not, please first Add Host. Before deploying a ReplicaSet, ensure that the Whaleal Platform has an available MongoTars. If not, please first Upload MongoTar. Procedure Step 1. Navigate to the MongoDB Cluster List a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. The page will display all the MongoDB Clusters that the user can operate on. Step 2. Create a ReplicaSet a. Click on the \"Create Project\" button on the right side. b. Select the \"ReplicaSet\" option. Step 3. Configure the ReplicaSet ReplicaSet Configuration Configuration Item Value ReplicaSet Name The value of replSetName in the ReplicaSet configuration Tags The value of tag in the ReplicaSet configuration Enable Authentication true: Enable authentication, configure username and password false: Disable authentication Username If authentication is enabled, the admin user of the ReplicaSet. Authentication database is admin and role is root Password If authentication is enabled, the admin password of the ReplicaSet Member Configuration Configuration Item Value Member Type Member type in the ReplicaSet: Member Node: The node in the ReplicaSet that holds data and has voting rights. It can be elected as the primary node.Arbiter Node: A node that does not store data in the cluster and is used only for voting and elections.Hidden Node: A node in the ReplicaSet that holds data and has voting rights. Configuration parameter is hidden.Hidden Delayed Node: A node in the ReplicaSet that holds data and has voting rights. Configuration parameters are slaveDelay and hidden. Hostname The host where you want to deploy the ReplicaSet node Port The port to be used by the node Version The version of the MongoTar corresponding to the node version Votes The number of votes for elections during the ReplicaSet elections Priority The priority during the ReplicaSet elections. If priority is 0, the node cannot be elected as the primary node Delay The time (in seconds) the node is behind the primary node, only applicable to Hidden Delayed Node Build Index true: Build indexes in MongoDB false: Do not build indexes in MongoDB Data Directory The absolute path to the ReplicaSet data files Log File The absolute path to the ReplicaSet log output file Cluster Configuration Configuration Item Value Protocol Version ReplicaSet replication protocol version Chaining Allowed true: Allow data replication from secondary nodes false: Do not allow data replication from secondary nodes Write Concern Majority Journal Default Write to majority of nodes before returning Heartbeat Timeout (secs) Time between heartbeat checks between member nodes Election Timeout (ms) Time between checks when a member is unreachable CatchUp Timeout (ms) Time for a newly elected primary node to catch up with the latest writes CatchUp Takeover Delay (ms) Time to wait before taking over when a member node leads the primary node Advanced Configuration a. Click on the \"Add Option\" button. b. Select the startup configuration item to add, then click the \"Confirm\" button to add. c. Set the value of the configuration item. Step 4. Create ReplicaSet Click the \"Create\" button to create the ReplicaSet. "},"whalelaPlatform/02-Usage/MongoDB/CreateDeployment/CreateShardedCluster.html":{"url":"whalelaPlatform/02-Usage/MongoDB/CreateDeployment/CreateShardedCluster.html","title":"CreateShardedCluster","keywords":"","body":"Create Sharded Cluster Creating a Sharded Cluster involves the following sections: - Prerequisites - Procedure Sharded Cluster deployment provides both high availability and horizontal scalability mechanisms. It is recommended for production use. Using the Whaleal Platform, you can create a Sharded Cluster, add nodes, and scale up or down. Prerequisites Before deploying a Sharded Cluster, ensure that the host has been managed by the Whaleal Platform. If not, please first Add Host. Before deploying a Sharded Cluster, ensure that the Whaleal Platform has an available MongoTars. If not, please first Upload MongoTar. Procedure Step 1. Navigate to the MongoDB Cluster List a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. The page will display all the MongoDB Clusters that the user can operate on. Step 2. Create a Sharded Cluster a. Click on the \"Create Project\" button on the right side. b. Select the \"Sharding\" option. Step 3. Configure the Sharded Cluster Sharded Cluster Configuration Configuration Item Value Cluster Name The name of the Sharded Cluster Tags The value of tag in the configuration Enable Authentication true: Enable authentication, configure username and password false: Disable authentication Username If authentication is enabled, the admin user of the Sharded Cluster. Authentication database is admin and role is root Password If authentication is enabled, the admin password of the Sharded Cluster Member Configuration Shard Settings Configuration Item Value Member Type Member type in the Sharded Cluster: Member Node: The node in the Sharded Cluster that holds data and has voting rights. It can be elected as the primary node.Arbiter Node: A node that does not store data in the cluster and is used only for voting and elections.Hidden Node: A node in the Sharded Cluster that holds data and has voting rights. Configuration parameter is hidden.Hidden Delayed Node: A node in the Sharded Cluster that holds data and has voting rights. Configuration parameters are slaveDelay and hidden. Hostname The host where you want to deploy the Sharded Cluster node Port The port to be used by the node Version The version of the MongoTar corresponding to the node version Votes The number of votes for elections during the Sharded Cluster elections Priority The priority during the Sharded Cluster elections. If priority is 0, the node cannot be elected as the primary node Delay The time (in seconds) the node is behind the primary node, only applicable to Hidden Delayed Node Build Index true: Build indexes in MongoDB false: Do not build indexes in MongoDB Data Directory The absolute path to the Sharded Cluster node data files Log File The absolute path to the Sharded Cluster node log output file Config Settings Configuration Item Value Member Type Member type in the Sharded Cluster: Member Node: The node in the Sharded Cluster that holds data and has voting rights. It can be elected as the primary node.Arbiter Node: A node that does not store data in the cluster and is used only for voting and elections.Hidden Node: A node in the Sharded Cluster that holds data and has voting rights. Configuration parameter is hidden.Hidden Delayed Node: A node in the Sharded Cluster that holds data and has voting rights. Configuration parameters are slaveDelay and hidden. Hostname The host where you want to deploy the Sharded Cluster node Port The port to be used by the node Version The version of the MongoTar corresponding to the node version Votes The number of votes for elections during the Sharded Cluster elections Priority The priority during the Sharded Cluster elections. If priority is 0, the node cannot be elected as the primary node Delay The time (in seconds) the node is behind the primary node, only applicable to Hidden Delayed Node Build Index true: Build indexes in MongoDB false: Do not build indexes in MongoDB Data Directory The absolute path to the Sharded Cluster node data files Log File The absolute path to the Sharded Cluster node log output file Mongos Settings Configuration Item Value Hostname The host where you want to deploy the mongos node Port The port to be used by the node Version The version of the MongoTar corresponding to the mongos node Log File The absolute path to the mongos log output file Cluster Configuration Configuration Item Value Protocol Version ReplicaSet replication protocol version Chaining Allowed true: Allow data replication from secondary nodes false: Do not allow data replication from secondary nodes Write Concern Majority Journal Default Write to majority of nodes before returning Heartbeat Timeout (secs) Time between heartbeat checks between member nodes Election Timeout (ms) Time between checks when a member is unreachable CatchUp Timeout (ms) Time for a newly elected primary node to catch up with the latest writes CatchUp Takeover Delay (ms) Time to wait before taking over when a member node leads the primary node Advanced Configuration a. Click on the \"Add Option\" button. b. Select the startup configuration item to add, then click the \"Confirm\" button to add. c. Set the value of the configuration item. Step 4. Create Sharded Cluster Click the \"Create\" button to create the Sharded Cluster. "},"whalelaPlatform/02-Usage/MongoDB/CreateDeployment/CreateStandalone.html":{"url":"whalelaPlatform/02-Usage/MongoDB/CreateDeployment/CreateStandalone.html","title":"CreateStandalone","keywords":"","body":"Create Standalone Creating a Standalone involves the following sections: - Prerequisites - Procedure Using the Whaleal Platform, you can create a Standalone instance. Standalone instances are suitable for testing and development purposes. It is not recommended to use the Standalone deployment method in a production environment due to the lack of high availability mechanisms. For production environments, it is recommended to use the ReplicaSet deployment method. Prerequisites Before deploying a Standalone instance, ensure that the host has been managed by the Whaleal Platform. If not, please first Add Host. Before deploying a Standalone instance, ensure that the Whaleal Platform has an available MongoTars. If not, please first Upload MongoTar. Procedure Step 1. Navigate to the MongoDB Cluster List a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. The page will display all the MongoDB Clusters that the user can operate on. Step 2. Create a Standalone Instance a. Click on the \"Create Project\" button on the right side. b. Select the \"Single Node\" option. Step 3. Configure the Standalone Instance Configure the following settings on the page Configuration Item Value Hostname The host where you want to deploy the Standalone instance Port The port to be used by the Standalone instance Data Directory The absolute path to the Standalone instance data files Log File The absolute path to the Standalone instance log output file Version The version of the MongoTar corresponding to the Standalone instance version Authentication true: Enable authentication, configure username and password false: Disable authentication Username If authentication is enabled, the admin user of the Standalone instance. Authentication database is admin and role is root Password If authentication is enabled, the admin password of the Standalone instance Step 4. Configure Options a. Click the \"Add Option\" button. b. Select the startup configuration item to add, then click the \"Confirm\" button to add. c. Set the value of the configuration item. Step 5. Create Standalone Instance Click the \"Create\" button to create the Standalone instance. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/ReplicaSet/ClusterLogs.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/ReplicaSet/ClusterLogs.html","title":"ClusterLogs","keywords":"","body":"Cluster Logs Cluster Logs provide the following operation: - Cluster Logs Cluster Logs View Cluster Logs data a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"ReplicaSet\". e. On the cluster information page, select \"Cluster Logs\". Collect log information from all nodes within the cluster, analyze it, and confirm the cluster's status. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/ReplicaSet/Data.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/ReplicaSet/Data.html","title":"Data","keywords":"","body":"Data Data provides the following operation: - Find Data Find Data View data within the cluster a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"ReplicaSet\". e. On the cluster information page, select \"Data Management\". You can use this page to view the data stored in the cluster, making it convenient for users to perform data queries. Perform a query by entering your search criteria in the input box. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/ReplicaSet/EventLogs.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/ReplicaSet/EventLogs.html","title":"EventLogs","keywords":"","body":"Event Logs Event Logs provides the following operation: - Event Logs Event Logs View event logs data a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"ReplicaSet\". e. On the cluster information page, select \"Event Logs\". Event logs record all the actions that users perform on the cluster and keep track of the progress and results of the actions. Click the \"View\" button to see detailed information about an event. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/ReplicaSet/Info.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/ReplicaSet/Info.html","title":"Info","keywords":"","body":"Info ReplicaSet Manage ReplicaSet provides the following operations: - Monitoring Data - MongoDB Logs - Real-time Diagnosis - Alert - Details - Operation View ReplicaSet node data a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"ReplicaSet\". Monitoring Data View Monitoring data a. On the cluster information page, select \"Node Info\". b. Under \"Node Info\", click on the node name (usually in the format hostname:port). Whaleal Platform provides a rich set of monitoring metrics and allows you to query and filter data for any time range. MongoDB Logs View MongoDB Logs data a. On the cluster information page, select \"Node Info\". b. Under \"Node Info\", click on \"View Logs\". Whaleal Platform records and stores complete MongoDB logs, providing filtering options to quickly locate issues. Real-time Diagnosis View Real-time Diagnosis data a. On the cluster information page, select \"Node Info\". b. Under \"Node Info\", click on \"Real-time Diagnosis\". Top Top displays hot collections at the current moment. Op Explain Alert View Alert data a. On the cluster information page, select \"Node Info\". b. Under \"Node Info\", click on \"Alert Monitoring\". Details View Details data a. On the cluster information page, select \"Node Info\". b. Under \"Node Info\", click on \"Details\". Operation Perform various operations on this node, including updating node information, starting, shutting down, restarting, deleting nodes, removing nodes from management, enabling/disabling QPS monitoring, enabling/disabling TopAndOp monitoring, enabling/disabling MongoDB log collection. Update Node Information: By default, node information is updated every 10 seconds. Click the button to trigger an immediate update and display the latest status information of the node. Start Node: Click to start a stopped node. Shutdown Node: Click to shut down a running node. Restart Node: Restart a running node. Delete Node: Remove this node from the cluster. Remove from Management: Whaleal Platform will no longer monitor or manage this node. Enable/Disable QPS Monitoring: Decide whether to collect QPS monitoring data. Enable/Disable TopAndOp Monitoring: Decide whether to collect real-time diagnosis data. Enable/Disable MongoDB Log Collection: Decide whether to collect MongoDB log data. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/ReplicaSet/Operation.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/ReplicaSet/Operation.html","title":"Operation","keywords":"","body":"Operation Operation provides the following operations: - Add Node - Cluster Info - Authentication - Modify Version Cluster Operations a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"ReplicaSet\". e. On the cluster information page, select \"Operation\". Add Node This operation allows you to add nodes to the ReplicaSet cluster and specify node configuration information. Cluster Info View configuration information of nodes in the cluster. Authentication Enable authentication. You must specify a user in the admin database. Automated user creation, configuration file modification, and service restart operation. Modify Version Upgrade or downgrade the version of the cluster by selecting a version. One-click operation to modify FCV, change the media package, and restart services. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/ReplicaSet/RoleAndUser.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/ReplicaSet/RoleAndUser.html","title":"RoleAndUser","keywords":"","body":"Role And User Role And User provides the following operations: - Role - User View Role and User data a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"ReplicaSet\". e. On the cluster information page, select \"User Management\". Role Click on the \"Role Management\" button to display all role information in the cluster. Click on the \"View Permissions\" button for a role to see its specific permissions. User Click on the \"User Management\" button to display all users in the cluster. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Sharding/ClusterLogs.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Sharding/ClusterLogs.html","title":"ClusterLogs","keywords":"","body":"Cluster Logs Cluster Logs provides the following operation: - Cluster Logs Cluster Logs View Cluster Logs data a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"Sharded Cluster\". e. On the cluster information page, select \"Cluster Logs\". Collect logs from all nodes within the cluster, analyze them, and confirm the status of the cluster. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Sharding/Data.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Sharding/Data.html","title":"Data","keywords":"","body":"Data The Data section provides the following operation: - Find Data Find Data View data within the cluster a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"Sharded Cluster\". e. On the cluster information page, select \"Data Management\". You can use the page to view the data stored in the cluster, making it convenient for users to perform data queries. You can perform queries by entering search criteria into the input box. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Sharding/EventLogs.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Sharding/EventLogs.html","title":"EventLogs","keywords":"","body":"Event Logs The Event Logs section provides the following operation: - Event Logs Event Logs View Event Logs data a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"Sharded Cluster\". e. On the cluster information page, select \"Event Logs\". Event logs record all operations performed by users on the cluster, along with the progress and results of those operations. By clicking the \"View\" button, you can see detailed information about specific events. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Sharding/Info.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Sharding/Info.html","title":"Info","keywords":"","body":"Info Sharding The Info Sharding section provides the following operations: - Monitoring Data - MongoDB Logs - Real-time Diagnosis - Alert - Details - Operation View Sharding node data a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"Sharded Cluster\". Monitoring Data View Monitoring Data a. On the cluster information page, select node information. b. Under node information, click on the node name (usually in the format hostname:port). Whaleal Platform provides a rich set of monitoring metrics and allows filtering for any time range. MongoDB Logs View MongoDB Logs data a. On the cluster information page, select node information. b. Under node information, click on \"View Logs\". Whaleal Platform records and stores complete MongoDB logs, providing filtering options to easily locate and diagnose issues. Real-time Diagnosis View Real-time Diagnosis data a. On the cluster information page, select node information. b. Under node information, click on \"Real-time Diagnosis\". Top Top displays hot collections at the current time. Op Op displays specific operation commands executed at the current time. Explain Explain analyzes query operation execution plans, facilitating query optimization and adjustments. Alert View Alert data a. On the cluster information page, select node information. b. Under node information, click on \"Alert Monitoring\". Configure alerts for specific metrics. When triggered, alerts can be sent to users via email, SMS, DingTalk, etc. Details View Details data a. On the cluster information page, select node information. b. Under node information, click on \"Details\". Displays detailed node information, including creation time, version, startup command, and node configuration. Operation Perform other operations on this node, including: update node information, start node, shut down node, restart node, delete node, detach from management, enable/disable QPS monitoring, enable/disable TopAndOp monitoring, enable/disable MongoDB log collection. Update Node Information: By default, updates node information every 10 seconds. Click the button to trigger an immediate update and display the latest node status. Start Node: Click to start a stopped node. Shut Down Node: Click to shut down a running node. Restart Node: Restart a running node. Delete Node: Remove this node from the cluster. Detach from Management: Whaleal Platform will no longer monitor or manage this node. Enable/Disable QPS Monitoring: Choose whether to collect QPS monitoring data. Enable/Disable TopAndOp Monitoring: Choose whether to collect real-time diagnosis data. Enable/Disable MongoDB Log Collection: Choose whether to collect MongoDB log data. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Sharding/Operation.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Sharding/Operation.html","title":"Operation","keywords":"","body":"Operation The Operation section provides the following operations: - Node Manage - Cluster Info - Authentication - Modify Version Cluster operations a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"Sharded Cluster\". e. On the cluster information page, select \"Operation\". Node Manage This operation allows you to add shards to the sharded cluster, add nodes to shards, and add mongos nodes to the cluster. Cluster Info View configuration information of nodes in the cluster. Authentication Enable authentication. A user must be specified under the admin database. Automatically create users, modify configuration files, and restart services. Modify Version Perform version upgrade or downgrade operations on the cluster by selecting a version. This one-click operation allows you to modify the Feature Compatibility Version (FCV), change the media package, and restart services. Note: For sharded cluster upgrades or downgrades, the balancer must be manually turned off. After sharded cluster upgrades or downgrades are completed, restore the balancer to its previous state. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Sharding/RoleAndUser.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Sharding/RoleAndUser.html","title":"RoleAndUser","keywords":"","body":"Role And User The Role And User section provides the following operations: - Role - User View Role and User data a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"Sharded Cluster\". e. On the cluster information page, select \"User Management\". Role Click on the \"Role Management\" button to display information about all roles in the cluster. Click on a role and then click the \"View Permissions\" button to see the specific permissions associated with that role. User Click on the \"User Management\" button to display information about all users in the cluster. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Standalone/ClusterLogs.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Standalone/ClusterLogs.html","title":"ClusterLogs","keywords":"","body":"Cluster Logs The Cluster Logs section provides the following operation: - Cluster Logs Cluster Logs View Cluster Logs data a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"Standalone\". e. On the cluster information page, select \"Cluster Logs\". Collect logs from all nodes within the cluster, analyze them, and confirm the cluster's status. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Standalone/Data.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Standalone/Data.html","title":"Data","keywords":"","body":"Data The Data section provides the following operation: - Find Data Find Data View data within the cluster a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"Standalone\". e. On the cluster information page, select \"Data Management\". You can use the page to view the data stored within the cluster, making it convenient for users to query the data. You can enter query conditions in the input box to perform queries. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Standalone/EventLogs.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Standalone/EventLogs.html","title":"EventLogs","keywords":"","body":"Event Logs The Event Logs section provides the following operation: - Event Logs Event Logs View Event Logs data a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"Standalone\". e. On the cluster information page, select \"Event Logs\". Event logs record all operations performed by users on the cluster, including the progress and results of the operations. By clicking the \"View\" button, you can see the detailed information about specific events. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Standalone/Info.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Standalone/Info.html","title":"Info","keywords":"","body":"Info Standalone The Info Standalone section provides the following operations: - Monitoring Data - MongoDB Logs - Real-time Diagnosis - Alert - Details - Operation View Standalone node data a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"Standalone\". Monitoring Data View Monitoring Data a. On the cluster information page, select the node information. b. Under the node information, click on the node name (usually in the format hostname:port). Whaleal Platform provides rich monitoring metrics and allows filtering within any time range. MongoDB Logs View MongoDB Logs Data a. On the cluster information page, select the node information. b. Under the node information, click on \"View Logs\". Whaleal Platform records and saves complete MongoDB logs, providing filtering options to quickly locate issues. Real-time Diagnosis View Real-time Diagnosis Data a. On the cluster information page, select the node information. b. Under the node information, click on \"Real-time Diagnosis\". Top Top displays hot collections at the current time point. Op Op displays specific command execution at the current time. Explain Explain analyzes the execution plan of queries, facilitating query adjustment and optimization. Alert View Alert Data a. On the cluster information page, select the node information. b. Under the node information, click on \"Alert Monitoring\". Configure alerts for specific metrics. Once triggered, users are notified via email, SMS, DingTalk, and other methods. Details View Details Data a. On the cluster information page, select the node information. b. Under the node information, click on \"Details\". Display detailed node information, including creation time, version, startup command, and node configuration. Operation Perform other operations on this node, including: updating node information, starting node, shutting down node, restarting node, removing node from management, enabling/disabling QPS monitoring, enabling/disabling TopAndOp monitoring, enabling/disabling MongoDB log collection. Update Node Information: The default interval is 10 seconds to trigger the update of node information. Click the button to trigger it immediately and display the latest node status. Start Node: Click to start a stopped node. Shutdown Node: Click to shut down a running node. Restart Node: Restart a running node. Remove from Management: Whaleal Platform will no longer monitor or manage this node. Enable/Disable QPS Monitoring: Choose whether to collect QPS monitoring data. Enable/Disable TopAndOp Monitoring: Choose whether to collect real-time diagnosis data. Enable/Disable MongoDB Log Collection: Choose whether to collect MongoDB log data. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Standalone/Operation.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Standalone/Operation.html","title":"Operation","keywords":"","body":"Operation The Operation section provides the following operations: - Standalone to ReplicaSet - Cluster Info - Authentication - Modify Version Cluster operations a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"Standalone\". Standalone to ReplicaSet This operation converts a \"Standalone\" cluster into a \"ReplicaSet\" cluster. It automates the configuration and restart process, reducing user operation steps. Cluster Info View cluster node configuration information. Authentication Enable authentication, a user in the admin database must be specified. This operation automates user creation, configuration file modification, and service restart. Modify Version Perform version upgrade or downgrade operations on the cluster. It's a one-click operation that allows modifying FCV, changing media packages, and restarting services. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Standalone/RoleAndUser.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster/Standalone/RoleAndUser.html","title":"RoleAndUser","keywords":"","body":"Role And User The Role And User section provides the following operations: - Role - User View Role and User data a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. On the MongoDB static information page, click on the name of the cluster with the type \"Standalone\". Role Click on the \"Role Management\" button to display all role information in the cluster. Click on a role and then click the \"View Permissions\" button to see the specific permissions associated with that role. User Click on the \"User Management\" button to display all users in the cluster. "},"whalelaPlatform/02-Usage/MongoDB/UploadMongoTar.html":{"url":"whalelaPlatform/02-Usage/MongoDB/UploadMongoTar.html","title":"UploadMongoTar","keywords":"","body":"Upload MongoDB Tar Before uploading the MongoDB Tar file, please read the following instructions: - Procedure Whaleal Platform requires the use of TGZ files for creating clusters. Other file types are not currently supported. Procedure Step 1. Navigate to the Directory a. Go to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoTars\" option. The page will display all available TGZ files for creating clusters. Step 2. Upload the File a. Click on \"Upload\" button. b. Choose the TGZ file you want to upload. c. Click \"Confirm.\" d. Wait for the upload progress bar to complete. Once it's finished, the upload is successful. "},"whalelaPlatform/02-Usage/MongoDB/ManageCluster.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ManageCluster.html","title":"ManageCluster","keywords":"","body":"Manage Cluster The Manage Cluster section provides the following operation: - Cluster Operation Cluster Operation Cluster operations include: a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. In the MongoDB static information page, click on an operation (Update Node Information, Start Cluster, Shut Down Cluster, Restart Cluster, Detach from Management, Rename). These operations allow you to perform various actions on the cluster, such as updating node information, starting, shutting down, restarting the cluster, detaching a node from management, and renaming the cluster. "},"whalelaPlatform/02-Usage/MongoDB/ExistingMongoDBDeployment.html":{"url":"whalelaPlatform/02-Usage/MongoDB/ExistingMongoDBDeployment.html","title":"ExistingMongoDBDeployment","keywords":"","body":"Existing MongoDB Deployment The Existing MongoDB Deployment section provides the following operations: - Existing MongoDB Deployment Existing MongoDB Deployment a. Navigate to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. d. Click on the \"Create Project\" button, then select \"Manage\". By configuring the information of any node in the cluster, Whaleal Platform will discover all nodes in the cluster, allowing for monitoring and management of all nodes. "},"whalelaPlatform/03-UseCases/HowToCreateReplicaSet.html":{"url":"whalelaPlatform/03-UseCases/HowToCreateReplicaSet.html","title":"HowToCreateReplicaSet","keywords":"","body":"How to Create ReplicaSet Create ReplicaSet Step 1. Navigate to the Directory a. Go to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. The page will display all MongoDB clusters that the user can operate. Step 2. Create ReplicaSet a. Click on the \"Create Project\" button on the right side. b. Choose the \"Replica Set\" option. Step 3. Configure ReplicaSet a. Click the \"Add Option\" button. b. Select the startup configuration options to add by clicking the \"Confirm\" button. c. Set the values for the configuration options: Replica Set Configuration: Member Configuration: Cluster Configuration: Advanced Configuration: Step 4. Create Click the \"Create\" button to create the ReplicaSet. "},"whalelaPlatform/03-UseCases/HowToCreateShardedCluster.html":{"url":"whalelaPlatform/03-UseCases/HowToCreateShardedCluster.html","title":"HowToCreateShardedCluster","keywords":"","body":"How to Create Sharded Cluster Create Sharded Cluster Step 1. Navigate to the Directory a. Go to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. The page will display all MongoDB clusters that the user can operate. Step 2. Create Sharded Cluster a. Click on the \"Create Project\" button on the right side. b. Choose the \"Sharding\" option. Step 3. Configure Sharded Cluster Sharded Cluster Configuration: Member Configuration: a. Click the \"Add Option\" button. b. Select the startup configuration options to add by clicking the \"Confirm\" button. c. Set the values for the configuration options: Shard Settings: Config Settings: Mongos Settings: Cluster Configuration: Advanced Configuration: Step 4. Create Click the \"Create\" button to create the Sharded Cluster. "},"whalelaPlatform/03-UseCases/HowToCreateStandalone.html":{"url":"whalelaPlatform/03-UseCases/HowToCreateStandalone.html","title":"HowToCreateStandalone","keywords":"","body":"How to Create Standalone Create Standalone Step 1. Navigate to the Directory a. Go to the left-side navigation bar. b. Click on the \"MongoDB\" option. c. Select the \"MongoList\" option. The page will display all MongoDB clusters that the user can operate. Step 2. Create Standalone a. Click on the \"Create Project\" button on the right side. b. Choose the \"Standalone\" option. Step 3. Configure Standalone Fill in the cluster configuration. Step 4. Configure Options a. Click the \"Add Configuration Option\" button. b. Select the startup configuration options to add by clicking the \"Confirm\" button. c. Set the values for the configuration options. Step 5. Create Click the \"Create\" button to create the Standalone cluster. "},"whalelaPlatform/03-UseCases/HowToFindBottleNeckinHost.html":{"url":"whalelaPlatform/03-UseCases/HowToFindBottleNeckinHost.html","title":"HowToFindBottleNeckinHost","keywords":"","body":"Find Bottleneck in Host You can identify and address bottlenecks on a host using the following operations: Check the Monitor On Linux systems, the primary bottlenecks are typically related to memory (RAM), computation (CPU), or I/O operations (disk). For memory, speed could be a factor, and running out of memory is a significant issue. For the CPU, if older hardware is used, the performance of each CPU core may be slow, and there might not be enough processing power. Regarding I/O, reading from mechanical hard drives and excessive disk writes can be problematic. CPU Check CPU monitoring data to inspect CPU usage. If the CPU reaches 95% or more while memory (Mem) and swap (Swp) are within normal ranges, it indicates a CPU bottleneck. If the application or process isn't running at the expected performance level and consistently shows 95%+ CPU utilization, you can take the following steps: Immediate Solution: Add more CPU cores to the server. Troubleshooting: Investigate and locate the problematic application and address the issues accordingly. If adding more CPU cores still results in CPU utilization above 95%, but the application's performance and throughput improve, consider adding CPUs to address the problem. Otherwise, focus on troubleshooting issues within the application. RAM Review RAM monitoring data. If Memory usage is at 100%, and Swap usage is at 50%, the system is likely swapping heavily. Swapping is the process of moving content between disk and main memory (using a specialized swap partition), and with Memory at 100%, the system will become significantly slower as it continues to swap. For example, you might see that only 20% of memory is actively used, yet a lot of memory remains free. This might indicate that the operating system has moved some infrequently used memory regions to disk to optimize the main memory. As long as there is still plenty of free memory available, this situation is not a problem. I/O When observing monitoring data and neither CPU nor RAM appear to be bottlenecks, you should focus on I/O. For example, if you notice that I/O on an SSD is not very high, but there is heavy read/write I/O to an HDD, you'll need to address the I/O issue. This could involve actions such as stopping some excessive write operations, upgrading the I/O system, replacing slower I/O devices with faster ones, or upgrading to faster SSDs. "},"whalelaPlatform/03-UseCases/HowToFindBottleNeckinMongoDB.html":{"url":"whalelaPlatform/03-UseCases/HowToFindBottleNeckinMongoDB.html","title":"HowToFindBottleNeckinMongoDB","keywords":"","body":"Find Bottleneck in MongoDB Finding bottlenecks in MongoDB can be divided into the following two parts: Find Bottleneck Review MongoDB monitoring data to observe metrics such as reads per second, executed commands, read-write queue lengths, network throughput, and connection counts for nodes. Performance monitoring data provides insight into the overall connection count, read and write request counts, as well as the ratio between reads and writes for MongoDB instances. Pay attention to the read-write queue lengths. If this value exceeds 3 or the number of CPU cores, it indicates that CPU resources are constrained, and there is a backlog of business requests. Analyze MongoDB real-time diagnostic data to identify tables with high query times. Following the Pareto principle, focus on analyzing and optimizing slow queries for the \"hot\" tables that contribute to more than 80% of the request time. Examine diagnostic data to see the specific slow query requests currently executed by the MongoDB instance. For databases with frequent aggregation analysis requests, queries that take more than 100 seconds to execute might be observed. This leads to intense CPU and IO resource usage. To prevent disruptions to normal business operations, you might need to temporarily terminate many accumulated slow queries. Adjust and Optimize Optimization strategies for a MongoDB sharded cluster: If a specific shard in the sharded cluster exhibits high load: Part-1: Start by checking the MongoDB monitoring page to understand the overall concurrent load and read-write ratio of the system. Identify where the bottleneck is likely located. Part-2: If the load concentrates on a particular node, record the tables that have frequent operations using real-time diagnostic data. Part-3: Analyze the top 10 slow queries that occur during periods of high load using diagnostic data. Part-4: Identify the target tables for optimization and focus on query optimization. Often, Part-2 and Part-3 will reveal many common tables. Frequently accessed tables and slow queries often share similar tables. These tables are your optimization targets. Key points for MongoDB sharding optimization: a. Review table shard keys, data distribution, total data volume, and data storage space. Pay attention to whether the data shard key settings are appropriate and if data distribution is uniform. b. Examine the specific queries in the slow query information printed in the diagnostic data. Check if there are suitable indexes on the slow query tables to fulfill the query conditions. Analyze the specific execution plans of slow queries using explain(). c. Extract original query statements related to slow query tables from the original logs of the MongoDB instance during peak business periods. Record these queries for communication with development teams to discuss potential optimizations based on business scenarios. d. For log-type tables (logs, events, sessions, etc.), retain only valid data within a certain time frame based on business requirements. Coordinate with development teams to determine the retention period. Once determined, use MongoDB TTL index features to create an index on the specific time field and set a record expiration time. Part-5: Implement read-write separation optimization at the architecture level. If the top 10 slow queries identified in Part-3 include queries that can effectively use indexes, the execution time should be normally fast (within 200ms). If this issue cannot be resolved, consider implementing read-write separation optimization at the architectural level. High-concurrency reads and writes to hotspot tables can overwhelm the CPU and cause blockages for normally efficient queries. In summary, the key to MongoDB optimization is identifying system bottlenecks and root causes of problems. After pinpointing tables that require optimization, a simple addition of an index or implementation of read-write separation often effectively resolves performance issues. "},"whalelaPlatform/04-Troubleshooting/AddHostFaild.html":{"url":"whalelaPlatform/04-Troubleshooting/AddHostFaild.html","title":"AddHostFailed","keywords":"","body":"Host Issues Agent Jar Cannot Run When the agent jar cannot run, first check if you have Java environment installed on your host. If not, follow the Java environment setup instructions. Host Abnormal Shutdown The platform will continuously monitor the status of each host that has been onboarded. If the platform shows that a host has abnormally shut down, first check if the host is running properly. If the host has shut down unexpectedly, take physical maintenance measures. If the host is running normally and hasn't actually shut down, check if the agent process is running properly. If the process has crashed or was killed abnormally, restart it. Cannot Connect to Server Check if the server side is functioning properly. Check if the agent ID is correct and restart if necessary. Insufficient Host Memory When creating clusters on a host, the platform allocates half of the available resources by default. If not properly configured, creating too many clusters can lead to host crashes. During cluster creation, configure an appropriate cache size in the advanced settings to prevent excessive resource consumption and waste. "},"whalelaPlatform/04-Troubleshooting/LoginFaild.html":{"url":"whalelaPlatform/04-Troubleshooting/LoginFaild.html","title":"LoginFailed","keywords":"","body":"Login Issues Unable to Open Login Page Make sure your computer has domain name resolution configured. Details can be found in the domain name resolution setup section of the host pre-configuration. Login Failed If you're unable to log in, first check if your network is functioning properly. Then verify that you've entered the correct username and password. If you've forgotten your password, contact an administrator to reset it. "},"whalelaPlatform/04-Troubleshooting/MongoFaild.html":{"url":"whalelaPlatform/04-Troubleshooting/MongoFaild.html","title":"MongoFailed","keywords":"","body":"Mongo Issues Creation Failed Check if the host is running properly. Verify that the port used by the node is not already in use. Ensure that the data directory doesn't contain data from other clusters. Replica Set Initialization Failed If replica set initialization fails, manual initialization can be performed. Failed to Add Replica Set Node When adding a node, make sure the port is not already in use. Check if the data storage directory contains data from other clusters. Version Upgrade/Downgrade Failed During version upgrades/downgrades, ensure that the target version is higher than the current version for upgrades and lower for downgrades. Version upgrades/downgrades must be done sequentially and cannot skip versions. (For example, you cannot directly upgrade from version 4.2 to 5.0. You would first upgrade to 4.4, then to 5.0. Similarly, downgrading from 5.0 to 4.2 is not allowed. You would first downgrade to 4.4, then to 4.2.) If upgrading an arbiter node in a replica set fails, manually replace the arbiter node's data directory. Authentication Enable/Disable Failed Authentication toggles are only available for the \"admin\" database and use the username/password method. Sharded Cluster Addition Failed Ensure that port conflicts are resolved and that the data directory does not contain data from other clusters. When adding a replica set to a sharded cluster, arbiters and hidden delay nodes cannot be added. Node Showing \"No State\" After Creation Clicking the \"Update Node Information\" button will resolve this. Member Node Becomes Primary and Reverts Check if the nodes have different priority settings. A higher-priority node will become the primary node. Monitoring Displays No Data Some monitoring data is collected only after specific actions are performed. If there's no data in monitoring, adjust the time range to view more data. Sharding Addition Failed Check if ports or data directories are already in use. If so, replace them. Authentication Deactivation Failed The cluster may not be accessible externally, requiring manual startup. Hidden Delay Node Operation Abnormality When a cluster with hidden delay nodes has authentication enabled, abnormal operations can occur. This is because the state of the primary node must be synchronized with the hidden delay nodes after the delay period. "},"whalelaPlatform/05-ReleaseNotes/releaseNote-1.0.0.html":{"url":"whalelaPlatform/05-ReleaseNotes/releaseNote-1.0.0.html","title":"ReleaseNote-1.0.0","keywords":"","body":"Whaleal Platform ChangeLog Whaleal Platform V1.0.0 Whaleal Platform V1.0.0 is the initial release version of the platform. It includes the following functional modules: 1. Login and Registration Registration User registration only requires basic format validation and checks for existing accounts. It's recommended to provide a phone number (limited to mainland China) and an email address. Login Login methods include phone number + password, email + password, and account + password. 2. Dashboard Host Overview Displays the status of hosts, CPU, memory, and disk in a pie chart. Host Summary Displays detailed information about CPU, memory, disk, network in/out, and other metrics in graphical form. Mongo Overview Displays real-time information about MongoDB nodes, clusters, and cluster types in a pie chart. MongoDB Summary Displays summary data for clusters, collections, crashed nodes, QPS, connections, and slowest queries in graphical form. 3. Host List Host Statistics Displays general information about managed hosts and allows actions like detaching a host or updating its data. Add Host Allows adding new hosts, with details about the process in the AddHost section. Host Information Clicking on a host's name opens a detailed page with information about the host, including monitoring, logs, commands, and alerts. More details can be found in HostInfos. 4. MongoDB List MongoDB Static Information Displays information about managed MongoDB clusters. You can search for clusters and perform various operations, such as updating node information, starting, stopping, restarting, detaching from management, and renaming. Create Project Allows creating different types of MongoDB clusters. Cluster types include standalone, replica set, and sharded cluster. You can also manage existing clusters. Detailed steps for creating a standalone deployment --> CreateStandalone Detailed steps for creating a replica set --> CreateReplicaSet Detailed steps for creating a sharded cluster --> CreateShardedCluster MongoDB Cluster Operations MongoDB Media Package Management When creating a cluster, you can select different MongoDB versions. You can upload MongoDB media packages using the MongoTars page. Detailed steps for uploading a media package --> UploadMongoTar 5. User Center Personal Center Displays personal information provided during registration and allows updates and additions. User Management User management is accessible only to the \"admin\" account. This page allows user deletion and role assignment. Clicking on a username opens a user's resource page, where you can manage their permissions, such as adding hosts and creating clusters. On the Server and Mongo pages, you can show or hide specific hosts or clusters for the user. Account Configuration Account configuration lets you set the time zone and choose whether to receive alert notifications. 6. Support & Help Documentation Column Whaleal Community Documentation Community Address Whaleal Community Whaleal Platform Agent V1.0.0 "},"whalelaPlatform/06-FAQ/ForOpsManagerUser.html":{"url":"whalelaPlatform/06-FAQ/ForOpsManagerUser.html","title":"ForOpsManagerUser","keywords":"","body":"For OpsManager User Can Whaleal help troubleshoot issues using monitoring data? Whaleal provides extensive monitoring metrics with a granularity of up to 1 second. Combined with real-time diagnostic information collection such as Top, Op, and Explain, it can help users quickly troubleshoot and pinpoint issues, facilitating rapid issue resolution. What alerting methods are supported by Whaleal? Whaleal not only supports email alerts, but also provides SMS and DingTalk (a messaging app) alerting methods. This ensures that users can receive alert notifications through various means, allowing them to stay informed about the cluster's status anytime and anywhere. Does Whaleal support changes to cluster architecture? Whaleal supports changing the architecture from Standalone to ReplicaSet. It automates the complex process of architectural changes, allowing users to accomplish the transition with a single click through the platform interface. Does Whaleal support changing the version of a cluster? Whaleal offers the functionality to upgrade and downgrade clusters. Through simple page configurations, users can perform version changes between adjacent versions of MongoDB clusters. This feature not only supports upgrades but also provides straightforward downgrade operations. These version changes can be performed in a rolling manner without causing any service disruption. Which MongoDB versions does Whaleal support? Without changing the version of Whaleal, it supports 98% of MongoDB versions available in the market, ranging from MongoDB 3.4 to 5.0. "},"whalelaPlatform/06-FAQ/ForPMMUser.html":{"url":"whalelaPlatform/06-FAQ/ForPMMUser.html","title":"ForPMMUser","keywords":"","body":"For PMM (Persona Monitoring and Management) User Does Whaleal support user permission segregation? Whaleal's administrator users can restrict resource access for platform users, controlling whether they can access Server or MongoDB resources. Does Whaleal support creating MongoDB clusters? Whaleal supports creating MongoDB clusters in various configurations: Create Standalone: CreateStandalone Create Replica Set Cluster: CreateReplicaSet Create Sharded Cluster: CreateShardedCluster Does Whaleal support managing existing MongoDB clusters? Whaleal supports monitoring and managing existing MongoDB clusters. You can use ExistingMongoDBDeployment to add monitoring and management capabilities to an existing MongoDB cluster. Whaleal supports discovering all nodes in a cluster through a single node configuration and monitors them. Does Whaleal support operations on MongoDB clusters? Whaleal provides users with common operations used in usage and maintenance processes, enabling users to make changes to clusters through configuration and clicks on the platform interface. What MongoDB operations does Whaleal provide? Diagnostic Analysis By combining real-time diagnostic data such as Top, Op, and Explain, Whaleal helps confirm the cause of current node issues and provides solutions. Alert Monitoring By configuring alert threshold parameters, users can receive alert notifications through email, SMS, DingTalk, and other means when nodes experience abnormal conditions and increased pressure. Data Management Whaleal provides a display box to showcase data obtained from user-customized queries, making it easy and user-friendly to visualize data. User Management Displays all roles and users in the cluster, along with detailed permissions for roles and users. Node Management Users can use Whaleal to easily add nodes to Replica Set clusters and Sharded clusters/shard/config. This helps avoid potential failures due to incorrect command-line inputs. Authentication Management Users can enable cluster authentication with a single click through Whaleal. Whaleal restarts the cluster in a rolling manner, ensuring minimal service disruption. Version Changes Whaleal offers rolling upgrade and downgrade operations, enabling version changes between adjacent versions of clusters without affecting service usage. Architecture Changes Whaleal provides the ability to change the architecture from Standalone to Replica Set. "},"whalelaPlatform/06-FAQ/ForZabbixUser.html":{"url":"whalelaPlatform/06-FAQ/ForZabbixUser.html","title":"ForZabbixUser","keywords":"","body":"For Zabbix User Does Whaleal support creating MongoDB clusters? Whaleal supports creating MongoDB clusters with various configurations: Create Standalone: CreateStandalone Create Replica Set Cluster: CreateReplicaSet Create Sharded Cluster: CreateShardedCluster Does Whaleal support managing existing MongoDB clusters? Whaleal supports monitoring and managing existing MongoDB clusters. You can use ExistingMongoDBDeployment to add monitoring and management capabilities to an existing MongoDB cluster. Whaleal supports discovering all nodes in a cluster through a single node configuration and monitors them. Does Whaleal support operations on MongoDB clusters? Whaleal provides users with common operations used in usage and maintenance processes, enabling users to make changes to clusters through configuration and clicks on the platform interface. What MongoDB operations does Whaleal provide? Diagnostic Analysis By combining real-time diagnostic data such as Top, Op, and Explain, Whaleal helps confirm the cause of current node issues and provides solutions. Alert Monitoring By configuring alert threshold parameters, users can receive alert notifications through email, SMS, DingTalk, and other means when nodes experience abnormal conditions and increased pressure. Data Management Whaleal provides a display box to showcase data obtained from user-customized queries, making it easy and user-friendly to visualize data. User Management Displays all roles and users in the cluster, along with detailed permissions for roles and users. Node Management Users can use Whaleal to easily add nodes to Replica Set clusters and Sharded clusters/shard/config. This helps avoid potential failures due to incorrect command-line inputs. Authentication Management Users can enable cluster authentication with a single click through Whaleal. Whaleal restarts the cluster in a rolling manner, ensuring minimal service disruption. Version Changes Whaleal offers rolling upgrade and downgrade operations, enabling version changes between adjacent versions of clusters without affecting service usage. Architecture Changes Whaleal provides the ability to change the architecture from Standalone to Replica Set. "},"whalelaPlatform/06-FAQ/QA.html":{"url":"whalelaPlatform/06-FAQ/QA.html","title":"QA","keywords":"","body":"Frequently Asked Questions and Answers Which operating systems are supported by Whaleal platform? Currently, the platform supports CentOS 6, CentOS 7, and CentOS 8. Other operating systems are under development. Which databases are supported by the Whaleal platform? Currently, only MongoDB is supported. Other databases are under development. Can I reset my password? Regular users cannot reset their passwords. You need to contact an administrator to reset your password. How do I add a new host? For details on adding a new host, refer to AddHost. How do I create a cluster? For details on creating a cluster, refer to the following links: Create Standalone: CreateStandalone Create Replica Set: CreateReplicaSet Create Sharded Cluster: CreateShardedCluster What does an alarm condition mean? An alarm condition refers to setting threshold values for CPU, memory, swap, disk, and bandwidth based on your needs. When these thresholds are triggered, abnormal conditions are sent to administrator users. After configuring alarm information, how do I receive alarms? Once an alarm condition is configured and triggered, alarm notifications will be sent through email, DingTalk, SMS, and other methods. I configured alarm information and notifications, but I didn't receive any alarms. In the user settings, there's an option to configure whether to receive alarm notifications. Make sure this option is turned on. Do MongoDB nodes support synchronization? Currently not supported; under development. What MongoDB authentication methods are supported? Currently, only username and password authentication is supported. When I remove a node from management, is it shut down? When a cluster is removed from management, it is no longer managed and displayed on this platform, but it is not shut down on the host. Deleting a node involves shutting it down. What should I do if adding a shard fails? Manually check the MongoDB logs and investigate the error message to identify the cause. "},"whalelaPlatform/07-APIReference/Agent.html":{"url":"whalelaPlatform/07-APIReference/Agent.html","title":"Agent","keywords":"","body":"Agent Interface When making API calls, the whaleal-Token should be set in the request header, and the returned content will be in JSON format. For time-related parameters, timestamps should be used. If you need to use hostId, agentId, or eventId, you can obtain them through the following methods: hostId is obtained from the \"Search Basic Host Information by Hostname\" interface. agentId is obtained from the \"Generate agentId\" interface. eventId is found in the \"Retrieve Cluster Log Information\" interface for the required event ID. Default request header format, special cases require special declaration The whaleal-Token is returned when calling the \"Login\" interface. For subsequent API calls, include the token in the request header. Login API for obtaining whaleal-Token KEY VALUE Accept-Encoding gzip,deflate,br Connection keep-alive Content-Type application/json whaleal-token \"token\" 1 Search Basic Host Information by Hostname (Hostname and Host ID) 1.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/getAllAgentHostNameAndHostId 1.2 Request Parameters Name Located in Description Required Schema hostName Params Hostname No String 1.3 Response Description Schema code Status code: 1000 for success, others for exceptions int id Host ID String name Hostname String { \"code\": 1000, \"data\": [ { \"id\": \"62b153a344ba1b7771c42df7\", \"name\": \"server100\" }, { \"id\": \"62bbfbe9a46517610435d615\", \"name\": \"chen\" }, { \"id\": \"62cbbd7607bebb71b8429e5e\", \"name\": \"server200\" }, { \"id\": \"62d626969026c712d786e707\", \"name\": \"usdp\" } ] } 2 Get Agent Statistics 2.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/getAgentStatistics 2.2 Request 2.3 Response Description Schema code Status code: 1000 for success, others for exceptions long activeAgentNum Number of Active Agents long activeAgentCpuNum Number of Active Agent CPUs long activeAgentDiskNum Number of Active Agent Disks long deadAgentMemoryNum Number of Dead Agent Memory long deadAgentCpuNum Number of Dead Agent CPUs long activeAgentMemoryNum Number of Active Agent Memory long deadAgentNum Number of Dead Agents long deadAgentDiskNum Number of Dead Agent Disks long { \"code\": 1000, \"data\": { \"activeAgentNum\": 4, \"activeAgentCpuNum\": 88, \"activeAgentDiskNum\": 23647738, \"deadAgentMemoryNum\": 0, \"deadAgentCpuNum\": 0, \"activeAgentMemoryNum\": 273086, \"deadAgentNum\": 0, \"deadAgentDiskNum\": 0 } } 3 Get All Host Information 3.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/getAllAgentData// 3.2 Request Parameters status: true for normal, false for offline Name Located in Description Required Schema pageIndex Path Page index Yes int pageSize Path Page size Yes int hostName Params Hostname No String ip Params Host IP No String status Params Host status No boolean 3 Get All Host Information 3.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data List { \"code\": 1000, \"data\": [ { \"id\": \"62b153a344ba1b7771c42df7\", \"createTime\": 1658212423773, \"updateTime\": 1658459349919, \"hostId\": \"62b153a344ba1b7771c42df7\", \"hostName\": \"server100\", \"hostNameLong\": \"server100\", // Basic host information \"ipInfo\": [ { \"ip\": \"192.168.3.100\", \"type\": \"ipv4\" } ], \"memory\": 128722, \"osVersion\": \"CentOS Linux release 7.9.2009 (Core)\", \"cpuInfo\": \" Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz\", \"cpuNum\": 40, \"swap\": 4095, // Kernel information \"kernelInfo\": \"3.10.0-1160.24.1.el7.x86_64\", \"totalDiskSize\": 7893956, \"run\": true, // System property information \"systemPropertyInfo\": { \"javaVersion\": \"11.0.9\", \"javaVendor\": \"Oracle Corporation\", \"javaVendorUrl\": null, \"javaHome\": \"/root/jdk-11.0.9\", \"javaVmSpecificationVersion\": \"11\", \"javaVmSpecificationVendor\": null, \"javaVmSpecificationName\": \"Java Virtual Machine Specification\", \"javaVmVersion\": \"11.0.9+7-LTS\", \"javaVmVendor\": \"Oracle Corporation\", \"javaVmName\": \"Java HotSpot(TM) 64-Bit Server VM\", \"javaSpecificationVersion\": null, \"javaSpecificationVendor\": \"Oracle Corporation\", \"javaSpecificationName\": \"Java Platform API Specification\", \"javaClassVersion\": \"55.0\", \"javaClassPath\": \"agent-collection-1.0.0.jar\", \"javaLibraryPath\": \"/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib\", \"javaIoTmpdir\": \"/tmp\", \"javaCompiler\": null, \"javaExtDirs\": null, \"fileSeparator\": \"/\", \"pathSeparator\": \":\", \"lineSeparator\": \"\\n\", \"userName\": \"root\", \"userHome\": \"/root\", \"userDir\": \"/home/jmops\", \"osname\": \"Linux\", \"osarch\": \"amd64\", \"osversion\": \"3.10.0-1160.24.1.el7.x86_64\" } } ] } 4 Get All Host Count 4.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/getAllAgentCount 4.2 Request Parameters status: true for normal, false for offline Name Located in Description Required Schema hostName Params Hostname No String ip Params Host IP No String status Params Host status No boolean 4.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Count of returned hosts long { \"code\": 1000, \"data\": 1 } 5 Get Static Information of a Host 5.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/getAgentInfo/ 5.2 Request Parameters Name Located in Description Required Schema hostId Path Host ID Yes String 5.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data JSON { \"code\": 1000, \"data\": { \"id\": \"62bbfbe9a46517610435d615\", \"createTime\": 1658286068557, \"updateTime\": 1658459546253, \"hostId\": \"62bbfbe9a46517610435d615\", \"hostName\": \"chen\", \"hostNameLong\": \"chen\", \"ipInfo\": [ { \"ip\": \"192.168.3.80\", \"type\": \"ipv4\" } ], \"memory\": 7821, \"osVersion\": \"CentOS Linux release 7.7.1908 (Core)\", \"cpuInfo\": \" Intel(R) Xeon(R) CPU L5640 @ 2.27GHz\", \"cpuNum\": 4, \"swap\": 8063, \"kernelInfo\": \"3.10.0-1062.el7.x86_64\", \"totalDiskSize\": 213035, \"run\": true, \"systemPropertyInfo\": { \"javaVersion\": \"1.8.0_172\", \"javaVendor\": \"Oracle Corporation\", \"javaVendorUrl\": null, \"javaHome\": \"/home/docker20220629BAK/java/jre\", \"javaVmSpecificationVersion\": \"1.8\", \"javaVmSpecificationVendor\": null, \"javaVmSpecificationName\": \"Java Virtual Machine Specification\", \"javaVmVersion\": \"25.172-b11\", \"javaVmVendor\": \"Oracle Corporation\", \"javaVmName\": \"Java HotSpot(TM) 64-Bit Server VM\", \"javaSpecificationVersion\": null, \"javaSpecificationVendor\": \"Oracle Corporation\", \"javaSpecificationName\": \"Java Platform API Specification\", \"javaClassVersion\": \"52.0\", \"javaClassPath\": \"agent-collection-1.0.0.jar\", \"javaLibraryPath\": \"/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib\", \"javaIoTmpdir\": \"/tmp\", \"javaCompiler\": null, \"javaExtDirs\": \"/home/docker20220629BAK/java/jre/lib/ext:/usr/java/packages/lib/ext\", \"fileSeparator\": \"/\", \"pathSeparator\": \":\", \"lineSeparator\": \"\\n\", \"userName\": \"root\", \"userHome\": \"/root\", \"userDir\": \"/root\", \"osname\": \"Linux\", \"osarch\": \"amd64\", \"osversion\": \"3.10.0-1062.el7.x86_64\" } } } 6 Get Monitoring Information of an Agent 6.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/getAgentMonitor// 6.2 Request Parameters timeType: REAL_TIME, ONE_DAY, ONE_WEEK dataType: netInAndOut, memory, diskInAndOut, cpu Name Located in Description Required Schema hostId Path Host ID Yes String timeType Path Monitoring Type Yes String startTimeForTimeInterval Params Start time of a time interval Yes long endTimeForTimeInterval Params End time of a time interval Yes long timeGranularity Params Time granularity Yes long dataType Params Data type Yes String 6.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data JSON { \"code\": 1000, \"data\": { \"us\": [ 2.51, 2.29, 2.72, ], \"sy\": [ 0.89, 0.84, 1.89, ], \"id\": [ 96.57, 96.83, 94.97, ] }, \"createTime\": [ 1659512400000, 1659512460000, 1659512520000 ], \"name\": \"cpu\", \"message\": { \"id\": \"Idle CPU Rate (Percentage)\", \"us\": \"User CPU Usage Rate (Percentage)\", \"sy\": \"System CPU Usage Rate (Percentage)\" }, \"info\": { \"id\": { \"max\": \"96.83\", \"min\": \"86.81\", \"avg\": \"94.29\" }, \"us\": { \"max\": \"9.58\", \"min\": \"1.72\", \"avg\": \"3.00\" }, \"sy\": { \"max\": \"3.25\", \"min\": \"0.84\", \"avg\": \"2.08\" } } } 7 Get Agent Log Information with Paginated Display 7.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/logData/// 7.2 Request Parameters type: info, warn, trace, error, mongodb Name Located in Description Required Schema hostId Path Host ID Yes String pageIndex Path Page index Yes int pageSize Path Page size Yes int type Params Log type No String startTime Params Start time No long endTime Params End time No long content Params Content No String 7.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data List { \"code\": 1000, \"data\": [ { \"id\": \"62c418a8e945184b27fae4c6\", \"createTime\": 1657018536725, \"updateTime\": 0, \"hostId\": \"62b153a344ba1b7771c42df7\", \"type\": \"info\", \"content\": \" [MongodbRealTimeData.run-94] server100:20190 started monitoring\" } ] } 8 Get the Number of Agent Log Entries 8.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/logCount/ 8.2 Request Parameters type: info, warn, trace, error, mongodb Name Located in Description Required Schema hostId Path Host ID Yes String type Params Log type No String startTime Params Start time No long endTime Params End time No long content Params Keyword No String 8.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Number of log entries long 9 Execute Commands on Agent 9.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/operate// 9.2 Request Parameters operateType: delete, updateAgentInfo Name Located in Description Required Schema hostId Path Host ID Yes String operateType Path Operation type Yes String 9.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Returned message String 10 Generate Agent ID 10.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/generateAgentId 10.2 Request 10.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data: agentId String 11 Download Agent File 11.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/downAgentFile//agent-collection-1.0.0.jar 11.2 Request Parameters Name Located in Description Required Schema agentId Path Agent ID Yes String 11.3 Response Description Schema File File in binary stream form File 12 Get Agent Command Execution Records 12.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/getExecCommandDataList/// 12.2 Request Parameters Status types: -1 for all, 1 for issued, 2 for executing, 3 for successful completion, 4 for exception during execution, 5 for exception completion Name Located in Description Required Schema hostId Path Host ID Yes String pageIndex Path Page index Yes int pageSize Path Page size Yes int status Params Status No Int startTime Params Start time No long endTime Params End time No long content Params Content No String result Params Result No String eventId Params Event ID No String 12.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data List { \"code\": 1000, \"data\": [ { \"id\": \"62c51e6ad6ea982573f41e4d\", \"createTime\": 1657085546634, \"updateTime\": 1657085549086, \"hostId\": \"62b153a344ba1b7771c42df7\", \"commandType\": 221, \"status\": 3, \"eventId\": \"62c51e6ad6ea982573f41e4c\", \"commandNote\": \"server100:20190获取集群角色\", \"content\": \"{}\", \"execResult\": \"已完成\" } ] } 13 Get the Number of Agent Command Execution Records 13.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/getExecCommandDataCount/ 13.2 Request Parameters Status types: -1 for all, 1 for issued, 2 for executing, 3 for successful completion, 4 for exception during execution, 5 for exception completion Name Located in Description Required Schema hostId Path Host ID Yes String Status Params Status No int startTime Params Start time No long endTime Params End time No long content Params Command type No String result Params Result No String eventId Params Event ID No String 13.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Number of records long 14 Get Host CPU Usage 14.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/getHost/CpuUsage/ 14.2 Request Parameters Name Located in Description Required Schema count Path Number of records to fetch Yes int beginTime Params Start time Yes long endTime Params End time Yes long 14.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data List { \"code\": 1000, \"data\": [ { \"_id\": \"630ddf510901ea6464159609\", \"hostId\": \"630ddf510901ea6464159609\", \"hostName\": \"server190\", \"usage\": 100.0 }, { \"_id\": \"6305fa4491c2f64abf18c581\", \"hostId\": \"6305fa4491c2f64abf18c581\", \"hostName\": \"server100\", \"usage\": 100.0 }, { \"_id\": \"631837b0e8f4ff5c079e9c55\", \"hostId\": \"631837b0e8f4ff5c079e9c55\", \"hostName\": \"server14\", \"usage\": 100.0 }, { \"_id\": \"6316dbf322197b14b79a4793\", \"hostId\": \"6316dbf322197b14b79a4793\", \"hostName\": \"server84\", \"usage\": 100.0 }, { \"_id\": \"630eddeff3d9e72e3695ea48\", \"hostId\": \"630eddeff3d9e72e3695ea48\", \"hostName\": \"chen\", \"usage\": 100.0 } ] } 15 Get Host Memory Usage 15.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/getHost/MemUsage/ 15.2 Request Parameters Name Located in Description Required Schema count Path Number of records to fetch Yes int beginTime Params Start time Yes long endTime Params End time Yes long 15.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data List { \"code\": 1000, \"data\": [ { \"_id\": \"63031ffab652427a5bb8a667\", \"hostId\": \"63031ffab652427a5bb8a667\", \"usage\": \"58.68GB\", \"hostName\": \"server200\" }, { \"_id\": \"6305fa4491c2f64abf18c581\", \"hostId\": \"6305fa4491c2f64abf18c581\", \"usage\": \"44.73GB\", \"hostName\": \"server100\" }, { \"_id\": \"630eddeff3d9e72e3695ea48\", \"hostId\": \"630eddeff3d9e72e3695ea48\", \"usage\": \"32.17GB\", \"hostName\": \"chen\" } ] } 16 Get Host Disk Usage 16.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/getHost/DiskUsage/ 16.2 Request Parameters | Name | Located in | Description | Required | Schema | | -------------------|----------------------|-------------------------------|-----------------|----------- | | count | Path | Number of records to fetch | Yes |int | | beginTime | Params | Start time | Yes |long | | endTime | Params | End time | Yes |long | 16.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data List { \"code\": 1000, \"data\": [ { \"_id\": \"631837b0e8f4ff5c079e9c55\", \"hostId\": \"631837b0e8f4ff5c079e9c55\", \"hostName\": \"server14\", \"usage\": 22.81 }, { \"_id\": \"63031ffab652427a5bb8a667\", \"hostId\": \"63031ffab652427a5bb8a667\", \"hostName\": \"server200\", \"usage\": 10.01 }, { \"_id\": \"6305fa4491c2f64abf18c581\", \"hostId\": \"6305fa4491c2f64abf18c581\", \"hostName\": \"server100\", \"usage\": 9.49 } ] } 17 Get Network Card Input Usage 17.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/getHost/NetIn/ 17.2 Request Parameters Name Located in Description Required Schema count Path Number of records to fetch Yes int beginTime Params Start time Yes long endTime Params End time Yes long 17.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data List { \"code\": 1000, \"data\": [ { \"_id\": \"63031ffab652427a5bb8a667\", \"usage\": \"121.42MB/s\", \"hostName\": \"server200\" }, { \"_id\": \"630eddeff3d9e72e3695ea48\", \"usage\": \"81.28MB/s\", \"hostName\": \"chen\" }, { \"_id\": \"6316dbf322197b14b79a4793\", \"usage\": \"78.8MB/s\", \"hostName\": \"server84\" } ] } 18 Get Network Card Output Usage 18.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/getHost/NetOut/ 18.2 Request Parameters Name Located in Description Required Schema count Path Number of records to fetch Yes int beginTime Params Start time Yes long endTime Params End time Yes long 18.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data List { \"code\": 1000, \"data\": [ { \"_id\": \"63031ffab652427a5bb8a667\", \"usage\": \"121.42MB/s\", \"hostName\": \"server200\" }, { \"_id\": \"6322b50f0b810f7b5109403c\", \"usage\": \"6.39MB/s\", \"hostName\": \"server202\" }, { \"_id\": \"630eddeff3d9e72e3695ea48\", \"usage\": \"2.61MB/s\", \"hostName\": \"chen\" } ] } --- "},"whalelaPlatform/07-APIReference/Alert.html":{"url":"whalelaPlatform/07-APIReference/Alert.html","title":"Alert","keywords":"","body":"Alert API When calling the API, you need to set the whaleal-Token in the request header, and the response content will be in JSON format. All time-related parameters are expected to be in the form of timestamps. The API requires the usage of hostId and objectId parameters. hostId is obtained from the \"Get Host Basic Information by Fuzzy Hostname\" API. objectId can be either the host ID or the MongoDB node ID. The MongoDB node ID can be found in the data collection of the result returned by the \"Retrieve MongoDB Cluster Information Data\" API under the mongo collection's \"id\". Default Request Header Format KEY VALUE Accept-Encoding gzip,deflate,br Connection keep-alive Content-Type application/json 1 Verify the Correctness of Alert Messages 1.1 Request Path POST: http://{Server-Host}:{Port}/api/alert/judgeAlertMsg 1.2 Request Parameters Name Located in Description Required Schema alertMsgEntity Body Alert message entity Yes AlertMsgEntity Example: Verify the correctness of an alert message. The AlertMsgEntity is as follows: { \"alertStrategyId\": \"62fa15c51bf5144438e5290f\", \"createTime\": 1660556741231, \"endTime\": 1660556741231, \"id\": \"62fa15c51bf5144438e5290d\", \"msg\": \"test_alert\", \"objectId\": \"62fa15c51bf5144438e5290e\", \"objectType\": 1, \"startTime\": 1660556741231, \"updateTime\": 1660556741231 } 1.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned message String 2 Get Alert Strategies 2.1 Request Path GET: http://{Server-Host}:{Port}/api/alert/getAlertStrategy 2.2 Request Parameters Type: 1 for agent, 2 for mongo Name Located in Description Required Schema objectId Params Object ID Yes String type Params Type of object Yes int 2.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data JSON 3 Get All Member Alert Strategies (Get All MongoDB Node Alert Strategies on a Single Agent) 3.1 Request Path GET: http://{Server-Host}:{Port}/api/alert/getAllMongoMemberAlertStrategy 3.2 Request Parameters Name Located in Description Required Schema hostId Params Host ID Yes String 3.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data List { \"code\": 1000, \"data\": [ { \"id\": \"62fa1679266fb301295fd555\", \"createTime\": 1660556921496, \"updateTime\": 1660556923891, \"name\": \"\", \"objectId\": \"62f5bf10c329264bb2d6deb1\", \"type\": 2, \"timeFrequencyStrategyList\": [], \"continuousGranularityStrategyList\": [ { \"type\": \"qps_insert\", \"cmp\": \">\", \"value\": 80.0, \"count\": 20, \"alarmFrequency\": 30, \"duration\": 60 } ] } ] } 4 Update Alert Information 4.1 Request Path POST: http://{Server-Host}:{Port}/api/alert/update 4.2 Request Parameters Name Located in Description Required Schema alertStrategyEntity Body Alert strategy entity Yes alertStrategyEntity 4.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data JSON 5 Send Alert Messages 5.1 Request Path GET: http://{Server-Host}:{Port}/api/alert/sendAlertMsg 5.2 Request Parameters type: 1 for host, 2 for mongo Name Located in Description Required Schema objectId Params ID of host or mongo Yes String msg Params Notification message Yes String type Params Notification type Yes int 5.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Returned message String "},"whalelaPlatform/07-APIReference/Collection.html":{"url":"whalelaPlatform/07-APIReference/Collection.html","title":"Collection","keywords":"","body":"Collection API When calling the API, you need to set the agentId in the request header, and the response content will be in JSON format. All time-related parameters are expected to be in the form of timestamps. In some cases, you will need to use hostId, agentId, clusterId, and eventId as parameters: hostId is obtained from the \"Retrieve Basic Host Information by Hostname\" API. agentId is obtained from the \"Generate Agent ID\" API. eventId is found in the \"Get Cluster Log Information\" API. clusterId is obtained from the \"Retrieve MongoDB Cluster Information\" API. Default Request Header Format The agentId is obtained from the \"Generate Agent ID\" API. KEY VALUE Accept-Encoding gzip,deflate,br Connection keep-alive Content-Type application/json agentId \"agentId\" 1 Save Agent's Log Record 1.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/host/save/log 1.2 Request Parameters Name Located in Description Required Schema agentLogEntity Body Log information entity Yes AgentLogEntity 1.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String 2 Update Agent's MongoDB File Information 2.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/host/updateAgentMongoFile/ 2.2 Request Parameters Name Located in Description Required Schema agentId Path Agent ID Yes String mongoFileList Body List of MongoDB files Yes List Ex. Update Agent's MongoDB File Information; where MongoFileList is shown below: [ { \"_id\": \"62d62a9bbfa6b71dad85b68a\", \"createTime\": \"1658202779363\", \"hostId\": \"62b153a344ba1b7771c42df7\", \"md5\": \"1\", \"name\": \"mongodb-linux-x86_64-enterprise-rhel70-4.4.14.tgz\", \"path\": \"/var/ops/agent/mongodb-linux-x86_64-enterprise-rhel70-4.4.14.tgz\", \"server\": false, \"shortName\": \"mongodb-linux-x86_64-enterprise-rhel70-4.4.14\", \"size\": 133646249, \"updateTime\": \"1658202779363\" } ] 2.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String 3 Query Pending Commands for an Agent 3.1 Request Path GET: http://{Server-Host}:{Port}/api/collection/command/getCommand/ 3.2 Request Parameters Name Located in Description Required Schema hostId Path Host ID Yes String 3.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data List { \"code\": 1000, \"data\": [ { \"id\": \"632bfca83b74be1d9fe7ddb7\", \"createTime\": 1663827112988, \"updateTime\": 1663827112988, \"hostId\": \"630eddeff3d9e72e3695ea48\", \"commandType\": 101, \"status\": 0, \"eventId\": null, \"commandNote\": \"updateHostInfo\", \"content\": \"\\\"updateHostInfo\\\"\", \"execResult\": null } ] } 4 Update Status of Command Entity 4.1 Request Path PUT: http://{Server-Host}:{Port}/api/collection/command/update 4.2 Request Parameters Name Located in Description Required Schema commandEntity Body Command entity Yes | CommandEntity Ex. Update the status of the command entity; where CommandEntity is shown below: { \"id\": \"62c54a395dc04d3d4c13be75\", \"commandNote\": \"server100:20190获取集群角色\", \"commandType\": 221, \"content\": \"{}\", \"createTime\": \"1657096761802\", \"execResult\": \"已完成\", \"hostId\": \"62b153a344ba1b7771c42df7\", \"status\": 3, \"updateTime\": \"1657096769089\" } 4.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Response message String 5 Update Agent Heartbeat Information 5.1 Request Path GET: http://{Server-Host}:{Port}/api/collection/host/updateRunTime// 5.2 Request Parameters Name Located in Description Required Schema hostId Path Host ID Yes String timeStamp Path Timestamp Yes String 5.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String 6 Save Host Information 6.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/hostInfo 6.2 Request Parameters Name Located in Description Required Schema hostInfoMongoEntity Body Host information entity Yes HostInfoMongoEntity Ex. Save host information; where HostInfoMongoEntity is shown below: { \"_id\": \"62cbbd7607bebb71b8429e5e\", \"cpuInfo\": \" Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz\", \"cpuNum\": 40, \"hostId\": \"62cbbd7607bebb71b8429e5e\", \"hostName\": \"server200\", \"hostNameLong\": \"server200\", \"ipInfo\": [ { \"ip\": \"172.17.0.1\", \"type\": \"ipv4\" } ], \"kernelInfo\": \"3.10.0-1062.el7.x86_64\", \"osVersion\": \"CentOS Linux release 7.7.1908 (Core)\", \"run\": true, \"systemPropertyInfo\": { \"fileSeparator\": \"/\", \"javaClassPath\": \"agent-collection-1.0.0.jar\", \"javaClassVersion\": \"55.0\", \"javaHome\": \"/root/jdk-11.0.9\", \"javaIoTmpdir\": \"/tmp\", \"javaLibraryPath\": \"/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib\", \"javaSpecificationName\": \"Java Platform API Specification\", \"javaSpecificationVendor\": \"Oracle Corporation\", \"javaVendor\": \"Oracle Corporation\", \"javaVersion\": \"11.0.9\", \"javaVmName\": \"Java HotSpot(TM) 64-Bit Server VM\", \"javaVmSpecificationName\": \"Java Virtual Machine Specification\", \"javaVmSpecificationVersion\": \"11\", \"javaVmVendor\": \"Oracle Corporation\", \"javaVmVersion\": \"11.0.9+7-LTS\", \"lineSeparator\": \"\\n\", \"oSArch\": \"amd64\", \"oSName\": \"Linux\", \"oSVersion\": \"3.10.0-1062.el7.x86_64\", \"pathSeparator\": \":\", \"userDir\": \"/home/jmops\", \"userHome\": \"/root\", \"userName\": \"root\" } } 6.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String 7 Save Real-Time Host Information 7.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/host/addHostRealTimeData 7.2 Request Parameters Name Located in Description Required Schema hostRealTimeDataMongoEntity Body Real-time host information entity Yes HostRealTimeDataMongoEntity Ex. Save real-time host information; where hostRealTimeDataMongoEntity is shown below: { \"_id\": \"62c64f99f9872b46f1ce953a\", \"cpuInfo\": { \"hi\": 0, \"id\": 98.1, \"ni\": 0, \"si\": 0, \"st\": 0, \"sy\": 0.9, \"us\": 1, \"wa\": 0 }, \"createTime\": \"1657163672000\", \"diskInAndOutInfoList\": [ { \"avgqu_sz\": 0.05, \"avgrq_sz\": 18.89, \"await\": 0.34, \"device\": \"sda\", \"r_await\": 13.32, \"r_s\": 0.17, \"rkB_s\": 10.82, \"rrqm_s\": 0. 06, \"svctm\": 0.05, \"util\": 0.75, \"w_await\": 0.32, \"w_s\": 144.65, \"wkB_s\": 1357.06, \"wrqm_s\": 2.03 } ], \"diskInfoList\": [ { \"fileSystem\": \"devtmpfs\", \"mountedOn\": \"/dev\", \"size\": 64349, \"type\": \"devtmpfs\", \"used\": 0, \"utilization\": 0 } ], \"hostId\": \"62b153a344ba1b7771c42df7\", \"hostName\": \"server100\", \"memoryInfo\": { \"memAvail\": 95150, \"memBuffCache\": 26138, \"memFree\": 69641, \"memTotal\": 128722, \"memUsed\": 32942, \"swapFree\": 1958, \"swapTotal\": 4095, \"swapUsed\": 2137 }, \"netInAndOutInfoList\": [ { \"io\": 0, \"networkCardName\": \"em3:\", \"out\": 0 } ], \"timeGranularity\": 1, \"updateTime\": \"1657163672000\" } 7.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Response message String 8 Agent Calls to Get Server Time 8.1 Request Path GET: http://{Server-Host}:{Port}/api/collection/util/get/server/date 8.2 Request 8.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Timestamp long 9 Agent Retrieves its IP Address 9.1 Request Path GET: http://{Server-Host}:{Port}/api/collection/util/get/agent/ip 9.2 Request 9.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data IP address String 10 Logging to MongoDB 10.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/mongodb/insertMongoClusterLog// 10.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String eventId Path Event ID Yes String logList Body List of logs Yes List 10.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Response message String 11 Insert MongoDB Member Log 11.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/mongodb/insertMongoMemberLog/// 11.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String memberInfo Path Member Info Yes String eventId Path Event ID Yes String logList Body Log List Yes List 11.3 Response Description Schema code Status Code: 1000 for success, others for exceptions int data Response Message String 12 Update MongoDB Node Information 12.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/mongodb/updateMongoMember 12.2 Request Parameters Name Located in Description Required Schema mongoMember Body MongoDB Cluster Member Yes MongoMember Example: Update MongoDB node information; where MongoMember is as follows: { \"id\": \"62f76749e011b442d7c91ec6\", \"createTime\": 0, \"updateTime\": 1660466332000, ... \"operateVersion\": 5723 } 12.3 Response Description Schema code Status Code: 1000 for success, others for exceptions int data Response Message String 13 Update Replica Set Information 13.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/mongodb/updateMongoRepl/ 13.2 Request Parameters Name Located in Description Required Schema isUpdateMemberList Path Update Member List Yes boolean mongoReplica Body MongoDB Replica Set Yes MongoReplica 13.3 Response Description Schema code Status Code: 1000 for success, others for exceptions int data Response Message String 14 Update Cluster Information 14.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/mongodb/updateCluster 14.2 Request Parameters Name Located in Description Required Schema mongoClusterInformation Body MongoDB Cluster Information Yes MongoClusterInformation 14.3 Response Description Schema code Status Code: 1000 for success, others for exceptions int data Response Message String 15 Save Real-Time Information of MongoDB Members 15.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/mongodb/realtime 15.2 Request Parameters Name Located in Description Required Schema tableName Path Table Name Yes String mongodbNodeMetrics Body MongoDB Real-Time Data Yes MongodbNodeMetrics Example: Save real-time information of a MongoDB member; where MongodbNodeMetrics is as follows: { \"anAssert\": { \"msg\": 0, \"regular\": 0, \"user\": 0, \"warning\": 0 }, \"cacheFlow\": { \"brin\": 8717624, \"bwfr\": 6421369 } \"createTime\": \"1660469450000\", ... \"operateVersion\": 8168 } 15.3 Response Description Schema code Status Code: 1000 for success, others for exceptions int data File Offset long Deprecated (No Longer Used) 16 Save Batch of Monitoring Data to Database 16.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/mongodb/realtime/save/many 16.2 Request Parameters Name Located in Description Required Schema mongoDBRealtimeDataEntityList Body List of Real-Time Data Yes List ![img_12.png](../../../images/whalealPlatformImages//realtime_save_many.png ) 16.3 Response Description Schema code Status Code: 1000 for success, others for exceptions int data Response Message String 17 Get MongoDB Node Information on Agent Instance 17.1 Request Path POST http://{Server-Host}:{Port}/api/collection/mongodb/getAgentMongoMember/ 17.2 Request Parameters Name Located in Description Required Schema agentId Path Agent ID Yes String 17.3 Response Description Schema code Status Code: 1000 for success, others for exceptions int data Returned Data List { \"code\": 1000, \"data\": [ { \"id\": \"62d6506ec5b6206027b99052\", \"createTime\": 1658212462005, \"updateTime\": 1658302192001, ... \"operateVersion\": 8168 } ] } 18 Save mongo.log Log 18.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/mongodb/save/mongoLog// 18.2 Request Parameters Name Located in Description Required Schema mongoMemberId Path MongoDB member ID Yes String fileOffset Path File offset Yes long logList Body Log list Yes List 18.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data File offset long 19 Save Mongo Top and Op 19.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/mongodb/save/mongoTopAndOp 19.2 Request Parameters Name Located in Description Required Schema documentList Body Document list Yes List 19.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Response message String 20 Update FCV 20.1 Request Path GET: http://{Server-Host}:{Port}/api/collection/mongodb/updateFCV// 20.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String fcv Path FCV Yes String 20.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Response message String 21 Save MongoDB Collections 21.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/mongodb/saveMongoDBCollections 21.2 Request Parameters Name Located in Description Required Schema mongoDBCollections Body MongoDB actual collections Yes MongoDBCollections Example: Save MongoDB collections; where MongoDBCollections is as follows: { \"_id\" : \"62ea1db298c0825187aee96e\", \"clusterId\" : \"62ea1db298c0825187aee96e\", \"createTime\" : \"1659686288006\", \"dbTables\" : [ { \"name\" : \"fs.files\", \"type\" : \"collection\", \"options\" : { }, \"info\" : { \"readOnly\" : false, \"uuid\" : { \"type\" : 4, \"data\" : \"q/X3q+2aQVC9dGCnS4wKZA==\" } }, \"idIndex\" : { \"v\" : 2, \"key\" : { \"_id\" : 1 }, \"name\" : \"_id_\", \"ns\" : \"record.fs.files\" }, \"storageSize\" : 20, \"size\" : 16, \"ns\" : \"record.fs.files\" } ], \"fromServerExe\" : false, \"updateTime\" : 0 } 21.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Response message String 22 Save MongoDB Cluster User 22.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/mongodb/saveMongoDBClusterUser/ 22.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String list Body Document list Yes List Example: Save MongoDB cluster users; where List is as follows: [{ \"name\" : \"hostRealTimeDataMongoEntity\", \"type\" : \"collection\", \"options\" : { }, \"storageSize\" : 8836, \"size\" : 44721, \"ns\" : \"ops.hostRealTimeDataMongoEntity\", \"indexSizes\" : { \"_id_\" : 248, \"createTime_1\" : 152, \"hostId_1\" : 84, \"hostId_1_createTime_1_timeGranularity_1\" : 172 } }] 22.3 Response Description Schema |----------------------|--------------------------- | code | Status code: 1000 for success, others for exceptions | int || data | Response message | String | 23 Save MongoDB Cluster Role 23.1 Request Path POST: http://{Server-Host}:{Port}/api/collection/mongodb/saveMongoDBClusterRole/ 23.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String list Body Document list Yes List 23.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Response message String 24 Save Diagnostic Data. 24.1 Request Path POST: http://{Server-Host}:9601/api/collection/mdiag/saveMdiagLog 24.2 Request Parameters Name Located in Description Required Schema document Body Diagnostic log Yes Document 24.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Response message String 24 Get Config Information. 24.1 Request Path GET http://{Server-Host}:9601/api/collection/config/getConfig 24.2 Request Parameters 24.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Return data ConfigEntity --- "},"whalelaPlatform/07-APIReference/MongoDbData.html":{"url":"whalelaPlatform/07-APIReference/MongoDbData.html","title":"DBData","keywords":"","body":"MongoDBData接口 When making API calls, it is necessary to set the whaleal-Token in the request header and provide the required parameters to initiate the request. The response content will be in JSON format, and special entity classes for the response will be provided in the entity class table at the end. For parameters related to time, use timestamp format. For some API calls, you need to use nodeId, mongoMemberId, clusterId, and eventId: nodeId is the same as mongoMemberId, found in the data collection of the result set returned by the \"Retrieve MongoDB Cluster Information Data\" API. eventId can be found in the \"Retrieve Cluster Log Information\" API for the desired event. clusterId is present in the result set returned by the \"Retrieve MongoDB Cluster Information Data\" API. Default Request Header Format, Special Cases Require Special Declaration The whaleal-Token is returned when calling the login API. It should be placed in the request header when making subsequent API calls. Call the Login API to Obtain whaleal-Token KEY VALUE Accept-Encoding gzip, deflate, br Connection keep-alive Content-Type application/json whaleal-token \"token\" --- 1 Retrieve MongoDB Cluster Information Deprecated 已弃用 1.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/monitor/project/data// 1.2 Request Parameters Name Located in Description Required Schema clusterName Path Cluster Name Yes String projectType Path Type Yes String 2 Retrieve Top Five Cluster Sizes 2.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/monitor/cluster/size/top/five 2.2 Request Parameters Name Located in Description Required Schema beginTime Params Start Time Yes long endTime Params End Time Yes long 2.3 Response Description Schema code Status Code: 1000 for success, others for errors int data Response Data List { \"code\": 1000, \"data\": [ { \"_id\": \"62d666c50f57845ee4c76090\", \"clusterSize\": 0, \"size\": \"0.00KB\", \"clusterName\": \"test_repl\" }, { \"_id\": \"62d65068561b4a25b8339740\", \"clusterSize\": 0, \"size\": \"0.00KB\", \"clusterName\": \"shard\" } ] } 3 Retrieve Top Five Collection Sizes 3.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/monitor/collection/size/top/five 3.2 Request Parameters Name Located in Description Required Schema beginTime Params Start Time Yes long endTime Params End Time Yes long 3.3 Response Description Schema code Status Code: 1000 for success, others for errors int data Response Data List { \"code\": 1000, \"data\": [ { \"_id\": \"62d67d21239d00094230b08f\", \"clusterId\": \"62d67d21239d00094230b08f\", \"createTime\": 1658394516783, \"dbTables\": { \"name\": \"fs.chunks\", \"type\": \"collection\", \"options\": {}, \"info\": { \"readOnly\": false, \"uuid\": { \"type\": 4, \"data\": \"8MfjmDBFR5q9BYztGFDJQQ==\" } }, \"idIndex\": { \"v\": 2, \"key\": { \"_id\": 1 }, \"name\": \"_id_\", \"ns\": \"test.testColl\" }, \"storageSize\": 20, \"size\": 0, \"ns\": \"test.testColl\" }, \"fromServerExe\": false, \"updateTime\": 0, \"clusterName\": \"shard\", \"dbName\": \"test\", \"collectionName\": \"testColl\", \"size\": \"0.00KB\" } ] } 4 Retrieve Top Five QPS 4.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/monitor/QPS/size/top/five 4.2 Request Parameters Name Located in Description Required Schema beginTime Params Start Time Yes long endTime Params End Time Yes long 4.3 Response Description Schema code Status Code: 1000 for success, others for errors int data Response Data List { \"code\": 1000, \"data\": [ { \"_id\": { \"hostId\": \"62cbbd7607bebb71b8429e5e\", \"port\": \"47018\" }, \"host\": \"server200\", \"port\": \"47018\", \"QPS\": 5520, \"instance\": \"server200:47018\" } ] } 5 Retrieve Top Five Connection Instances 5.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/monitor/connection/instance/top/five 5.2 Request Parameters Name Located in Description Required Schema beginTime Params Start Time Yes long endTime Params End Time Yes long 5.3 Response Description Schema code Status Code: 1000 for success, others for errors int data Response Data List { \"code\": 1000, \"data\": [ { \"_id\": { \"hostId\": \"62cbbd7607bebb71b8429e5e\", \"port\": \"47018\" }, \"host\": \"server200\", \"port\": \"47018\", \"Conn\": 76, \"instance\": \"server200:47018\" } ] } 6 Retrieve Top Five Slow Queries 6.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/monitor/slowest/instance/top/five 6.2 Request Parameters Name Located in Description Required Schema beginTime Params Start Time Yes long endTime Params End Time Yes long 6.3 Response Description Schema code Status Code: 1000 for success, others for errors int data Response Data List { \"code\": 1000, \"data\": [ { \"_id\": \"62d66d3cc5b6206027b993b0\", \"slow count\": 8, \"instance\": \"server200:47018\" } ] } 7 Retrieve Node Real-time Monitoring Information 7.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/monitor/data// 7.2 Request Parameters timeType: REAL_TIME, ONE_DAY, ONE_WEEK dataType: qps, conn, pageFaults, memory, net, anAssert, cacheFlow, cacheUsage, latency, tickets, targetQ, scanAndOrder, collectionScan, documentOp, lockCondition, databaseLock, collectionLock, transactionCondition, deletedDocument Name Located in Description Required Schema nodeId Path Node ID Yes String timeType Path Query Time Type Yes String timeGranularity Params Time Granularity No long startTimeForTimeInterval Params Start Time Interval No long endTimeForTimeInterval Params End Time Interval No long dataType Params Data Type Yes long 7.3 Response Description Schema code Status Code: 1000 for success, others for errors int data Response Data List { \"code\": 1000, \"data\": { \"delete\": [ 0.0, 0.0, 0.0 ], \"insert\": [ 8.0, 15.0, 2.0 ], \"query\": [ 0.0, 0.0, 0.0 ], \"cmd\": [ 6.0, 5.0, 3.0 ], \"getMore\": [ 1.0, 2.0, 2.0 ], \"update\": [ 0.0, 0.0, 0.0 ] }, \"createTime\": [ 1659511920000, 1659511980000, 1659512040000 ], \"name\": \"qps\", \"message\": { \"insert\": \"The average rate of inserts performed per second over the selected sample period\", \"delete\": \"The average rate of deletes performed per second over the selected sample period\", \"update\": \"The average rate of updates performed per second over the selected sample period\", \"query\": \"The average rate of queries performed per second over the selected sample period\", \"command\": \"The average rate of commands performed per second over the selected sample period\", \"getMore\": \"The average rate of getMores performed per second on any cursor over the selected sample period. On a primary, this number can be high even if the query count is low as the secondaries \\\"getMore\\\" from the primary often as part of replication.\" }, \"info\": { \"delete\": { \"max\": 10, \"min\": 0, \"avg\": \"0.35\" }, \"insert\": { \"max\": 32, \"min\": 0, \"avg\": \"8.75\" }, \"query\": { \"max\": 0, \"min\": 0, \"avg\": \"0.01\" }, \"cmd\": { \"max\": 10, \"min\": 1, \"avg\": \"4.42\" }, \"getMore\": { \"max\": 2, \"min\": 0, \"avg\": \"0.93\" }, \"update\": { \"max\": 0, \"min\": 0, \"avg\": \"0.05\" } } } 8 Query Cluster Information by ID 8.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMongoCluster/ 8.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String 8.3 Response Description Schema code Status: 1000 for success, other values for exceptions int data Response Data JSON { \"code\": 1000, \"data\": { \"id\": \"62d67d21239d00094230b08f\", \"createTime\": 1658223967052, \"updateTime\": 1658223967052, \"clusterName\": \"test\", \"type\": 2, \"mongoMember\": null, \"mongoReplica\": { \"id\": \"62d67d21239d00094230b08f\", \"createTime\": 0, \"updateTime\": 0, \"replicaName\": \"test\", \"memberList\": [ // Node information ... ], \"type\": 1, //1: Single node, 2: Replica set, 3: Sharded \"clusterId\": \"62d67d21239d00094230b08f\", \"deleteDataAndLogAble\": false, \"status\": \"Running\", \"operaLog\": [], \"replicationSettings\": {}, \"replicationOtherSettings\": { \"securityKeyFileValue\": }, \"authAble\": true, \"userName\": \"root\", \"password\": \"123456\", \"authDbName\": \"admin\", \"protocolVersion\": 1, \"writeConcernMajorityJournalDefault\": false }, \"mongoShard\": null, \"status\": \"Normal\", \"fcv\": \"4.2\", \"tag\": \"ys\", \"create\": true } } 9 Get Cluster Log Information 9.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMongoClusterLogData/// 9.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String pageIndex Path Page Index Yes int pageSize Path Page Size Yes String memberName Params Node Name Yes String logContent Params Log Content Yes String startTime Params Start Time No long endTime Params End Time No long 9.3 Response Description Schema code Status: 1000 for success, other values for exceptions int data Response Data List { \"code\": 1000, \"data\": [ { \"id\": \"62d4f0363e50046ce51d44f3\", \"createTime\": 1658122294338, \"updateTime\": 1658122294338, \"memberName\": \"cluster\", \"clusterId\": \"62d4bdfd3e50046ce51d41f6\", \"eventId\": null, \"logInfoList\": [ { \"createTime\": 1658122294338, \"log\": \"rz Cluster operation [updateMongoMemberInfo] successful\" } ] } ] } 10 Get Mongo Cluster Log Count 10.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMongoClusterLogCount/ 10.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String memberName Params Node Name Yes String logContent Params Log Content Yes String startTime Params Start Time No long endTime Params End Time No long 10.3 Response Description Schema code Status: 1000 for success, other values for exceptions int data Response Count long 11 Query MongoD Log Information 11.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMongoDLogData/// 11.2 Request Parameters Type: If empty, query all. Possible values: SHARDING, STORAGE, RECOVERY, CONTROL Name Located in Description Required Schema mongoMemberId Path Mongo node ID Yes String pageIndex Path Page Index Yes int pageSize Path Page Size Yes int type Params Type No String startTime Params Start Time No long endTime Params End Time No long content Params Content No String ![ img_5.png](../../../images/whalealPlatformImages/getMongoDLogData.png) 11.3 Response Description Schema code Status: 1000 for success, other values for exceptions int data Response Data MongoMember { \"code\": 1000, \"data\": [ { \"id\": \"62d5037fbb551e67507f9a32\", \"createTime\": 0, \"updateTime\": 0, \"log\": { \"t\": \"2022-07-18T06:53:49.151+00:00\", \"s\": \"I\", \"c\": \"NETWORK\", \"id\": \"[conn3161]\", \"msg\": \"end connection 192.168.3.80:58778 (5 connections now open)\" }, \"nodeId\": \"62d4be9d3e50046ce51d4228\", \"fileOffset\": 0 } ] } 12 Query MongoD Log Count 12.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMongoDLogCount/ 12.2 Request Parameters Type: If empty, query all. Possible values: STORAGE, RECOVERY, CONTROL Name Located in Description Required Schema mongoMemberId Path Mongo node ID Yes String type Params Type Yes String startTime Params Start Time Yes String endTime Params End Time No String content Params Search Content No String 12.3 Response Description Schema code Status: 1000 for success, other values for exceptions int data Response Count long 13 Get Mongo Top and Op 13.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMongoTopAndOp// 13.2 Request Parameters Type: 1 for top, 2 for op Name Located in Description Required Schema mongoMemberId Path Mongo node ID Yes String type Path Type Yes int 13.3 Response Description Schema code Status: 1000 for success, other values for exceptions int data Response Data List 14 Update Cluster Name 14.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/updateClusterName// 14.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String newClusterName Path New Name Yes String 14.3 Response Description Schema code Status: 1000 for success, other values for exceptions int msg Response Message String 15 Get Mongo Statistics 15.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMongoStatistics 15.2 Request 15.3 Response Description Schema code Status: 1000 for success, other values for exceptions int data Response Data JSON 16 Query Mongo Event by Event ID 16.1 Request Path Get: http://{Server-Host}:{Port}/api/server/mongo/findMongoEventLogByEventId/ 16.2 Request Parameters Name Located in Description Required Schema eventId Path Event ID Yes String 16.3 Response Description Schema code Status: 1000 for success, other values for exceptions int data Response Data List { \"code\": 1000, \"data\": [ { \"createTime\": 1658131316409, \"log\": \"chen:45463 Operation [openQPS] successful\" }, { \"createTime\": 1658131317418, \"log\": \"Event group ended\" } ] } 17 Get Mongo Event Log Data 17.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMongoEventLogData/// 17.2 Request Parameters Status: 'Initialized', 'Running', 'Paused', 'Ended', 'Exception Ended', 'Aborted' | Name | Located in | Description | Required | Schema | | -------------------|----------------------|-------------------------------|-----------------|----------- | | clusterId | Path | Cluster ID | Yes |String | pageSize | Path | Page Size | Yes |int | pageIndex | Path | Page Index | Yes |int | status | Params | Status | No |String | eventName | Params | Event Name | No |String | operatorName | Params | Operator | No |String 17.3 Response Description Schema code Status: 1000 for success, other values for exceptions int data Response Data List { \"code\": 1000, \"data\": [ { \"id\": \"62d5281602d41247cf3741d0\", \"createTime\": 1658136598663, \"updateTime\": 1658136704891, \"clusterId\": \"62d4bdfd3e50046ce51d41f6\", \"eventName\": \"Cluster operation: delete\", \"operatorId\": \"62b2d434e0869c777c439867\", \"operatorName\": \"lhp1234\", \"status\": \"Ended\", \"logList\": null } ] } 18 Get Mongo Event Log Count 18.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMongoEventLogCount/ 18.2 Request Parameters Status: 'Initialized', 'Running', 'Paused', 'Ended', 'Exception Ended', 'Aborted' Name Located in Description Required Schema clusterId Path Cluster ID Yes String eventName Params Event Name No String status Params Status No String operatorName params Operator No String 18.3 Response Description Schema code Status: 1000 for success, other values for exceptions int data Response Count long 19 Retrieve MongoDB Cluster Information Data 19.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/findMongoDBClusterInfoData// 19.2 Request Parameters type: 1 for single node, 2 for replica set, 3 for sharded Name Located in Description Required Schema pageIndex Path Page index Yes int pageSize Path Page size Yes int type Params Cluster type No int clusterName Params Cluster name No String mongoMemberName Params Mongo member name No String fcv Params FCV No String 19.3 Response Name Description Schema code Status: 1000 for success int data Returned data List { \"code\": 1000, \"data\": [ { \"id\": \"62fa2017fe07726988b761fa\", \"createTime\": 1660559406829, ... \"configurationOptions\": { ... \"net_bindIp\": \"0.0.0.0\", \"net_port\": \"36398\" }, \"operateVersion\": 3916 }, ... ] } 20 Retrieve MongoDB Cluster Information Data Count 20.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/findMongoDBClusterInfoCount 20.2 Request Parameters type: 1 for single node, 2 for replica set, 3 for sharded Name Located in Description Required Schema fcv Params FCV Yes String clusterName Params Cluster name No String type Params Cluster type No int mongoMemberName Params Mongo member name No String 20.3 Response Name Description Schema code Status: 1000 for success int data Returned count long 21 Get MongoDB Collections 21.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMongoDBCollections// 21.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String eventId Path Event ID Yes String 21.3 Response Name Description Schema code Status: 1000 int data Returned data list List { \"code\": 1000, \"data\": [ { \"name\": \"coll\", \"type\": \"collection\", \"options\": {}, ... \"size\": 335, \"ns\": \"cc.coll\" }, ... ] } 22 Get MongoDB Cluster Users 22.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMongoDBClusterUser/ 22.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String 22.3 Response Name Description Schema code Status: 1000 int data Returned data list List { \"code\": 1000, \"data\": [ { \"_id\": \"admin.16581342589211\", \"userId\": { \"type\": 4, \"data\": \"LMMiWU2KT5GVoDCbkt3B4g==\" }, ... \"roles\": [ { \"role\": \"root\", \"db\": \"admin\" } ] }, ... ] } 23 Get MongoDB Cluster Roles 23.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMongoDBClusterRole/ 23.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String 23.3 Response Name Description Schema code Status: 1000 int data Returned data list List { \"code\": 1000, \"data\": [ { \"role\": \"__queryableBackup\", \"db\": \"admin\", ... \"privileges\": [ { \"resource\": { \"db\": \"config\", \"collection\": \"settings\" }, \"actions\": [ \"find\" ] } ] }, ... ] } 24 Execute an Explain Plan 24.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/exeExplainPlan// 24.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String mongoMemberId Path Mongo member ID Yes String document Body Request data Yes Map 24.3 Response | Name | Description | Schema | | ---- | ------------------ | ------ | | code | Status: 1000 | int | | data | Returned data | JSON | { \"code\": 1000, \"data\": { \"explain\": { ... \"executionStats\": { ... \"executionStages\": { ... \"isEOF\": 1 }, \"allPlansExecution\": [] }, ... }, ... } } 25 Get All MongoDB Configuration Parameters 25.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMongoDBProcessArgument 25.2 Request No specific request details provided. 25.3 Response Name Description Schema code Status: 1000 for success int data Returned data list List { \"code\": 1000, \"data\": [ { \"id\": \"62faf2bcd0810e3aeace6dae\", \"createTime\": 0, \"updateTime\": 0, \"maxVersion\": null, \"minVersion\": null, \"name\": \"SYSTEM_LOG_VERBOSITY\", \"options\": [ { \"label\": \"1 (v)\", \"value\": \"1\" }, ... ], \"path\": \"systemLog.verbosity\", \"processTypes\": \"ALL\", \"shortName\": \"verbosity\", \"type\": \"INTEGER\", \"credential\": false }, ... ] } 26 Get MongoDB Collections 26.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMongoDBCollections// 26.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String eventId Path Event ID Yes String 26.3 Response Name Description Schema code Status: 1000 int data Returned data list List { \"code\": 1000, \"data\": [ { \"name\": \"test\", \"sub\": [ { \"name\": \"a\", \"type\": \"collection\", ... \"ns\": \"test.a\" } ] }, ... ] } 27 Query Cluster Database Data 27.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/queryClusterDbData// 27.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String eventId Path Event ID Yes String map Body Query criteria Yes Map Example of the query criteria: { \"ns\": \"test.a\", \"query\": \"{}\", \"pageSize\": 10, \"pageIndex\": 1 } 27.3 Response Name Description Schema code Status: 1000 int data Returned data list List { \"code\": 1000, \"data\": [ { \"_id\": { \"date\": 1659684764000, \"timestamp\": 1659684764 }, \"a\": 1.0 }, ... ] } 28 Create Index 28.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/createIndex// 28.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String eventId Path Event ID Yes String map Body Index config Yes Map Example of index configuration: { \"indexName\": \"chen\", \"ns\": \"test.coll\", \"index\": \"{a:1}\", \"buildIndexInTheBackground\": false, ... } 28.3 Response Name Description Schema code Status: 1000 int msg Returned message String 29 Diagnostic Data 29.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/mdiagData/// 29.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String pageIndex Path Page index Yes int pageSize Path Page size Yes int 29.3 Response Name Description Schema code Status: 1000 int data Returned data list List { \"code\": 1000, \"data\": [ { \"_id\": \"62ecf7a2a3a6e138ea1f00b0\", \"filename\": \"mdiag_server100_1659696513419.gz\", ... \"id\": \"62ecf7a2a3a6e138ea1f00b0\" }, ... ] } 30 Get Diagnostic Count 30.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/mdiagCount/ 30.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String 30.3 Response Name Description Schema code Status: 1000 int data Returned count long 31 Update Event Status 31.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/updateEventStatus// **31 .2 Request Parameters** Name Located in Description Required Schema eventId Path Event ID Yes String status Path Status Yes String 31.3 Response Name Description Schema code Status: 1000 int msg Returned message String 32 Get Mdiag Log 32.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getMdiagLog// 32.2 Request Parameters Name Located in Description Required Schema eventId Path Event ID Yes String clusterId Path Cluster ID Yes String 32.3 Response Name Description Schema code Status: 1000 int data Returned data list List 33 Get All Cluster IDs and Names 33.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/getAllClusterIdAndName 33.2 Request Parameters Name Located in Description Required Schema clusterName Params Cluster name No String 33.3 Response Name Description Schema code Status: 1000 int data Returned data list List --- "},"whalelaPlatform/07-APIReference/ErrorCodes.html":{"url":"whalelaPlatform/07-APIReference/ErrorCodes.html","title":"ErrorCodes","keywords":"","body":"Error Codes When you encounter an error while sending a request to the API, the interface will return one of the following error codes. Error code list: 9: Common - Codes starting with this value are not displayed to the frontend as part of the message. 10: Indicates normal execution with no message (msg). 11: User-related errors. 12: Agent-related errors. Error HTTP Code Description UNKNOWN_EXCEPTION 901 Unknown system exception ERROR_SYSTEM 902 System error LIMIT_GATEWAY 903 Gateway limitation ERROR_EXE_COMMAND 903 Failed to update command status SUCCESS_CODE 1000 Normal execution NOT_EXIST_ACCOUNT 1101 Account does not exist ERROR_PASSWORD 1102 Incorrect password BLANK_ACCOUNT 1103 Account cannot be blank EXIST_PHONE 1104 Phone number already exists EXIST_EMAIL 1105 Email already exists EXIST_ACCOUNT 1106 Account already exists NOT_EXIST_TOKEN 1107 Token does not exist ERROR_UPDATE_MEMBER 1108 Failed to update information NOT_EXIST_AGENT_ID 1201 AgentId does not exist ERROR_SAVE_AGENT_LOG 1202 Failed to save log information ERROR_DOWN_LOAD_FILE 1203 File download failed OPS_COMMON_EXCEPTION 1900 Common OPS exception NOT_EXIST_DATA 1901 Data does not exist "},"whalelaPlatform/07-APIReference/Files.html":{"url":"whalelaPlatform/07-APIReference/Files.html","title":"Files","keywords":"","body":"File Interface When calling this interface, you need to set the whaleal-Token in the request header and provide the necessary parameters to initiate the request. The returned content will be in JSON format. The special entity classes will be provided in the final entity class table. Default Request Header Format, Special Cases for Special Declarations The whaleal-Token is returned when you call the login interface. When calling other interfaces, place the token in the request header. Call the login interface to get whaleal-Token KEY VALUE Accept-Encoding gzip,deflate,br Connection keep-alive Content-Type multipart/form-data; boundary=\\ whaleal-token \"token\" 1 Upload File to Server 1.1 Request Path POST: http://{Server-Host}:{Port}/api/server/file/web/upload/file 1.2 Request Parameters Name Located in Description Required Schema File Body Uploaded File Yes MultipartFile whaleal-Token Params Token Yes String 1.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String 2 Delete Server-side File Here, the Content-Type in the request header is application/json. 2.1 Request Path GET: http://{Server-Host}:{Port}/api/server/file/deleteFile/ 2.2 Request Parameters Name Located in Description Required Schema filename Path File Name Yes String 2.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String 3 Get Server-side File Information Here, the Content-Type in the request header is application/json. 3.1 Request Path GET: http://{Server-Host}:{Port}/api/server/file/getAllMongoFile 3.2 Request 3.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Response data JSON { \"code\": 1000, \"data\": [ { \"createTime\": 1658484806756, \"updateTime\": 1658484806756, \"name\": \"mongodb-linux-x86_64-rhel70-4.2.17.tgz\", \"shortName\": \"mongodb-linux-x86_64-rhel70-4.2.17\", \"size\": 133396543, \"md5\": \"1\", \"version\": null, \"path\": \"/home/whaleal/server/mongodb-linux-x86_64-rhel70-4.2.17.tgz\", \"hostId\": \"\", \"server\": true } ] } 4 Agents can Download Server-side Files 4.1 Request Path GET: http://{Server-Host}:{Port}/api/server/file/agent/download/ 4.2 Request Parameters Name Located in Description Required Schema filename Path File Name Yes String agentId Header AgentId Yes String 4.3 Response | | Description Schema File Binary representation of the downloaded file File 5 Update Server-side File Information Here, the Content-Type in the request header is application/json. 5.1 Request Path GET: http://{Server-Host}:{Port}/api/server/file/agent/updateAllMongoFileToAgent 5.2 Request 5.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String 6 Download Inspection Logs 6.1 Request Path GET: http://{Server-Host}:{Port}/api/server/file/download/mdiag/// 6.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String fileID Path File ID Yes String filename Path File Name Yes String whaleal-Token Params Token Yes String 6.3 Response Description Schema mdiag Downloaded file in binary form File 7 Download Mongo Cluster Files 7.1 Request Path GET: http://{Server-Host}:{Port}/api/server/file/download/mongoClusterFile// 7.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String filename Path File Name Yes String fileIdList Params List of File IDs Yes List whaleal-Token Params Token Yes String 7.3 Response Description Schema file Downloaded file in binary form File 8.1 Request Path GET: http://{Server-Host}:{Port}/api/server/agent/downAgentFile// 8.2 Request Parameters Name Located in Description Required Schema agentId Path Agent ID Yes String fileName Path File Name Yes String 8.3 Response Description Schema File Binary representation of the downloaded file File "},"whalelaPlatform/07-APIReference/Member.html":{"url":"whalelaPlatform/07-APIReference/Member.html","title":"Member","keywords":"","body":"Member Interface When making requests to this interface, you need to set the whaleal-Token in the request header and provide the required parameters to initiate the request. The returned content will be in JSON format. Special entity classes will be provided in the final entity class table. Default Request Header Format, Special Cases for Special Declarations The whaleal-Token is returned when you call the login interface. When calling other interfaces, place the token in the request header. Call the login interface to get whaleal-Token KEY VALUE Accept-Encoding gzip,deflate,br Connection keep-alive Content-Type application/json whaleal-token \"token\" 1 Login 1.1 Request Path POST: http://{Server-Host}:{Port}/api/server/member/login 1.2 Request Parameters Name Located in Description Required Schema account Body Account Name Yes String password Body Password Yes String 1.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Response data JSON generateAgentIdAble Whether the user has permission to generate agentId boolean token Token token String createMongoDBAble Whether the user has permission to create a MongoDB cluster boolean { \"code\": 1000, \"data\": { \"id\": \"62be61c7cbeff906da28f6ff\", \"createTime\": 1656644040004, \"updateTime\": 1657690356662, \"account\": \"chen123\", \"password\": \"\", \"email\": \"1q@q.com\", \"areaCode\": \"86\", \"phone\": \"17698999999\", \"role\": \"admin\", \"timezone\": \"Asia/Shanghai\", \"receiveAlert\": true, \"dingDingList\": [] }, \"createMongoDBAble\": true, \"generateAgentIdAble\": true, \"token\": \"\" } 2 Save New User Information 2.1 Request Path POST: http://{Server-Host}:{Port}/api/server/member/register 2.2 Request Parameters Name Located in Description Required Schema memberMongoEntity Body User entity object Yes MemberMongoEntity Example: Save new user information; where MemberMongoEntity is as follows: { \"account\": \"chen123556\", \"password\": \"123456\", \"email\": \"123356789@qq.com\", \"phone\": \"17699969999\" } 2.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Response data JSON { \"code\": 1000, \"data\": { \"id\": \"62da7bd6239d00094230b525\", \"createTime\": 1658485718459, \"updateTime\": 1658485718459, \"account\": \"chen123556\", \"password\": \"\", \"email\": \"123356789@qq.com\", \"areaCode\": \"86\", \"phone\": \"17699969999\", \"role\": \"admin\", \"timezone\": \"Asia/Shanghai\", \"receiveAlert\": true, \"dingDingList\": [] } } 3 Update User Information 3.1 Request Path POST: http://{Server-Host}:{Port}/api/server/member/update 3.2 Request Parameters Name Located in Description Required Schema memberMongoEntity Body User entity object Yes MemberMongoEntity Example: Update user information; where MemberMongoEntity is as follows: { \"id\": \"62be61c7cbeff906da28f6ff\", \"createTime\": 1659602792412, \"updateTime\": 1659605792412, \"account\": \"chen123\", \"password\": \"\", \"email\": \"110236111@qq.com\", \"areaCode\": \"86\", \"phone\": \"17699999999\", \"role\": \"admin\", \"timezone\": \"A1\", \"receiveAlert\": true, \"dingDingList\": [ \"_\" ], \"avatar\": \"\" } 3.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Response data JSON { \"code\": 1000, \"data\": { \"id\": \"62da7bd6239d00094230b525\", \"createTime\": 1658485718459, \"updateTime\": 1658486089634, \"account\": \"chen123556\", \"password\": \"\", \"email\": \"98765221@qq.com\", \"areaCode\": \"86\", \"phone\": \"17699954999\", \"role\": \"admin\", \"timezone\": \"Asia/Shanghai\", \"receiveAlert\": true, \"dingDingList\": [] } } 4 Search Users 4.1 Request Path POST: http://{Server-Host}:{Port}/api/server/member/findMemberData// 4.2 Request Parameters Name Located in Description Required Schema -------------------------------|-----------------|----------- | | pageSize | Path | Page size | Yes |int | | pageIndex | Path | Page index | Yes |int | | map | Body | User information | Yes |Map | Example: Search users; where Map is as follows: { \"account\": \"chen\", \"phone\": \"176\", \"email\": \"11\" } 4.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Response data List { \"code\": 1000, \"data\": [ { \"id\": \"62d8b50b239d00094230b37c\", \"createTime\": 1658369291763, \"updateTime\": 1658369291763, \"account\": \"chen123456\", \"password\": null, \"email\": \"123456789@qq.com\", \"areaCode\": \"86\", \"phone\": \"17699999999\", \"role\": \"admin\", \"timezone\": \"Asia/Shanghai\", \"receiveAlert\": true, \"dingDingList\": [] } ] } 5 Query User Count 5.1 Request Path POST: http://{Server-Host}:{Port}/api/server/member/findMemberCount 5.2 Request Parameters Name Located in Description Required Schema map Body User information Yes Map Example: Search user count; where Map is as follows: { \"account\": \"chen\", \"phone\": \"\", \"email\": \"\" } 5.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Number of users long 6 Update Receive Alerts 6.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/update/receiveAlert// 6.2 Request Parameters Name Located in Description Required Schema memberId Path User ID Yes String value Path Enable or disable Yes boolean 6.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String 7 Update Timezone 7.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/update/timezone/ 7.2 Request Parameters timezone: Asia/Shanghai Name Located in Description Required Schema memberId Path User ID Yes String timezone Params Timezone Yes String 7.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String 8 Update Role 8.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/update/role// 8.2 Request Parameters value: user, admin Name Located in Description Required Schema memberId Path User ID Yes String value Path Role Yes String 8.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String 9 Update MongoDB Creation Permission 9.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/update/createMongoDBAble// 9.2 Request Parameters Name Located in Description Required Schema memberId Path User ID Yes String value Path Enable or disable Yes boolean 9.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String 10 Update Generate AgentId Permission 10.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/update/generateAgentIdAble // 10.2 Request Parameters Name Located in Description Required Schema memberId Path User ID Yes String value Path Enable or disable Yes boolean 10.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String 11 Update User Resource Information 11.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/update/userResourceInfo//// 11.2 Request Parameters value: read, write, null type: mongodb, host Name Located in Description Required Schema memberId Path User ID Yes String objectId Path ID based on type Yes String type Path Type Yes String value Path Permission Yes String 11.3 Response Description Schema code Status code: 1000 for success, others for errors int msg Response message String 12 Delete User 12.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/delete/user/ 12.2 Request Parameters Name Located in Description Required Schema memberId Path User ID Yes String 12.3 Response Description Schema code Status code: 1000 for success, others for errors int msg Response message String 13 Get User Resource 13.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/getUserResource/ 13.2 Request Parameters Name Located in Description Required Schema memberId Path User ID Yes String 13.3 Response Description Schema code Status code: 1000 for success, others for errors int data Response data (JSON) - { \"code\": 1000, \"data\": { \"id\": \"62eb99cdca0e230d4a13c423\", \"createTime\": 1659607501509, \"updateTime\": 1660121964509, \"createMongoDBAble\": true, \"generateAgentIdAble\": true, \"mongoDBClusterList\": [ { \"id\": \"62eb915e32f3671236d6a0be\", \"competence\": \"write\" }, { \"id\": \"62ec7ac2ca0e230d4a13c490\", \"competence\": \"write\" } ], \"hostList\": [ { \"id\": \"62ecaf96ca0e230d4a13c75f\", \"competence\": \"write\" }, { \"id\": \"62ecb027ca0e230d4a13c764\", \"competence\": \"write\" } ] } } 14 Get User Server Resource Data 14.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/getUserServerResourceData//// 14.2 Request Parameters competence: write, read, null Name Located in Description Required Schema memberId Path User ID Yes String competence Path Permission Yes String pageSize Path Page size Yes int pageIndex Path Page index Yes int hostName Params Host name No String 14.3 Response Description Schema code Status code: 1000 for success, others for errors int data Response data (List) - { \"code\": 1000, \"data\": [ { \"_id\": \"62eb906a32f3671236d6a0af\", \"hostName\": \"server121\", \"osVersion\": \"CentOS Linux release 7.7.1908 (Core)\" }, { \"_id\": \"62eb90ea32f3671236d6a0b7\", \"hostName\": \"server90\", \"osVersion\": \"CentOS Linux release 7.7.1908 (Core)\" } ] } 15 Get User Server Count 15.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/getUserServerResourceCount// 15.2 Request Parameters competence: write, read, null Name Located in Description Required Schema memberId Path User ID Yes String competence Path Permission Yes String 15.3 Response Description Schema code Status code: 1000 for success, others for errors int data Response count long 16 Get User MongoDB Cluster Resource Data 16.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/getUserMongoDBClusterResourceData//// 16.2 Request Parameters competence: write, read, null Name Located in Description Required Schema memberId Path User ID Yes String competence Path Permission Yes String pageSize Path Page size Yes int pageIndex Path Page index Yes int clusterName Params Cluster name No String 16.3 Response Description Schema code Status code: 1000 for success, others for errors int data.clusterName Cluster name String data.type Type: Single node, Replica set, Sharded cluster, Managed String 17 Get User MongoDB Cluster Count 17.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/getUserMongoDBClusterResourceCount// 17.2 Request Parameters competence: write, read, null Name Located in Description Required Schema memberId Path User ID Yes String competence Path Permission Yes String clusterName Params Cluster name No String 17.3 Response Description Schema code Status code: 1000 for success, others for errors int data Response count long 18 Get Information Data 18.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/getMessageData// 18.2 Request Parameters Name Located in Description Required Schema memberId Path User ID Yes String pageSize Path Page size Yes int pageIndex Path Page index Yes int operatorName Params Operator's name No String objectName Params Object's name No String status Params Status No boolean message Params Message No String startTime Params Start time No long endTime Params End time No long 18.3 Response Description Schema code Status code: 1000 for success, others for errors int data Response data (List) - { \"code\": 1000, \"data\": [ { \"id\": \"62fb00088e34f36c92fb013d\", \"createTime\": 1660616712771, \"updateTime\": 1660616712771, \"message\": \"Host: server190 is down\\r\\n\\tAlert time UTC: 2022-08-16 02:22:56\", \"type\": \"alert\", \"objectId\": \"62f343406ccc6972abb87818\", \"objectName\": \"server190\", \"operatorId\": null, \"operatorName\": null, \"eventId\": null, \"list\": [] } ] } 19 Get Message Count 19.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/getMessageCount 19.2 Request Parameters Name Located in Description Required Schema memberId Path User ID Yes String operatorName Params Operator's name No String objectName Params Object's name No String status Params Status No boolean message Params Message No String startTime Params Start time No long endTime Params End time No long 19.3 Response Description Schema code Status code: 1000 for success, others for errors int data Response count long 20 Update Message Status 20.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/update/messageStatus// 20.2 Request Parameters Name Located in Description Required Schema memberId Path User ID Yes String messageId Path Message ID Yes String 20.3 Response Description Schema code Status code: 1000 for success, others for errors int msg Response message String 21 Update All Message Status 21.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/update/allMessageStatus/ 21.2 Request Parameters Name Located in Description Required Schema memberId Path User ID Yes String 21.3 Response Description Schema code Status code: 1000 for success, others for errors int msg Response message String 22 Get All Member IDs and Names 22.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/getAllMemberIdAndName 22.2 Request Parameters Name Located in Description Required Schema memberName Params User name Yes String 22.3 Response Description Schema code Status code: 1000 for success, others for errors int data Response data (List) - { \"code\": 1000, \"data\": [ { \"id\": \"63031cb149d5ad2d50af5d15\", \"name\": \"admin\" }, { \"id\": \"630321262ef5221f75e9f0c6\", \"name\": \"chen\" } ] } 23 Reset Password 23.1 Request Path GET: http://{Server-Host}:{Port}/api/server/member/resetPassword/ 23.2 Request Parameters Name Located in Description Required Schema memberId Path User ID Yes String 23.3 Response Description Schema code Status code: 1000 for success, others for errors int data Response data (List) | - | "},"whalelaPlatform/07-APIReference/MongoOperate.html":{"url":"whalelaPlatform/07-APIReference/MongoOperate.html","title":"Mongo","keywords":"","body":"MongoOperate接口 Certainly, here's the translated version of the information you provided, while keeping the format and not modifying file paths: API calls require setting the 'whaleal-Token' in the request header. Fill in the parameters to initiate the request, and the returned content will be in JSON format. Special entity classes for return will be provided at the end. Parameters related to time should be passed in timestamp format. For some API calls, you need to use the following parameters: - 'eventId': Obtain this from the 'Get Cluster Log Information' API call. It represents the ID of the desired event. - 'mongoMemberId': Retrieve this from the 'Find MongoDB Cluster Information Data' API call. It can be found within the 'mongoMember' collection in the returned result. - 'replicateId': Similar to 'clusterId', acquire it from the result of the 'Find MongoDB Cluster Information Data' API call. It is located within the 'replicate' collection. - 'clusterId': This parameter can be found in the result of the 'Find MongoDB Cluster Information Data' API call. Please note that the parameter values representing time should be passed as timestamps. A timestamp represents the number of milliseconds since January 1, 1970 (UTC), also known as the Unix epoch. Next, you will find the translated entity class table at the end. Default Request Header Format, Special Cases with Special Declarations The whaleal-Token is obtained from the login API call and should be included in the request header for subsequent API calls. API call to retrieve whaleal-Token from Login KEY VALUE Accept-Encoding gzip,deflate,br Connection keep-alive Content-Type application/json whaleal-token \"token\" Deprecated (No Longer in Use) 1 Create MongoDB Standalone 1.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/createMongoStandalone/// 1.2 Request Parameters Name Located in Description Required Schema isNewCluster Path Whether it's a new cluster Yes boolean clusterId Path Cluster ID Yes String replicateId Path Replication ID Yes String mongoMember Body Entity object Yes MongoMember tag Params Tag No String Example - Create MongoDB Standalone; where MongoMember is as follows: { \"hostName\": \"chen\", \"hostId\": \"62bbfbe9a46517610435d615\", \"port\": \"25567\", \"dataDirectory\": \"/home/chen/data25567\", \"logFile\": \"/home/chen/log25567.log\", \"version\": \"mongodb-linux-x86_64-rhel70-4.2.21\", \"deleteDataAndLogAble\": \"false\", \"authAble\": \"false\", \"userName\": \"\", \"password\": \"\", \"configurationOptions\": { \"storage.wiredTiger.engineConfig.cacheSizeGB\": \"0.3\" } } 1.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String eventId Event ID String data Response data JSON { \"msg\": \"In Progress\", \"eventId\": \"62ce9a7ded494511782ff392\", \"code\": 1000, \"data\": { \"id\": null, \"createTime\": 0, \"updateTime\": 0, \"memberName\": \"null:27017\", \"hostName\": null, \"hostId\": null, \"port\": \"27017\", \"version\": null, \"upgradeVersion\": null, \"userName\": null, \"password\": null, \"authDbName\": \"admin\", \"currentTimeMillis\": 1657707133455, \"dataDirectory\": \"/var/ops/mongodb1657707133455/data/\", \"logFile\": \"/var/ops/mongodb1657707133455/log/log.log\", \"confPath\": \"/var/ops/mongodb1657707133455/mongo.conf\", \"deleteDataAndLogAble\": false, \"authAble\": false, \"runShCmd\": null, \"type\": 11, \"status\": \"No State\", \"monitorServerStatus\": false, \"monitorTopAndOp\": false, \"collectMongoLog\": false, \"mongoLogFileOffset\": 0, \"operaLogTemp\": [], \"votes\": 1, \"priority\": 1.0, \"delay\": 0, \"buildIndexes\": true, \"procId\": \"\", \"clusterId\": \"62ce9a7ded494511782ff393\", \"replId\": null, \"clusterName\": null, \"tags\": {}, \"configurationOptions\": {}, \"operateVersion\": 0 } } 2 Convert Single Node to Replica Set 2.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/standaloneToReplicate// 2.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String replName Path Replication name Yes String 2.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String 3 Create MongoDB Replica Set 3.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/createMongoReplica 3.2 Request Parameters Name Located in Description Required Schema mongoReplica Body Entity object Yes MongoReplica tag Params Tag No String Example - Create MongoDB Replica Set; where MongoReplica is as follows: { \"userName\": \"\", \"password\": \"\", \"type\": 1, \"clusterId\": \"\", \"replicaName\": \"qaq\", \"deleteDataAndLogAble\": false, \"status\": \"\", \"authAble\": \"false\", \"operaLog\": [], \"memberList\": [ { \"type\": 31, \"hostName\": \"chen\", \"hostId\": \"62bbfbe9a46517610435d615\", \"port\": \"25025\", \"version\": \"mongodb-linux-x86_64-rhel70-4.2.21\", \"votes\": \"1\", \"priority\": \"1\", \"delay\": \"\", \"buildIndexes\": true, \"dataDirectory\": \"/home/chen/data25025\", \"logFile\": \"/home/chen/log25025.log\", \"configurationOptions\": { \"storage.wiredTiger.engineConfig.cacheSizeGB\": \"0.3\" } } ], \"replicationSettings\": { \"protocolVersion\": null, \"chainingAllowed\": null, \"writeConcernMajorityJournalDefault\": null, \"heartbeatTimeoutSecs\": null, \"electionTimeoutMillis\": null, \"catchUpTimeoutMillis\": null, \"catchUpTakeoverDelayMillis\": null, \"getLastErrorDefaults\": null, \"forceReconfigure\": null } } 3.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Response data JSON msg Response message String eventId Event ID String { \"msg\": \"In Progress\", \"eventId\": \"62da7357239d00094230b51a\", \"code\": 1000, \"data\": { \"id\": null, \"createTime\": 0, \"updateTime\": 0, \"replicaName\": null, \"memberList\": [], \"type\": 1, \"clusterId\": \"62cf7903ed494511782ff4f9\", \"deleteDataAndLogAble\": false, \"status\": null, \"operaLog\": [], \"replicationSettings\": {}, \"replicationOtherSettings\": {}, \"authAble\": false, \"userName\": null, \"password\": null, \"authDbName\": \"admin\", \"protocolVersion\": 1, \"writeConcernMajorityJournalDefault\": false } } 4 Create MongoDB Sharded Cluster 4.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/createMongoSharded 4.2 Request Parameters Name Located in Description Required Schema mongoShard Body Entity object Yes MongoShard tag Params File name No String Example - Create MongoDB Sharded Cluster; where MongoShard is as follows: { \"clusterName\": \"fenpian\", \"deleteDataAndLogAble\": \"false\", \"authAble\": \"false\", \"userName\": \"\", \"password\": \"\", \"shardingMap\": { \"shard1\": { \"memberList\": [ { \"type\": 1, \"hostName\": \"chen\", \"hostId\": \"62bbfbe9a46517610435d615\", \"port\": \"44567\", \"version\": \"mongodb-linux-x86_64-rhel70-4.2.21\", \"votes\": \"1\", \"priority\": \"1\", \"delay\": \"\", \"buildIndexes\": \"true\", \"dataDirectory\": \"/home/chen/data44567\", \"logFile\": \"/home/chen/log44567.log\", \"configurationOptions\": { \"storage.wiredTiger.engineConfig.cacheSizeGB\": \"0.3\" } } ], \"replicationSettings\": { \"replicaSetId\": \"shard1\", \"protocolVersion\": null, \"chainingAllowed\": null, \"writeConcernMajorityJournalDefault\": null, \"heartbeatTimeoutSecs\": null, \"electionTimeoutMillis\": null, \"catchUpTimeoutMillis\": null, \"catchUpTakeoverDelayMillis\": null, \"getLastErrorDefaults\": null, \"forceReconfigure\": null } } }, \"config \": { \"memberList\": [ { \"type\": 1, \"hostName\": \"server100\", \"hostId\": \"62b153a344ba1b7771c42df7\", \"port\": \"44567\", \"version\": \"mongodb-linux-x86_64-rhel70-4.2.21\", \"votes\": \"1\", \"priority\": \"1\", \"delay\": \"\", \"buildIndexes\": \"true\", \"dataDirectory\": \"/home/chen/data44567\", \"logFile\": \"/home/chen/log44567.log\", \"configurationOptions\": { \"storage.wiredTiger.engineConfig.cacheSizeGB\": \"0.3\" } } ], \"replicationSettings\": { \"replicaSetId\": \"config\", \"protocolVersion\": \"\", \"chainingAllowed\": \"\", \"writeConcernMajorityJournalDefault\": \"\", \"heartbeatTimeoutSecs\": \"\", \"electionTimeoutMillis\": \"\", \"catchUpTimeoutMillis\": \"\", \"catchUpTakeoverDelayMillis\": \"\", \"getLastErrorDefaults\": \"\", \"forceReconfigure\": \"\" } }, \"mongoS\": [ { \"logFile\": \"/home/chen/log44567.log\", \"dataDirectory\": \"/home/chen/data44567\", \"hostName\": \"server200\", \"version\": \"mongodb-linux-x86_64-rhel70-4.2.21\", \"port\": \"44567\", \"configurationOptions\": { \"storage.wiredTiger.engineConfig.cacheSizeGB\": \"0.3\" }, \"hostId\": \"62cbbd7607bebb71b8429e5e\" } ] } 4.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String eventId Event ID String data Response data JSON { \"msg\": \"In Progress\", \"eventId\": \"62da73c4239d00094230b51c\", \"code\": 1000, \"data\": { \"id\": null, \"createTime\": 0, \"updateTime\": 0, \"clusterName\": null, \"clusterId\": \"62cf8e51ed494511782ff6c8\", \"config\": null, \"mongoS\": [], \"shardingMap\": {}, \"operaLog\": [], \"deleteDataAndLogAble\": false, \"authAble\": false, \"userName\": null, \"password\": null, \"authDbName\": \"admin\", \"status\": null } } 5 Operate on Auth-Enabled Cluster 5.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/operateClusterAbleAuth/ 5.2 Request Parameters Name Located in Description Required Schema clusterId Path File name Yes String map Body Parameters Yes Map Example - Operate on Auth-Enabled Cluster; where map is as follows: { \"authAble\": \"true\", // Set to \"false\" when other parameters are not needed \"userName\": \"123\", \"password\": \"123\" } 5.3 Response Description Schema code Status code: 1000 for success, others for exceptions int msg Response message String eventId Event ID String 6 Add Shard 6.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/addShard/ 6.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String mongoReplica Body MongoDB Replica Yes MongoReplica Example of adding a shard; where MongoReplica is as follows: { \"type\": 3, \"clusterId\": \"\", \"replicaName\": \"qwe\", \"authAble\": \"true\", \"userName\": \"\", \"password\": \"\", \"deleteDataAndLogAble\": false, \"status\": \"\", \"operaLog\": [], \"memberList\": [ { \"type\": 51, \"hostName\": \"chen\", \"hostId\": \"62bbfbe9a46517610435d615\", \"port\": \"44453\", \"version\": \"mongodb-linux-x86_64-rhel70-4.2.21\", \"votes\": \"1\", \"priority\": \"1\", \"delay\": \"\", \"buildIndexes\": true, \"dataDirectory\": \"/home/chen/data44453\", \"logFile\": \"/home/chen/log44453.log\", \"configurationOptions\": { \"storage.wiredTiger.engineConfig.cacheSizeGB\": \"0.3\" } } ] } 6.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int msg Response message String 7 Manage Cluster Information 7.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/mongoManaged 7.2 Request Parameters Name Located in Description Required Schema mongoMember Body MongoDB Member Yes MongoMember 7.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int msg Response message String data Response data JSON 8 Upgrade/Downgrade 8.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/upgrade/// 8.2 Request Parameters type: 1 for upgrade, -1 for downgrade Name Located in Description Required Schema clusterId Path Cluster ID Yes String version Path Version Yes String type Path Cluster type Yes String 8.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int msg Response message String 9 Perform Operations on Nodes 9.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/operate/// 9.2 Request Parameters operateType: updateMongoMemberInfo, startUp, shuntDown, restart, delete, canalQPS, openQPS, canalTopAndOP, openTopAndOP, canalCollectMongoLog, openColletMongoLog, becomePrimary, removeMember Name Located in Description Required Schema clusterId Path Cluster ID Yes String mongoMemberId Path MongoDB Member ID Yes String operateType Path Operation Type Yes String 9.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int msg Response message String 10 Perform Operations on Clusters 10.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/operate// 10.2 Request Parameters operateType: updateMongoMemberInfo, startUp, shuntDown, restart, delete, mdiag Name Located in Description Required Schema clusterId Path Cluster ID Yes String operateType Path Operation Type Yes String 10.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int msg Response message String 11 Update Cluster Information 11.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/updateClusterInfo 11.2 Request Parameters Name Located in Description Required Schema mongoClusterInformation Body Mongo Cluster Information Yes JSON 11.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int msg Response message String 12 Create Single Node 12.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/createMongoStandalone 12.2 Request Parameters Name Located in Description Required Schema mongoMember Body MongoDB Member Yes JSON tag Params Tag No String 12.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int msg Response message String eventId Event ID String data Response data JSON 13 Add Node to Replica Set 13.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/replAddMember// 13.2 Request Parameters Name Located in Description Required Schema -------- | ------ | | mongoMember | Body | MongoDB Member | Yes | JSON | | clusterId | Path | Cluster ID | Yes | String | | replicateId | Path | Replica ID | Yes | String | 13.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int msg Response message String eventId Event ID String data Response data JSON 14 Add Mongos 14.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/addMongoS/ 14.2 Request Parameters Name Located in Description Required Schema mongoMember Body MongoDB Member Yes JSON clusterId Path Cluster ID Yes String 14.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int msg Response message String eventId Event ID String data Response data JSON 15 Update Cluster Name 15.1 Request Path GET: http://{Server-Host}:{Port}/api/server/mongo/updateClusterName// 15.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String newClusterName Path New Name Yes String 15.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int msg Response message String 16 Execute a Plan 16.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/exeExplainPlan// 16.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String mongoMemberId Path MongoDB Member ID Yes String document Body Request Parameters Yes Map 16.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int data Response data JSON 17 Replica Set Initialization 17.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/mongoReplicaInit// 17.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String replicateId Path Replica ID Yes String 17.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int data Response data JSON 18 Collect MongoDB Cluster Logs 18.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/collectMongoDLog/ 18.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String startTime Params Start Time Yes long endTime Params End Time Yes long 18.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int msg Response message String 19 Remove Node 19.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/removeShard// 19.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String shardReplId Path Shard Replica ID Yes String 19.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int msg Response message String 19 Create User 19.1 Request Path POST: http://{Server-Host}:{Port}/api/server/mongo/createMongoUser/ 19.2 Request Parameters Name Located in Description Required Schema clusterId Path Cluster ID Yes String map Body User Information Yes Map 19.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int msg Response message String "},"whalelaPlatform/07-APIReference/Other.html":{"url":"whalelaPlatform/07-APIReference/Other.html","title":"Other","keywords":"","body":"Other API To call this API, you need to set the whaleal-Token in the request header with the specified parameters. The returned content is in JSON format. Special entity classes for the response will be provided in the final table. Default Request Header Format, Special Cases are Specified KEY VALUE Accept-Encoding gzip, deflate, br Connection keep-alive Content-Type application/json 1 Get All MongoDB Version Information 1.1 Request Path GET: http://{Server-Host}:{Port}/api/server/other/getAllMongoVersion 1.2 Request 1.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int data Response data List 2 Get All Whaleal Version Information 2.1 Request Path GET: http://{Server-Host}:{Port}/api/server/other/getWhalealVersion 2.2 Request 2.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int data Response message String "},"whalelaPlatform/07-APIReference/Third_party.html":{"url":"whalelaPlatform/07-APIReference/Third_party.html","title":"Third_party","keywords":"","body":"Third-party API To call this API, you need to set the whaleal-Token in the request header with the specified parameters. The returned content is in JSON format. Special entity classes for the response will be provided in the final table. Default Request Header Format, Special Cases are Specified KEY VALUE Accept-Encoding gzip, deflate, br Connection keep-alive Content-Type application/json 1 Send DingTalk Message 1.1 Request Path GET: http://{Server-Host}:{Port}/api/third/ding/sendMsg 1.2 Request Parameters Name Located in Description Required Schema accessToken Params DingTalk robot token Yes String secret Params DingTalk robot secret Yes String content Params Message content Yes String 1.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int data Response message String 2 Send Email Message 2.1 Request Path GET: http://{Server-Host}:{Port}/api/third/email/sendMsg 2.2 Request Parameters Name Located in Description Required Schema email Params Email account Yes String content Params Message content Yes String 2.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int data Response message String 3 Send SMS Verification Code 3.1 Request Path GET: http://{Server-Host}:{Port}/api/third/sms/sendMsg 3.2 Request Parameters Name Located in Description Required Schema mobile Params Phone number Yes String content Params Message content Yes String 3.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int data Response message String "},"whalelaPlatform/07-APIReference/Configuration.html":{"url":"whalelaPlatform/07-APIReference/Configuration.html","title":"Configuration","keywords":"","body":"Configuration Interface To make API calls, you need to set the whaleal-Token in the request header and provide the necessary parameters in the request. The response content will be in JSON format, and special entity classes will be provided in a table at the end. For parameters involving time, use timestamps. Default Request Header Format, Special Cases Noted The whaleal-Token is obtained from the login API call and should be included in the request header for subsequent API calls. Retrieve whaleal-Token from Login API KEY VALUE Accept-Encoding gzip,deflate,br Connection keep-alive Content-Type application/json whaleal-token \"token\" 1. Get SMTP Settings 1.1 Request Path GET: http://{Server-Host}:{Port}/api/server/configuration/getSmtp 1.2 Request 1.3 Response Description Schema code Status code: 1000 for success, other codes for errors int data Returned data SmtpEntity 2. Update SMTP Settings 2.1 Request Path POST: http://{Server-Host}:{Port}/api/server/configuration/updateSmtp 2.2 Request Parameters: Name Located in Description Required Schema smtpEntity Body SMTP entity Yes SmtpEntity Example SMTP Entity: { \"id\": \"630864e29c477153b441b426\", \"createTime\": 0, \"updateTime\": 0, \"port\": \"465\", \"host\": \"smtp.qiye.163.com\", \"from\": \"notifications@jinmuinfo.com\", \"title\": \"whaleal\", \"userName\": \"notifications@jinmuinfo.com\", \"password\": \"89k235Xwma9caArk\", \"default_encoding\": \"utf-8\", \"propertiesMailSmtpSSLEnable\": true, \"propertiesMailSmtpSSLRequired\": true, \"propertiesMailSmtpPort\": null } 2.3 Response Description Schema code Status code: 1000 for success, other codes for errors int msg Returned message String 3. Get Granularity Settings 3.1 Request Path GET: http://{Server-Host}:{Port}/api/server/configuration/getConfig 3.2 Request 3.3 Response Description Schema code Status code: 1000 for success, other codes for errors int data Returned data ConfigEntity Example ConfigEntity: { \"code\": 1000, \"data\": { \"id\": \"whaleal\", \"createTime\": 0, \"updateTime\": 0, \"hostAcquisitionFrequency\": 2, \"mongoAcquisitionFrequency\": 2, \"logSaveTime\": 3600 } } 4. Update Granularity Settings 4.1 Request Path POST: http://{Server-Host}:{Port}/api/server/configuration/updateConfig 4.2 Request Parameters: Name Located in Description Required Schema configEntity Body Configuration entity Yes ConfigEntity Example Configuration Entity: { \"hostAcquisitionFrequency\": 2, \"mongoAcquisitionFrequency\": 2, \"logSaveTime\": 3600 } 4.3 Response Description Schema code Status code: 1000 for success, other codes for errors int msg Returned message String "},"whalelaPlatform/07-APIReference/Analysis.html":{"url":"whalelaPlatform/07-APIReference/Analysis.html","title":"Analysis","keywords":"","body":"Analysis API When calling the API, you need to set the whaleal-Token in the request header, and the response content will be in JSON format. All time-related parameters are expected to be in the form of timestamps. Default Request Header Format KEY VALUE Accept-Encoding gzip,deflate,br Connection keep-alive Content-Type application/json whaleal-token \"token\" 1 Data Analysis 1.1 Request Path GET: http://{Server-Host}:{Port}/api/server/analysis/analysis// 1.2 Request Parameters type: hostAndClusterAndNodeCount, mongoClusterTypeNum, mongoNodeVersionTypeNum, mongoNodeStatusNum, enterpriseAndCommunityNum, mongoClusterQPS, mongoClusterConnUS, mongoClusterDataSize, mongoClusterStorageSize, hostCPU_US, hostDiskIO, hostNetIn, hostNetOut, hostAlert, mongoAlert Name Located in Description Required Schema type Path Type Yes String count Path Count Yes int 1.3 Response Description Schema code Status code: 1000 for success, others for exceptions int data Returned data Document { \"code\": 1000, \"data\": { \"hostCount\": [ 8 ], \"mongoClusterCount\": [ 4 ], \"mongoNodeCount\": [ 11 ] }, \"createTime\": [ 1664150400000 ], \"name\": \"hostAndClusterAndNodeCount\", \"message\": \"Comparison chart of host, cluster, and node counts\" } "},"whalelaPlatform/07-APIReference/Project.html":{"url":"whalelaPlatform/07-APIReference/Project.html","title":"Project","keywords":"","body":"Project API To call this API, you need to set the whaleal-Token in the request header with the specified parameters. The returned content is in JSON format. Special entity classes for the response will be provided in the final table. Parameters related to time are passed in timestamp format. Default Request Header Format, Special Cases are Specified whaleal-Token is obtained when calling the login API and should be included in the request header for subsequent API calls. Login API to Obtain whaleal-Token KEY VALUE Accept-Encoding gzip,deflate,br Connection keep-alive Content-Type application/json whaleal-token \"token\" 1 Save Project 1.1 Request Path POST: http://{Server-Host}:{Port}/api/server/project/saveProject 1.2 Request Parameters: Name Located in Description Required Schema projectMongoEntity Body Project Entity Yes ProjectMongoEntity Example: Save a project. projectMongoEntity looks like this: { \"projectName\": \"test\", \"hostInfo\": [ { \"id\": \"6305fa4491c2f64abf18c581\", \"name\": \"server100\" } ], \"memberInfo\": [ { \"id\": \"630321262ef5221f75e9f0c6\", \"name\": \"chen\" } ], \"mongoInfo\": [ { \"id\": \"632ae4e496b892559b6a7aab\", \"name\": \"whaleal\" } ], \"dingDingList\": [ \"qwe123\" ] } 1.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int data Response data ProjectMongoEntity { \"msg\": \"Save success\", \"code\": 1000, \"data\": { \"id\": \"test\", \"createTime\": 1664245657405, \"updateTime\": 1664245657405, \"projectName\": \"test\", \"hostInfo\": [ { \"id\": \"630eddeff3d9e72e3695ea48\", \"name\": \"chen\" } ], \"mongoInfo\": [ { \"id\": \"632ae4e496b892559b6a7aab\", \"name\": \"whaleal\" } ], \"memberInfo\": [ { \"id\": \"63031cb149d5ad2d50af5d15\", \"name\": \"admin\" } ], \"dingDingList\": [ \"qwe123\" ] } } 2 Delete Project 2.1 Request Path POST: http://{Server-Host}:{Port}/api/server/project/deleteProject/ 2.2 Request Parameters: Name Located in Description Required Schema projectId Path Project ID Yes String 2.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int msg Response message String 3 Get All Projects 3.1 Request Path GET: http://{Server-Host}:{Port}/api/server/project/findAllProject// 3.2 Request Parameters: Name Located in Description Required Schema pageSize Path Page size Yes int pageIndex Path Page index Yes int projectName Params Project name No String hostName Params Host name No String mongoClusterName Params MongoDB cluster name No String memberName Params Member name No String 3.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int data Response data List { \"code\": 1000, \"data\": [ { \"id\": \"project\", \"createTime\": 1664245537831, \"updateTime\": 1664245537831, \"projectName\": \"project\", \"hostInfo\": [ { \"id\": \"630eddeff3d9e72e3695ea48\", \"name\": \"chen\" } ], \"mongoInfo\": [ { \"id\": \"632ae4e496b892559b6a7aab\", \"name\": \"whaleal\" } ], \"memberInfo\": [ { \"id\": \"63031cb149d5ad2d50af5d15\", \"name\": \"admin\" } ], \"dingDingList\": [ \"qwe123\" ] } ] } 4 Get Total Number of Projects 4.1 Request Path GET: http://{Server-Host}:{Port}/api/server/project/countAllProject 4.2 Request Parameters: Name Located in Description Required Schema projectName Params Project name No String hostName Params Host name No String mongoClusterName Params MongoDB cluster name No String memberName Params Member name No String 4.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int data Response count int 5 Get Project by ID 5.1 Request Path GET: http://{Server-Host}:{Port}/api/server/project/findProjectById/ 5.2 Request Parameters: Name Located in Description Required Schema projectId Path Project ID Yes String 5.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int data Response data ProjectMongoEntity { \"code\": 1000, \"data\": { \"id\": \"qwe\", \"createTime\": 1663899341009, \"updateTime\": 1663899341009, \"projectName\": \"qwe\", \"hostInfo\": [ { \"id\": \"630eddeff3d9e72e3695ea48\", \"name\": \"chen\" }, { \"id\": \"6305fa4491c2f64abf18c581\", \"name\": \"server100\" } ], \"mongoInfo\": [ { \"id\": \"632ae4e496b892559b6a7aab\", \"name\": \"whaleal\" } ], \"memberInfo\": [ { \"id\": \"632138e2eef4de695d5116db\", \"name\": \"chen123\" }, { \"id\": \"630321262ef5221f75e9f0c6\", \"name\": \"chen\" } ], \"dingDingList\": [] } } 6 Update Project 6.1 Request Path POST: http://{Server-Host}:{Port}/api/server/project/updateProject 6.2 Request Parameters: Name Located in Description Required Schema projectMongoEntity Body Project Entity Yes ProjectMongoEntity Example: Update a project. projectMongoEntity looks like this: { \"id\": \"qwe\", \"projectName\": \"qwe\", \"hostInfo\": [ { \"id\": \"630eddeff3d9e72e3695ea48\", \"name\": \"chen\" }, { \"id\": \"6305fa4491c2f64abf18c581\", \"name\": \"server100\" } ], \"mongoInfo\": [ { \"id\": \"632ae4e496b892559b6a7aab\", \"name\": \"whaleal\" } ], \"memberInfo\": [ { \"id\": \"632138e2eef4de695d5116db\", \"name\": \"chen123\" }, { \"id\": \"630321262ef5221f75e9f0c6\", \"name\": \"chen\" } ], \"dingDingList\": [] } 6.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int data Response data ProjectMongoEntity { \"msg\": \"Update success\", \"code\": 1000, \"data\": { \"id\": \"qwe\", \"createTime\": 1664248185424, \"updateTime\": 1664248185424, \"projectName\": \"qwe\", \"hostInfo\": [ { \"id\": \"630eddeff3d9e72e3695ea48\", \"name\": \"chen\" }, { \"id\": \"6305fa4491c2f64abf18c581\", \"name\": \"server100\" } ], \"mongoInfo\": [ { \"id\": \"632ae4e496b892559b6a7aab\", \"name\": \"whaleal\" } ], \"memberInfo\": [ { \"id\": \"632138e2eef4de695d5116db\", \"name\": \"chen123\" }, { \"id\": \"630321262ef5221f75e9f0c6\", \"name\": \"chen\" } ], \"dingDingList\": [] } } 7 Get Members in Project 7.1 Request Path GET: http://{Server-Host}:{Port}/api/server/project/getMemberInProject 7.2 Request 7.3 Response Result Description Schema code Status: 1000 for success, others for exceptions int data Response data List "},"whalealData/":{"url":"whalealData/","title":"Whaleal Data","keywords":"","body":"Whaleal-data Introduction Whaleal-data is an archiving platform designed for data archiving. It supports three types of archiving: cold, warm, and S3. For warm data archiving, it supports synchronizing data from MYSQL, Oracle, DB2, and MongoDB to MongoDB. It also supports synchronizing data from MYSQL to MYSQL. Cold data archiving allows archiving MongoDB data to disk files. S3 archiving supports uploading MongoDB Gridfs data to target S3 storage. The platform's homepage displays task execution statistics, archive capacity statistics, table job statistics, business connections, total archive capacity, and user operations. The platform enables independent configuration of data sources and target sources. Users can choose a source to sync data from and a target to sync data to within a table job. After configuring table jobs, multiple table jobs can be configured within a single task configuration. Once tasks are created, they can be managed in the task scheduling section by enabling, disabling, immediately executing, or taking tasks offline. After immediate execution, the corresponding task execution details can be viewed in the task monitoring section. Administrator users have access to various operational details. Platform Architecture Diagram Software Structure Diagram "},"whalealData/InstallationDeployment/InstallationRequirements.html":{"url":"whalealData/InstallationDeployment/InstallationRequirements.html","title":"InstallationRequirements","keywords":"","body":"Installation Requirements Hardware Requirements Operating System: Windows 10 or later, Linux distributions (such as Ubuntu, CentOS), MacOS. Processor: Intel Core i5 or higher. Memory: At least 8GB RAM. Storage Space: At least 100GB of available disk space. Network Adapter: Supports wired or wireless network connections. Network Requirements Network Access Requirements Configure according to your specific needs. Port Requirements Specific ports need to be open (e.g., port 80 for HTTP communication, port used for program startup). Software Requirements Operating System Requirements Supports Windows Server 2016 or later. Supports CentOS 7 or higher for Linux. Browser Support Google Chrome version 80 or higher. Mozilla Firefox version 75 or higher. "},"whalealData/InstallationDeployment/JDKInstallationDeployment.html":{"url":"whalealData/InstallationDeployment/JDKInstallationDeployment.html","title":"JDKInstallationDeployment","keywords":"","body":"JDK Installation and Deployment It is recommended to install JDK 11. 1. Dependency Environment For open-source JDK, font library support is required. If it is already present on the Linux system, there's no need to install it. yum install fontconfig fc-cache --force fc-cache -f 2. Extract JDK Installation Package tar -zxvf jdk-11.0.9_linux-x64_bin.tar.gz -C /usr/local/ 3. Configure Environment Variables Open the profile configuration file: vi /etc/profile Add the following configurations at the end of the file: export JAVA_HOME=/usr/local/jdk-11.0.9 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin Refresh the configuration to make it effective: source /etc/profile 4. Verification Check the installed Java version: java -version Please note that when copying and pasting these commands, ensure that the formatting remains consistent, and adjust paths and filenames as needed for your system. "},"whalealData/InstallationDeployment/MYSQLInstallationDeployment.html":{"url":"whalealData/InstallationDeployment/MYSQLInstallationDeployment.html","title":"MYSQLInstallationDeployment","keywords":"","body":"MySQL Installation and Deployment It is recommended to use MySQL version 8.0. Opening Specified Ports or Disabling Firewall View the ports that are already open: firewall-cmd --list-ports Open a specified port (e.g., port 3306 for MySQL): firewall-cmd --zone=public --add-port=3306/tcp --permanent Reload the firewall configuration: firewall-cmd --reload Confirm the opened ports: firewall-cmd --list-ports If needed, you can stop the firewall: systemctl stop firewalld Check the firewall status: systemctl status firewalld Basic Environment Preparation Create a user and group for MySQL: groupadd mysql useradd -r -g mysql -s /sbin/nologin mysql Install dependencies for MySQL: yum install -y libncurses* libaio* lrzsz* Extract the MySQL installation package: tar -xvf mysql-8.0.28-linux-glibc2.12-x86_64.tar -C /usr/local/ Rename the extracted directory: mv mysql-8.0.28-linux-glibc2.12-x86_64/ mysql Create required directories: cd /usr/local/mysql/ mkdir data Change directory ownership: chown -R mysql:mysql /usr/local/mysql/ Deploy MySQL Service Initialize the database: /usr/local/mysql/bin/mysqld --user=mysql --basedir=/usr/local/mysql/ --datadir=/usr/local/mysql/data/ --initialize Edit my.cnf configuration: Create/Edit the configuration file /etc/my.cnf and add the following content: [mysqld] basedir=/usr/local/mysql datadir=/usr/local/mysql/data socket=/usr/local/mysql/data/mysql.sock bind-address = 0.0.0.0 user=root port=3306 log-bin=mysql-bin server-id=1 max_connections=2048 character-set-server=utf8 default-storage-engine=INNODB [client] socket=/usr/local/mysql/data/mysql.sock Configure environment variables: echo \"export PATH=$PATH:/usr/local/mysql/bin\" >> /etc/profile source /etc/profile Configure startup script: cp /usr/local/mysql/support-files/mysql.server /etc/rc.d/init.d/mysqld chmod +x /etc/rc.d/init.d/mysqld cat > /lib/systemd/system/mysqld.service Reload systemd configuration: systemctl daemon-reload Set MySQL to start on boot: systemctl enable mysqld Start MySQL: systemctl start mysqld Check if MySQL port is active: netstat -tunlp | grep 3306 Configure Password for Remote Connection Enter the printed password to log in to MySQL: mysql -u root -p After logging in, change the root password: ALTER USER 'root'@'localhost' IDENTIFIED BY '123456'; Check user information: select user, host, ssl_type from mysql.user; use mysql; Update the host field to % to allow remote connections: update user set host = '%' where user = 'root'; Refresh privileges: flush privileges; Adding Archive Platform Fields Log in to MySQL: mysql -u root -p Create a database: create database filing; Add data from the provided SQL file: use filing; source /usr/local/filing.sql; Check the added tables: use filing; show tables; As always, ensure that you adapt paths, filenames, and other specifics to match your system's configuration. "},"whalealData/InstallationDeployment/NginxInstallationDeployment.html":{"url":"whalealData/InstallationDeployment/NginxInstallationDeployment.html","title":"NginxInstallationDeployment","keywords":"","body":"Nginx Installation and Deployment Opening Specified Ports or Disabling Firewall View the ports that are already open: firewall-cmd --list-ports Open a specified port (e.g., port 80 for Nginx): firewall-cmd --zone=public --add-port=80/tcp --permanent Reload the firewall configuration: firewall-cmd --reload Confirm the opened ports: firewall-cmd --list-ports If needed, you can stop the firewall: systemctl stop firewalld Check the firewall status: systemctl status firewalld Installation and Deployment Extract the Nginx installation package: tar -zxvf nginx-1.16.1.tar.gz -C /usr/local/ Install dependencies: yum install -y pcre pcre-devel yum install -y zlib zlib-devel Configure the installation path: cd /usr/local/nginx-1.16.1 ./configure --prefix=/usr/local/nginx Compile Nginx: make && make install Configure local hostname resolution: Edit /etc/hosts and add an entry for your local domain: ip cloud.whalealmg.com Edit the Nginx configuration file: server { listen 80; server_name cloud.whalealmg.com; location / { root /usr/local/nginx/html/dist/; index index.html index.htm; try_files $uri $uri/ /index.html; } location /filingAdmin/ { proxy_pass http://127.0.0.1:8000/; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } location ~ .*\\.(js|css|jpg|jpeg|gif|png|ico|pdf|txt)$ { root /usr/local/nginx/html/dist/; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } Start the Nginx service: /usr/local/nginx/sbin/nginx Make sure to adjust paths, domain names, and other configurations as needed for your specific environment. "},"whalealData/InstallationDeployment/RedisInstallationDeployment.html":{"url":"whalealData/InstallationDeployment/RedisInstallationDeployment.html","title":"RedisInstallationDeployment","keywords":"","body":"Redis Installation and Deployment Opening Specified Ports or Disabling Firewall View the ports that are already open: firewall-cmd --list-ports Open a specified port (e.g., port 6379 for Redis): firewall-cmd --zone=public --add-port=6379/tcp --permanent Reload the firewall configuration: firewall-cmd --reload Confirm the opened ports: firewall-cmd --list-ports If needed, you can stop the firewall: systemctl stop firewalld Check the firewall status: systemctl status firewalld Installation and Deployment Extract the Redis installation package: tar -zxvf redis-4.0.9.tar.gz -C /usr/local/ Rename the extracted folder: mv redis-4.0.9 redis Install the required dependencies (e.g., GCC): yum install gcc -y Compile the Redis files: cd /usr/local/redis make && make install Edit Configuration File Edit the Redis configuration file: vi redis.conf Set a password (e.g., \"123456\"): # Before # requirepass foobared # After requirepass 123456 Enable background daemon mode: # Before # daemonize no # After daemonize yes Allow remote access: # Before # bind 127.0.0.1 # After bind 0.0.0.0 Save the configuration file and exit the editor. Start Redis Start Redis using the modified configuration file: redis-server /usr/local/redis/redis.conf Validate the Redis server is running: redis-cli Make sure to adjust paths, passwords, and other configurations as needed for your specific environment. "},"whalealData/InstallationDeployment/ZookeeperInstallationDeployment.html":{"url":"whalealData/InstallationDeployment/ZookeeperInstallationDeployment.html","title":"ZookeeperInstallationDeployment","keywords":"","body":"Zookeeper Installation and Deployment Opening Specific Ports or Disabling Firewall Check already opened ports: firewall-cmd --list-ports Open a specific port: firewall-cmd --zone=public --add-port=2181/tcp --permanent Reload firewall configuration: firewall-cmd --reload Confirm opened ports: firewall-cmd --list-ports Stop the firewall: systemctl stop firewalld Check firewall status: systemctl status firewalld Installation and Deployment Unpack the installation package: tar -zxvf apache-zookeeper-3.6.1-bin.tar.gz -C /usr/local/ Rename the extracted folder: mv apache-zookeeper-3.6.1-bin/ zookeeper Start Zookeeper: /usr/local/zookeeper/bin/zkServer.sh start /usr/local/zookeeper/conf/zoo_sample.cfg Verify Zookeeper status: /usr/local/zookeeper/bin/zkServer.sh status /usr/local/zookeeper/conf/zoo_sample.cfg This installation guide provides steps for deploying Zookeeper, opening the required ports, and starting the service. Make sure to follow each step carefully to ensure a successful deployment. "},"whalealData/InstallationDeployment/Whaleal-dataInstallationDeployment.html":{"url":"whalealData/InstallationDeployment/Whaleal-dataInstallationDeployment.html","title":"Whaleal-dataInstallationDeployment","keywords":"","body":"Installation and Deployment of Whaleal-data High Availability Deployment To achieve high availability, deploy the service on multiple machines and distribute traffic through a load balancer to balance and share the requests. Common load balancing algorithms include round-robin, least connections, and hash algorithms. Use multiple servers with the same configuration to maintain system continuity by having other servers take over in case of a failure. Common redundancy backup modes include master-slave mode, active-active mode, and N+1 mode. Package Deployment Frontend Service Startup After compiling the source code, generate the \"dist\" distribution package. Send the \"dist\" package to the server. Path: The installation path configured in the Nginx configuration. Restart Nginx /usr/local/nginx/sbin/nginx -s reload -t Backend Service Startup After compiling the source code, generate the \"filing-system-0.0.1-SNAPSHOT.jar\" distribution package. Upload the distribution package to the server. Edit the configuration file \"application.yml\". Configuration File Content # Application server port server: port: 8000 # Database and other configurations... Start the Service nohup java -jar -Xms2048M -Xmx20000M -XX:PermSize=768M -XX:MaxPermSize=1536M -server -jar filing-system-0.0.1-SNAPSHOT.jar --spring.config.location=application.yml --jasypt.encryptor.password=SfXlqZmK4P257 & Check Logs for Successful Startup tail -f nohup.out Docker Container Deployment Navigate to the directory containing the docker-compose.yml file. Start the service using the command: docker-compose up -d. After the Docker service starts successfully, you can view the logs using the command: docker logs -f root_whaleal-data_1. For local access, bind the server's IP with the domain name in the hosts file using: sudo sh -c 'echo \"docker_server_ip whaleal-data.com\" >> /etc/hosts'. Access the Whaleal-data service: Web URL: http://docker_server_ip or http://whaleal-data.com Initial login: User: \"admin\" Password: \"123456\" The system will force you to change the password upon first login. Tips: Cold Data Archiving: The default path for cold data archiving is /whalealdb. For Docker, the service is mapped to an external path /opt/whalealdb. Quick Access Start the Whaleal-data service using Docker containers. This service depends on mysql, mongodb, redis, and zookeeper services. It runs in a local browser through the nginx service proxy. "},"whalealData/UserManual/LoginPage/UserFirstLogin.html":{"url":"whalealData/UserManual/LoginPage/UserFirstLogin.html","title":"UserFirstLogin","keywords":"","body":"First-Time User Login During the system initialization, the administrator account is set as \"admin\" with the password \"123456\". When logging in for the first time, users will be prompted to change their password. After modifying the password during the initial login, users can proceed to log in with the new password. "},"whalealData/UserManual/LoginPage/UserRegistration.html":{"url":"whalealData/UserManual/LoginPage/UserRegistration.html","title":"UserRegistration","keywords":"","body":"User Registration The platform supports self-registration of user accounts. After registering, the administrator user can assign resource permissions to new users. "},"whalealData/UserManual/HomepageDisplay/HomepageDisplay.html":{"url":"whalealData/UserManual/HomepageDisplay/HomepageDisplay.html","title":"HomepageDisplay","keywords":"","body":"Homepage Display The homepage displays various statistics and information related to the system's activities. Users can customize the time range for which they want to see the data. Here's a breakdown of the different sections on the homepage: Task Execution Statistics The \"Task Execution Statistics\" pie chart depicts the distribution of task execution results within a specific time range. Task execution statuses include running (in progress), succeeded (successfully completed), failed (execution failed), and other (remaining types). Archiving Capacity Statistics The \"Archiving Capacity Statistics\" section displays the total size of data synchronized from different source types to various destination types. The first column represents source types, the second column represents destination types, and the third column represents either cold data destination files or S3. The capacity statistics depend on the source and destination types, and certain combinations are supported based on compatibility. User Activity Statistics The \"User Activity Statistics\" section scrolls through and displays the actions taken by a user during a specific time period. It showcases login actions, task executions, and other relevant activities performed by users. Table Job Statistics The \"Table Job Statistics\" section presents a line chart depicting the quantity of different types of table jobs executed within the chosen time range. This helps users observe recent trends in task execution. Users can select different job types from the dropdown menu to view the corresponding line chart. Business Access (Number of Sources) The \"Business Access (Number of Sources)\" section displays the number of data source accesses made to the platform during the specified time period. It shows the growth trend of data source accesses over time. Similarly, users can choose different data source types from the dropdown menu. Total Archiving Volume The \"Total Archiving Volume\" section presents a line chart illustrating the total data volume archived by the platform within the selected time range. This helps users track the growth trend of archiving volume over time. "},"whalealData/UserManual/ConfigurationManagement/DataSourceManagement.html":{"url":"whalealData/UserManual/ConfigurationManagement/DataSourceManagement.html","title":"DataSourceManagement","keywords":"","body":"Data Source Management Adding a Data Source To add a new data source, navigate to \"Data Source Management\" under the \"Configuration Management\" menu. Here, you can view all the information about existing data sources. Click on the \"New\" button in the first image to bring up the form shown in the second image, where you can input the details of the new data source. Data sources can include MongoDB, MYSQL, Oracle, and DB2. Provide the required username and password for connecting to the database. The \"Options\" field contains optional settings for configuring specific connection options for the database. Click the \"Test\" button to verify if the provided user information can connect to the database. Finally, click \"Confirm\" to save the data source. Modifying a Data Source Click the blue button in the right-hand side action column to edit a data source. The form for editing a data source is similar to the form for adding a data source. After making the necessary modifications, click \"Test.\" If the test is successful, click \"Confirm\" to save the changes. Deleting a Data Source Click the red button in the right-hand side action column to delete a data source. A confirmation prompt will appear. If you intend to delete the data source, click \"Confirm.\" If you clicked by mistake, you can click \"Cancel.\" Binding Data Source to Users Navigate to \"User Management\" under \"System Management\" to display all user information. Click on the user you want to operate on, and then check the data sources you want to bind to this user on the right side. Afterward, click \"Save\" to bind the selected data sources to the user. "},"whalealData/UserManual/ConfigurationManagement/DestinationSourceManagement.html":{"url":"whalealData/UserManual/ConfigurationManagement/DestinationSourceManagement.html","title":"DestinationSourceManagement","keywords":"","body":"Target Source Management Adding Warm/Cold/S3 Data Target Sources To add a new target source, navigate to \"Target Source Management\" under the \"Configuration Management\" menu. If you want to add a warm data target source, click on \"Warm Data Target Source.\" For cold data target sources, click on \"Cold Data File Source.\" Similarly, for S3 target sources, click on \"S3.\" Then, click the \"New\" button to bring up the form for adding a new target source. The process for cold data and S3 is similar to adding a data source. After completing the necessary details, click \"Confirm\" to save the target source. For cold data file sources, click \"Confirm\" after filling in the required information. Modifying Warm/Cold/S3 Data Target Sources Click the blue button to edit a target source. The process for modifying warm data target sources and S3 is similar to modifying a data source. For cold data file sources, modify the path and click \"Confirm\" to save the changes. Deleting Warm/Cold/S3 Data Target Sources Click the red button to delete a target source. A confirmation prompt will appear. If you intend to delete the target source, click \"Confirm.\" If not, click \"Cancel.\" Binding Warm/Cold/S3 Data Target Sources to Users The process of binding target sources is similar to binding data sources. If you want to bind a target source, click on the target source, and then check the relevant sources. The process is the same for file sources and S3. After that, click \"Save\" to grant the user access to the selected sources. "},"whalealData/UserManual/ConfigurationManagement/TableJobConfiguration.html":{"url":"whalealData/UserManual/ConfigurationManagement/TableJobConfiguration.html","title":"TableJobConfiguration","keywords":"","body":"Table Job Configuration Adding Warm/Cold/S3 Jobs To configure a new table job, navigate to \"Table Job Configuration\" under the \"Configuration Management\" menu. If you want to configure a warm data table job, click on \"Warm Data Table Job Configuration.\" For cold data table job configuration, click on \"Cold Data Table Job Configuration.\" Similarly, for S3 table job configuration, click on \"S3 Table Job Configuration.\" Click the type that you want to add, and then a table will appear displaying the respective job information. The process for adding each type of job is similar. After selecting the data source and target source, you can choose options such as consistency verification, archiving method, and data processing mode. If you choose consistency verification, you can fill in the verification percentage. When the archiving mode is \"Full Update,\" you don't need to enter SQL conditions, as the platform archives the entire table data directly. When the archiving mode is \"Incremental Update,\" an input field for SQL conditions will appear along with a \"Custom SQL Configuration\" button, as shown in the third image. If you want to configure custom SQL, you can click the button to enter the visual configuration interface for table fields, as shown in the fourth image. When both the data source and target source are MongoDB, you can choose Gridfs. If Gridfs is enabled, MongoDB's Gridfs data type will be synchronized. Editing Warm/Cold/S3 Jobs Click the blue button on the right to edit a job. After making the necessary changes to the job, click \"Save\" to save the modifications. Deleting Warm/Cold/S3 Jobs Click the red button on the right to delete a job. A confirmation prompt will appear. If you intend to delete the job, click \"Confirm.\" If not, click \"Cancel.\" Searching Warm/Cold/S3 Jobs Click the search button at the top and fill in the first three condition boxes to filter all table job configurations that meet the specified criteria on the platform. Viewing Warm/Cold/S3 Jobs Click the \"View\" button on the right to see the job configuration details. However, you won't be able to perform any actions in this view. "},"whalealData/UserManual/ConfigurationManagement/TaskConfiguration.html":{"url":"whalealData/UserManual/ConfigurationManagement/TaskConfiguration.html","title":"TaskConfiguration","keywords":"","body":"Task Configuration Adding Warm/Cold/S3 Tasks To configure a new task, go to \"Task Configuration\" under the \"Configuration Management\" menu. Click on \"Warm Data Task Configuration\" to view all warm data archiving task information. Click on \"Cold Data Task Configuration\" to view all cold data archiving task information. Similarly, click on \"S3 Task Configuration\" to view all S3 archiving task information. After selecting the type of task you want to add, click the \"Add\" button to bring up the table shown in the second image. Click \"Add Job\" to show the already configured table job configuration, and then associate the desired table jobs with the task configuration, as shown in the third image. A task can have multiple table jobs associated with it. You can choose between manual tasks, recurring tasks, and one-time tasks. For one-time tasks, you can set the execution time using a Cron expression, while recurring tasks must have a Cron expression for scheduling. Since a task can have multiple table jobs, you can configure the execution mode to be either serial or parallel. You can also set the task timeout and configure the number of retries in case of failure. Additionally, you can set up email notifications for task success or failure. Once the configuration is complete, as shown in the fourth image, click \"Confirm.\" After configuration, the task needs to be reviewed by a management user. Editing Warm/Cold/S3 Tasks Click the edit button to open the table shown in the first image. This task configuration can be edited or deleted until it is reviewed by a management user. Once reviewed, the task cannot be edited or deleted. After editing the task, click \"Confirm\" to save the changes. Deleting Warm/Cold/S3 Tasks Before being reviewed by a management user, tasks can be edited or deleted. Click the red \"Delete\" button to show a confirmation prompt. If you are sure you want to delete the task, click \"Delete.\" If not, click \"Cancel.\" Searching Warm/Cold/S3 Tasks There are two condition boxes before the search button. Fill in the criteria and click \"Search\" to filter the task configurations that match the specified criteria. "},"whalealData/UserManual/TaskManagement/TaskScheduling.html":{"url":"whalealData/UserManual/TaskManagement/TaskScheduling.html","title":"TaskScheduling","keywords":"","body":"Task Scheduling Clicking on \"Task Scheduling\" under the \"Task Management\" menu will display a page that shows tasks that have been approved. The search button with associated search criteria boxes can be used to filter and display tasks based on the specified criteria. For individual tasks, you can modify their status, such as enabling, disabling, taking them offline, or executing them immediately. Enable Task After a task has been disabled, it cannot be executed. You can enable a disabled task by clicking the \"Enable Task\" button on the right side of the task. After enabling the task, you can click the \"Run Now\" button to execute the task immediately or let it run automatically at the scheduled time. Disable Task You can click the gray button on the right side of a task to disable it. Once a task is disabled, it will not be executed. You need to enable the task again for it to resume normal operation. Run Now The third button on the right side of a task allows you to execute it immediately. For full synchronization tasks, clicking this button will initiate the synchronization immediately. For incremental synchronization tasks, clicking \"Run Now\" will execute the next scheduled task that has not yet reached its execution time. Take Task Offline The rightmost button, \"Take Task Offline,\" allows you to take a task offline. Clicking this button will prompt a confirmation dialog to confirm the offline operation. Once a task is taken offline, it cannot be restored, and the task will become unavailable. If you want to proceed with taking the task offline, click \"Take Offline.\" If you want to cancel, click \"Cancel.\" "},"whalealData/UserManual/TaskManagement/WarmTaskMonitoring.html":{"url":"whalealData/UserManual/TaskManagement/WarmTaskMonitoring.html","title":"WarmTaskMonitoring","keywords":"","body":"Task Monitoring (Warm) Clicking on \"Task Monitoring (Warm)\" under the \"Task Management\" menu will display a page that shows the execution status of warm tasks. The page includes information about completed tasks, tasks in progress, and exceptional tasks. Each search button is associated with condition boxes that allow you to filter and display tasks based on specified criteria. Completed Tasks Clicking on completed tasks will display information about tasks that have been successfully completed. The information includes execution strategy, start and end times, execution duration, execution status, progress percentage, archived records count, source data status, and executed SQL statements. There are four buttons at the top: Search, Modify Source Data Status, Manually Delete Source Data, and Refresh. Search The green button at the top is the search button. By entering conditions in the provided boxes and clicking on the search button, you can filter and display completed tasks that match the criteria. Modify Source Data Status After a synchronization is completed and if source data has been manually deleted, you can click the yellow button to modify the source data status to \"processed.\" Manually Delete Source Data The red button allows you to manually delete source data. If automatic deletion is not configured in the table job settings, you can manually delete the source data from the database. Alternatively, you can click the \"Manually Delete Source Data\" button after selecting a task. Refresh The progress percentage of a task is updated every 3 seconds. As a result, the progress bar display might not be real-time. Clicking the refresh button updates the progress bar and some task statuses. Tasks in Progress Clicking on tasks in progress will display information about tasks that are currently being executed and archived. The information includes execution strategy, start and end times, execution duration, execution status, progress percentage, archived records count, executed SQL statements. There are three buttons: Search, Terminate Task, and Verify Task Status. Search The green button is the search button. Enter conditions in the provided boxes and click the search button to filter and display tasks in progress that match the criteria. Terminate Task The red button allows you to terminate a task. After selecting a task and clicking the \"Terminate Task\" button, the task will be terminated. The task will then appear in the exceptional tasks section if it was not completed normally. Verify Task Status A task can include multiple table jobs. When one table job is completed, the next one starts. If a task's status does not update promptly after a table job is completed, you can click the \"Verify Task Status\" button to update the task's status. Exceptional Tasks Clicking on exceptional tasks will display information about tasks that encountered exceptions. The information includes execution strategy, start and end times, execution duration, execution status, error details, progress percentage, archived records count, executed SQL statements, and rollback status. There are three buttons: Search, Rollback, and Re-execute. Search The green button is the search button. Enter conditions in the provided boxes and click the search button to filter and display exceptional tasks that match the criteria. Rollback Each exceptional task has a rollback button. Clicking the rollback button for a sub-task will roll back the exceptional data that was synchronized. Clicking the rollback button for a parent task will roll back all sub-tasks under that parent task. Re-execute Sub-tasks under an exceptional task have a re-execute button. Clicking the re-execute button will generate a new parent task. The exceptional task and the new parent task will be linked. After rolling back the exceptional data, both the exceptional task and the new parent task will appear in the \"Tasks in Progress\" section for re-execution. "},"whalealData/UserManual/TaskManagement/ColdTaskMonitoring.html":{"url":"whalealData/UserManual/TaskManagement/ColdTaskMonitoring.html","title":"ColdTaskMonitoring","keywords":"","body":"Task Monitoring (Cold) Task Status Clicking on \"Task Monitoring (Cold)\" under the \"Task Management\" menu will display information about the execution status of cold tasks. This page includes information about completed tasks, ongoing tasks, and tasks with exceptions. Each search button is associated with a set of search criteria, allowing you to filter and display tasks efficiently. Completed Tasks Clicking on \"Completed Tasks\" will display information about tasks that have been successfully completed. This page includes details such as the execution strategy, start and end times, duration, execution status, progress percentage, archived items, archive path, source table data status, and executed SQL statements. The page also features four buttons: search, modify source table data status, manually delete source table data, and refresh. Search The green button at the top is the search button. Enter criteria in the provided search boxes and click the search button to display filtered completed tasks. Modify Source Table Data Status After synchronization is completed, if source table data has been manually deleted, you can click the yellow button to mark the source table data as processed. Manually Delete Source Table Data The red button allows you to manually delete source table data. If the table job configuration does not include automatic deletion and you want to manually delete the source data after synchronization, you can use this button. Refresh The progress percentage of a task is updated every 3 seconds. Clicking the refresh button will update the progress bar and task status. Ongoing Tasks Clicking on \"Ongoing Tasks\" will display information about tasks that are currently in progress. This page includes details such as the execution strategy, start and end times, duration, execution status, progress percentage, archived items, and executed SQL statements. The page features three buttons: search, terminate task, and task status validation. Search The green button is the search button. Fill in the provided search boxes as needed, and then click the search button to display all ongoing tasks that match the criteria. Terminate Task The red button allows you to terminate an ongoing task. After clicking this button, the task will be terminated and will appear in the list of tasks with exceptions. If the termination is successful, the task will also be marked as completed. Task Status Validation Each task can contain multiple table jobs. When one table job is completed, the next one starts. If the task status does not update after a table job has been completed, you can click the \"Task Status Validation\" button to update the task status. Exception Tasks Clicking on \"Exception Tasks\" will display information about tasks that encountered exceptions. This page includes details such as the execution strategy, start and end times, duration, execution status, exception details, progress percentage, archived items, archive path, executed SQL statements, and rollback status. This page features three buttons: search, rollback, and re-execute. Search The green button at the top is the search button. Fill in the provided search boxes with criteria and click the search button to display filtered exception tasks. Rollback Each exception task has a rollback button. Clicking on this button will initiate a rollback of the exception data synchronized by the subtask. If the rollback button of the parent task is clicked, all subtasks under that parent task will be rolled back. Re-execute Each subtask of an exception task has a re-execute button. Clicking this button will generate a new parent task associated with the exception task. The exception task will be linked to the new parent task. After the rollback of the exception data is completed, the task will appear in the ongoing tasks list, and you can re-execute it. "},"whalealData/UserManual/TaskManagement/S3TaskMonitoring.html":{"url":"whalealData/UserManual/TaskManagement/S3TaskMonitoring.html","title":"S3TaskMonitoring","keywords":"","body":"Task Monitoring (S3) Clicking on \"Task Monitoring (S3)\" under the \"Task Management\" menu will display information about the execution status of S3 tasks. This page includes information about completed tasks, ongoing tasks, and tasks with exceptions. Each search button is associated with a set of search criteria, allowing you to filter and display tasks efficiently. Task Status Completed Tasks Clicking on \"Completed Tasks\" will display information about tasks that have been successfully completed. This page includes details such as the execution strategy, start and end times, duration, execution status, progress percentage, archived items, source table data status, and executed SQL statements. The page also features four buttons: search, modify source table data status, manually delete source table data, and refresh. Search The green button at the top is the search button. Enter criteria in the provided search boxes and click the search button to display filtered completed tasks. Modify Source Table Data Status After synchronization is completed, if source table data has been manually deleted, you can click the yellow button to mark the source table data as processed. Manually Delete Source Table Data The red button allows you to manually delete source table data. If the table job configuration does not include automatic deletion and you want to manually delete the source data after synchronization, you can use this button. Refresh The progress percentage of a task is updated every 3 seconds. Clicking the refresh button will update the progress bar and task status. Ongoing Tasks Clicking on \"Ongoing Tasks\" will display information about tasks that are currently in progress. This page includes details such as the execution strategy, start and end times, duration, execution status, progress percentage, archived items, and executed SQL statements. The page features three buttons: search, terminate task, and task status validation. Search The green button is the search button. Fill in the provided search boxes as needed, and then click the search button to display all ongoing tasks that match the criteria. Terminate Task The red button allows you to terminate an ongoing task. After clicking this button, the task will be terminated and will appear in the list of tasks with exceptions. If the termination is successful, the task will also be marked as completed. Task Status Validation Each task can contain multiple table jobs. When one table job is completed, the next one starts. If the task status does not update after a table job has been completed, you can click the \"Task Status Validation\" button to update the task status. Exception Tasks Clicking on \"Exception Tasks\" will display information about tasks that encountered exceptions. This page includes details such as the execution strategy, start and end times, duration, execution status, exception details, progress percentage, archived items, executed SQL statements, and rollback status. This page features three buttons: search, rollback, and re-execute. Search The green button at the top is the search button. Fill in the provided search boxes with criteria and click the search button to display filtered exception tasks. Rollback Each exception task has a rollback button. Clicking on this button will initiate a rollback of the exception data synchronized by the subtask. If the rollback button of the parent task is clicked, all subtasks under that parent task will be rolled back. Re-execute Each subtask of an exception task has a re-execute button. Clicking this button will generate a new parent task associated with the exception task. The exception task will be linked to the new parent task. After the rollback of the exception data is completed, the task will appear in the ongoing tasks list, and you can re-execute it. "},"whalealData/UserManual/ArchiveManagement/ColdTaskLogQuery.html":{"url":"whalealData/UserManual/ArchiveManagement/ColdTaskLogQuery.html","title":"ColdTaskLogQuery","keywords":"","body":"Log Query (Cold) Click on \"Log Query (Cold)\" under the \"Archive Management\" menu to query all file archiving log information. This page includes a search button, search condition fields, and a \"Rewrite\" button for each task. Searching The green button at the top is the search button. The left-hand side condition fields are used for specifying search criteria. After filling in the conditions, click the search button to filter the desired tasks. File Rewriting Click on the yellow \"File Rewrite\" button behind a task to initiate a file rewriting operation. This opens a page where you can click \"Create Rewrite Task\" to create the rewrite task. Creating a Rewrite Task Clicking the \"Create Rewrite Task\" button opens the following form. Fill in the required information and click \"Confirm.\" Delete Temporary Table After creating the rewrite task, you can start the rewriting process. Once it's completed, you can click the \"Delete Temporary Table\" button. A confirmation prompt will appear. If you want to proceed with deletion, click \"Execute.\" If not, click \"Cancel.\" "},"whalealData/UserManual/ArchiveManagement/FileInspectionManagement.html":{"url":"whalealData/UserManual/ArchiveManagement/FileInspectionManagement.html","title":"FileInspectionManagement","keywords":"","body":"File Inspection Management Click on \"File Inspection Management\" under the \"Archive Management\" menu to query all file inspection log information. Click the \"Create Inspection Task\" button to create a file inspection task. The system will perform the necessary file checking and verification logic. For files with exceptions, you can manually update the file path or perform a re-archive operation. Searching The green button at the top is the search button. The left-hand side has condition fields that you can optionally fill in. After filling in the conditions, click the search button to filter the desired inspection. Creating an Inspection Task Click the blue \"Create Inspection Task\" button to open the following form. Fill in the required information and click \"Confirm\" to generate inspection data for the corresponding target source of warm data. After completing the task, click \"View Details\" to see the inspection content as shown in the second image. "},"whalealData/UserManual/ArchiveManagement/FileFullTextSearch.html":{"url":"whalealData/UserManual/ArchiveManagement/FileFullTextSearch.html","title":"FileFullTextSearch","keywords":"","body":"File Full-Text Search Click on \"File Full-Text Search\" under the \"Archive Management\" menu to query all file full-text search log information. Click the \"Create Search Task\" button to create a file full-text search task. The system will perform a comprehensive search for files based on the search criteria, and for the found records, you can perform file rewriting operations. Searching The green button at the top is the search button. The left-hand side condition fields are optional. After filling in the conditions, click the search button to filter the desired search content. Creating a Search Task Click the blue \"Create Search Task\" button to open the following form. Fill in the required information and click \"Confirm.\" "},"whalealData/UserManual/SystemManagement/UserManagement.html":{"url":"whalealData/UserManual/SystemManagement/UserManagement.html","title":"UserManagement","keywords":"","body":"User Management Clicking on \"User Management\" under the \"System Management\" menu, visible only to administrator users, will take you to a page displaying all user information on the platform, including database permissions and bindings. Search The green button is the search button. Fill in the left two search boxes with relevant criteria and click the search button to filter the desired user information. Add User The second blue button is the \"Add\" button. Clicking on it will bring up the interface shown in the image above. Fill in the username, email, phone number, and password to add a new user. In the \"Role\" section, you can select the system permissions for the user, as well as the user's status (enabled or disabled). Export Users The third yellow button is the \"Export Users\" button. Clicking on it will generate an xlsx file containing information for all users. You can customize the file name. Modify User Details Clicking on the pencil icon on the right side of a user's row will allow you to modify user information. An edit page will pop up with editable fields for all user details except for the password. Modify the information as needed and click \"Confirm\" to save the changes. Change Password Clicking on the second password icon on the right side of a user's row will allow you to change the user's password. A form will appear where you can enter the new password. After entering the new password, click \"Confirm\" to save the changes. Delete User Clicking on the third red button on the right side of a user's row will prompt a confirmation message asking if you want to delete the user. If you confirm deletion, click \"Confirm.\" If you don't want to delete the user, click \"Cancel.\" Database Permissions (Data Sources, Target Sources, File Sources, S3) Clicking on a user's row will display the database permissions assigned to that user, including data sources, target sources, file sources, and S3. Check the sources that the user should have access to, then click \"Save.\" "},"whalealData/UserManual/SystemManagement/RoleManagement.html":{"url":"whalealData/UserManual/SystemManagement/RoleManagement.html","title":"RoleManagement","keywords":"","body":"Role Management Clicking on \"Role Management\" under the \"System Management\" menu, which is visible only to administrator users, will take you to a page where you can view information about all role permissions categories in the platform. Search The green button at the top is the search button. On the left side, there are two criteria fields. Fill in these fields and click \"Search\" to filter the desired role list. Add The blue button is the add button. Clicking the \"Add\" button will open the interface as shown in the image. Fill in the role name and role permissions to add role information. Edit Clicking the blue pencil icon on the right side of a role opens the edit role dialog. You can modify the role's name, permissions, or add a description. Click \"Save\" when done. Delete The red button on the right side is the delete button. Clicking the delete button will prompt whether you are sure you want to delete the role. If you confirm deletion, click \"OK\"; if not, click \"Cancel.\" Menu Assignment Clicking on a role reveals the menus associated with that role on the right side. Check and assign menus according to the actual permissions and menus the role should have. Save the data to bind the menus that the role can operate with. "},"whalealData/UserManual/SystemManagement/MenuManagement.html":{"url":"whalealData/UserManual/SystemManagement/MenuManagement.html","title":"MenuManagement","keywords":"","body":"Menu Management Clicking on \"Menu Management\" under the \"System Management\" menu allows administrators to manage the menus within the platform. This page is visible only to administrator users. On this page, you can view information about all the menus, as well as perform actions such as searching, adding, modifying, and deleting menus. Search The green button at the top is the search button. On the left side, there are two criteria fields. Fill in the filtering criteria in these fields, and then click \"Search\" to filter the desired menus. Add The blue button is the \"Add\" button. Clicking it will bring up a form, as shown in the image below. In this form, you can customize the menu type, menu icon, external link menu, menu visibility, menu title, route address, menu sorting, parent directory, and more. Once you've configured the menu, click \"Confirm\" to save it. Edit Clicking the blue pencil icon next to a menu's name will bring up an edit form where you can modify the menu's configuration according to your needs. Once you're done, click \"Confirm\" to save the changes. Delete The delete button for menus is not available, likely to prevent accidental deletion. Instead, menus can be hidden based on requirements. "},"whalealData/UserManual/SystemManagement/SystemSettings.html":{"url":"whalealData/UserManual/SystemManagement/SystemSettings.html","title":"SystemSettings","keywords":"","body":"SMTP Email Settings Clicking on \"System Settings\" under the \"System Management\" menu, which is visible only to administrator users, will take you to a page where SMTP configuration is displayed. After filling in the basic configuration, click the \"Test Connection\" button. If the test is successful and you receive a test email, a \"Save\" button will appear. Once the SMTP configuration is saved, alert emails for task configurations will be sent from the email address configured here. "},"whalealData/UserManual/SystemManagement/OperationLog.html":{"url":"whalealData/UserManual/SystemManagement/OperationLog.html","title":"OperationLog","keywords":"","body":"Operation Logs Search Clicking on \"Operation Logs\" under the \"System Management\" menu, which is visible only to administrator users, will take you to a page displaying operation logs of platform users. The green button is the search button. On the left side, there are criteria fields. Fill in the filtering criteria in these fields, and then click \"Search\" to filter the desired operation logs. "},"whalealData/UserManual/SystemManagement/ErrorLog.html":{"url":"whalealData/UserManual/SystemManagement/ErrorLog.html","title":"ErrorLog","keywords":"","body":"Error Logs Search Clicking on \"System Operation Logs\" under the \"System Management\" menu allows administrators to view error logs related to user actions on the platform. The page displays error logs generated from user actions. The green button is the search button. On the left side, there is a criteria field. Fill in the filtering criteria in the field, and then click \"Search\" to filter out the desired error logs. By clicking the \"View Details\" button on the right, you can view the details of the error and its causes. These error logs help administrators identify and address issues or unexpected behaviors in the system, providing insights into the activities that led to errors. "},"whalealData/UserManual/StatisticalReports/TableJobExecutionStatistics.html":{"url":"whalealData/UserManual/StatisticalReports/TableJobExecutionStatistics.html","title":"TableJobExecutionStatistics","keywords":"","body":"Table Job Execution Statistics Clicking on \"Table Job Execution Statistics\" under the \"Statistics Report\" menu allows you to query the archival information for all table jobs. This page includes a search button, search criteria fields, a clear criteria button, and an export button. Each entry represents a table job execution and includes information such as job name, table name, data source type, target source type, execution start and end times, execution status, exception errors, progress percentage, number of archived rows, archival path, executed SQL, and rollback status. Search The green button in the image above is the search button. On the left side, there are two criteria fields. After filling in the filtering criteria, click \"Search\" to filter out job executions that match the criteria. Clear The black button is the clear button, which clears the criteria fields. Export The yellow button is the export button, allowing you to export the table job execution statistics information as an xlsx file. These features help you keep track of and analyze the execution status and details of various table jobs in the system. Please note that due to the screenshot's resolution, some text may be difficult to read. "},"whalealData/UserManual/StatisticalReports/AbnormalJobExecutionStatistics.html":{"url":"whalealData/UserManual/StatisticalReports/AbnormalJobExecutionStatistics.html","title":"AbnormalJobExecutionStatistics","keywords":"","body":"Abnormal Job Execution Statistics Clicking on the \"Abnormal Job Execution Statistics\" under the \"Statistics Report\" menu allows you to query the archival information of all abnormal table jobs. This page includes a search button, search criteria fields, a clear criteria button, and an export button. Search The green button in the image above is the search button. On the left side, there's a criteria field. After filling in the filtering criteria, click \"Search\" to filter out abnormal table job statistics that match the criteria. Clear The black button is the clear button, which clears the time criteria fields. Export The yellow button is the export button, which allows you to export the abnormal table job statistics information as an XLSX file. "},"whalealData/UserManual/StatisticalReports/SystemAccessStatistics.html":{"url":"whalealData/UserManual/StatisticalReports/SystemAccessStatistics.html","title":"SystemAccessStatistics","keywords":"","body":"System Access Statistics Clicking on \"System Access Statistics\" under the \"Statistics Report\" menu allows you to query all the businesses that have accessed this platform. This page includes a search button, search criteria fields, a clear criteria button, and an export button. Each entry represents a business that has accessed the platform, and it includes information such as business name, data source type, target source type, and access time. Search The green button in the image above is the search button. On the left side, there are two criteria fields. After filling in the filtering criteria, click \"Search\" to filter out access information that matches the criteria. Clear The black button is the clear button, which clears the criteria fields. Export The yellow button is the export button, allowing you to export the system access statistics information as an xlsx file. Details Clicking on the \"Details\" link for each access record provides more detailed information about that access, including the business name, access type, source and target types, and access time. "},"whalealData/UserManual/StatisticalReports/RollbackRecordsStatistics.html":{"url":"whalealData/UserManual/StatisticalReports/RollbackRecordsStatistics.html","title":"RollbackRecordsStatistics","keywords":"","body":"Rollback Records Statistics Clicking on \"Rollback Records Statistics\" under the \"Statistics Report\" menu allows you to query all the rolled-back tasks. This page includes a search button, search criteria fields, and an export button. Each rolled-back task entry includes information such as task name, job name, archive type, execution start and end time, execution status, error message, progress percentage, archived item count, archive path, executed SQL, and rollback status. Search The green button in the image above is the search button. On the left side, there are three criteria fields. After filling in the filtering criteria, click \"Search\" to filter out rollback records that match the criteria. Export The yellow button is the export button, which allows you to export the rollback records statistics information as an xlsx file. "},"whalealData/UserManual/StatisticalReports/JobDetails.html":{"url":"whalealData/UserManual/StatisticalReports/JobDetails.html","title":"JobDetails","keywords":"","body":"Job Details Clicking on \"Job Details\" under the \"Statistics Report\" menu displays the detailed information about jobs within the platform. This page includes a search button, search criteria fields, and an export button. Search The green button in the image above is the search button. On the left side, there are three criteria fields. After filling in the filtering criteria, click \"Search\" to filter out job details that match the criteria. Export The yellow button is the export button, which allows you to export the job details statistics information as an xlsx file. Link For each job detail, there is a \"Link\" button. Clicking the link button will navigate you to the task monitoring page for that specific job. "},"whalealData/UserManual/StatisticalReports/DataHistoricalFlow.html":{"url":"whalealData/UserManual/StatisticalReports/DataHistoricalFlow.html","title":"DataHistoricalFlow","keywords":"","body":"Data Historical Flow Clicking on the \"Data Historical Flow\" under the \"Statistics Report\" menu displays the data source flow within the platform. This page includes a search button, search criteria fields, and a clear criteria button. Search The green button in the image above is the search button. On the left side, there are three criteria fields. After filling in the filtering criteria, click \"Search\" to filter out data flows that match the criteria. "},"whalealData/use cases/UserRegistration.html":{"url":"whalealData/use cases/UserRegistration.html","title":"UserRegistration","keywords":"","body":"User Registration After accessing the homepage, there is an \"Register Now\" button located at the bottom right corner of the login section. Clicking this button will take you to the registration page as shown in the second image. Fill in the required registration information and click \"Register\" to complete the process. "},"whalealData/use cases/UserLogin.html":{"url":"whalealData/use cases/UserLogin.html","title":"UserLogin","keywords":"","body":"User Login To log in, enter the registered account credentials. For the first login of an administrator, the initial password is \"123456.\" After logging in, you will be prompted to change the password. Other registered users have user-level permissions. Administrator users can assign permissions to new users. "},"whalealData/use cases/AddDataSource.html":{"url":"whalealData/use cases/AddDataSource.html","title":"AddDataSource","keywords":"","body":"Adding a Data Source To add a new data source to the platform, follow these steps: Click on the \"Configuration Management\" menu and select \"Data Source Management\". Click the blue \"Add\" button to open the form. Fill in the basic information for the new data source and click \"Test\". If you see a \"Test Passed\" message above, click \"Save\" to successfully add the data source. If the test fails, double-check the provided information for accuracy. After adding the data source, go to the \"System Management\" menu and select \"User Management\". Click on the user you want to bind the data source to. In the user details, select the added data source from the list and click \"Save\". This user will now have access to the newly added data source. By following these steps, you can easily add and configure new data sources, allowing users to access and utilize these sources for various operations within the Whaleal Data platform. "},"whalealData/use cases/AddDestinationSource.html":{"url":"whalealData/use cases/AddDestinationSource.html","title":"AddDestinationSource","keywords":"","body":"Adding a Target Source To add a new target source to the platform, follow these steps: Click on the \"Configuration Management\" menu and select \"Target Source Management\". Click the blue \"Add\" button to open the form. Fill in the basic information for the new target source and click \"Test\". If you see a \"Test Passed\" message above, click \"Save\" to successfully add the target source. If the test fails, double-check the provided information for accuracy. After adding the target source, go to the \"System Management\" menu and select \"User Management\". Click on the user you want to bind the target source to. In the user details, select the added target source from the list and click \"Save\". This user will now have access to the newly added target source. By following these steps, you can add and configure new target sources, allowing users to utilize these sources as destinations for data operations within the Whaleal Data platform. "},"whalealData/use cases/AddWarmDataFullLoadJob.html":{"url":"whalealData/use cases/AddWarmDataFullLoadJob.html","title":"AddWarmDataFullLoadJob","keywords":"","body":"Adding a Warm Data Full Load Job To add a new warm data full load job to the platform, follow these steps: Click on the \"Configuration Management\" menu and select \"Table Job Configuration\". In the Warm Data Table Job page, click the blue \"Add\" button to open the form. Choose the data source database table and the target destination database table that you want to synchronize. Select \"Full Load\" as the archive mode. The warm data table job also includes data consistency verification. If you choose to enable it, you can set the required verification percentage. After the synchronization, the platform will perform data consistency checks on the synchronized data. For MySQL-related jobs, you can choose the isolation level for synchronization. For MongoDB to MongoDB synchronization, you can choose whether to sync Gridfs. If you choose \"No,\" the platform will only synchronize regular documents. Choose a data processing method, either manual deletion or automatic deletion, after synchronization. The data source table will be deleted according to your choice after synchronization is completed. By following these steps, you can create a warm data full load job that synchronizes data from a data source database table to a target destination database table using full load mode. This allows for comprehensive synchronization of data within the Whaleal Data platform. "},"whalealData/use cases/AddColdDataFullLoadJob.html":{"url":"whalealData/use cases/AddColdDataFullLoadJob.html","title":"AddColdDataFullLoadJob","keywords":"","body":"Adding a Cold Data Full Job To set up a full job for cold data archiving, follow these steps: Click on the \"Configuration Management\" menu and select \"Table Job Configuration\". In the \"Cold Data Table Job\" page, click on the blue \"Add\" button to open the form. Fill out the form by selecting the data source table and file source for synchronization. Choose the archiving mode as \"Full Update\". Note that cold data archiving is applicable only to MongoDB data. The table job comes with consistency validation. Select \"Yes\" and specify the required validation percentage. After synchronization, the platform will perform consistency validation on the synchronized data. Choose the data handling method, either manual deletion or system deletion. This feature allows you to delete the source table after synchronization is completed. Following these steps will enable you to configure a full job for cold data archiving, ensuring efficient and accurate data synchronization and archiving. "},"whalealData/use cases/AddS3FullLoadJob.html":{"url":"whalealData/use cases/AddS3FullLoadJob.html","title":"AddS3FullLoadJob","keywords":"","body":"Adding an S3 Full Load Job To add a new S3 full load job to the platform, follow these steps: Click on the \"Configuration Management\" menu and select \"Table Job Configuration\". In the S3 Table Job page, click the blue \"Add\" button to open the form. Choose the data source database table and the target S3 bucket you want to synchronize. Ensure that the source endpoint is MongoDB's Gridfs data as the source for S3 synchronization. Select \"Full Load\" as the archive mode. The table job also includes data consistency verification. If you choose to enable it, you can set the required verification percentage. After the synchronization, the platform will perform data consistency checks on the synchronized data. Since S3 has the characteristic that files with the same name will overwrite the existing files, you can choose from synchronization modes like \"Replace without Handling\", \"Replace with Newest Files\", or \"ID + Filename\" mode. Choose a data processing method, either manual deletion or automatic deletion, after synchronization. The data source table will be deleted according to your choice after synchronization is completed. By following these steps, you can create an S3 full load job that synchronizes data from a MongoDB Gridfs data source to a target S3 bucket. This allows for efficient management and synchronization of data between different storage systems within the Whaleal Data platform. "},"whalealData/use cases/AddWarmDataIncrementalJob.html":{"url":"whalealData/use cases/AddWarmDataIncrementalJob.html","title":"AddWarmDataIncrementalJob","keywords":"","body":"Adding a Warm Data Incremental Load Job To add a new warm data incremental load job to the platform, follow these steps: Click on the \"Configuration Management\" menu and select \"Table Job Configuration\". In the Warm Data Table Job page, click the blue \"Add\" button to open the form. Choose the data source database table and the target destination database table that you want to synchronize. Select \"Incremental Load\" as the archive mode. If you choose the incremental load mode, you need to fill in the SQL configuration. Click the blue \"Custom SQL\" button to open the form where you can select the completion conditions for the SQL. Click \"Save\" to generate the SQL. The warm data table job also includes data consistency verification. If you choose to enable it, you can set the required verification percentage. After the synchronization, the platform will perform data consistency checks on the synchronized data. For MySQL-related jobs, you can choose the isolation level for synchronization. For MongoDB to MongoDB synchronization, you can choose whether to sync Gridfs. If you choose \"No,\" the platform will only synchronize regular documents. Choose a data processing method, either manual deletion or automatic deletion, after synchronization. The data source table will be deleted according to your choice after synchronization is completed. By following these steps, you can create a warm data incremental load job that synchronizes data from a data source database table to a target destination database table using incremental load mode. This allows for continuous synchronization of data within the Whaleal Data platform. "},"whalealData/use cases/AddColdDataIncrementalJob.html":{"url":"whalealData/use cases/AddColdDataIncrementalJob.html","title":"AddColdDataIncrementalJob","keywords":"","body":"Adding a Cold Data Incremental Job To set up an incremental job for cold data archiving, follow these steps: Click on the \"Configuration Management\" menu and select \"Table Job Configuration\". In the \"Cold Data Table Job\" page, click on the blue \"Add\" button to open the form. Fill out the form by selecting the data source table and file source for synchronization. Choose the archiving mode as \"Incremental Update\". Note that cold data archiving is applicable only to MongoDB data. For the incremental update mode, you need to fill in the SQL configuration. Click the blue \"Custom SQL\" button to open the table shown in the second image. Select the completion conditions and click \"Save\" to generate the SQL. The table job comes with consistency validation. Select \"Yes\" and specify the required validation percentage. After synchronization, the platform will perform consistency validation on the synchronized data. Choose the data handling method, either manual deletion or system deletion. This feature deletes the source table based on the configured batch. By following these steps, you can configure an incremental job for cold data archiving, enabling efficient and accurate synchronization of MongoDB data for archiving purposes. "},"whalealData/use cases/AddS3IncrementalJob.html":{"url":"whalealData/use cases/AddS3IncrementalJob.html","title":"AddS3IncrementalJob","keywords":"","body":"Adding an S3 Incremental Load Job To add a new S3 incremental load job to the platform, follow these steps: Click on the \"Configuration Management\" menu and select \"Table Job Configuration\". In the S3 Table Job page, click the blue \"Add\" button to open the form. Choose the data source database table and the target S3 bucket you want to synchronize. Ensure that the source endpoint is MongoDB's Gridfs data as the source for S3 synchronization. Select \"Incremental Load\" as the archive mode. You'll need to provide SQL configuration since you're choosing incremental mode. Click the blue \"Custom SQL\" button to open the form for defining the SQL conditions. Set up the conditions for the incremental synchronization SQL in the form. After defining the conditions, click \"Save\" to generate the SQL. The table job also includes data consistency verification. If you choose to enable it, you can set the required verification percentage. After the synchronization, the platform will perform data consistency checks on the synchronized data. Since S3 has the characteristic that files with the same name will overwrite the existing files, you can choose from synchronization modes like \"Replace without Handling\", \"Replace with Newest Files\", or \"ID + Filename\" mode. Choose a data processing method, either manual deletion or automatic deletion, after synchronization. The data source table will be deleted according to your choice after synchronization is completed. By following these steps, you can create an S3 incremental load job that synchronizes data from a MongoDB Gridfs data source to a target S3 bucket using incremental synchronization based on defined SQL conditions. This allows for efficient and selective data synchronization within the Whaleal Data platform. "},"whalealData/use cases/CreateSingleTask.html":{"url":"whalealData/use cases/CreateSingleTask.html","title":"CreateSingleTask","keywords":"","body":"Creating One-Time Tasks To create a one-time task, follow these steps: Navigate to the \"Task Configuration\" menu and select \"Task Configuration.\" Click the blue \"New\" button to open the task creation form. In the task creation form (second image), select the task mode as \"One-Time.\" Choose a specific execution time using a Cron expression. Cron expressions allow you to define the exact date and time when the task should be executed. Configure other settings as needed, such as execution mode, task timeout, and retry attempts. Optionally, set up a notification strategy by adding email addresses for alerts. Notifications will be sent based on the chosen strategy after the task completes. Click the \"Add Job\" button to attach a job (table job) to the task. In the job configuration form (third image), select the desired job(s) to be associated with this task. Click \"OK\" or \"Confirm\" to save the task configuration. Please note that one-time tasks need to be reviewed and approved by an administrator before they can be executed. Once the task is approved, it will be scheduled for execution based on the specified time using the Cron expression. One-time tasks are suitable for tasks that need to be executed at a specific point in time, such as data synchronization or archiving activities that are scheduled to happen once. "},"whalealData/use cases/CreateManualTask.html":{"url":"whalealData/use cases/CreateManualTask.html","title":"CreateManualTask","keywords":"","body":"Creating Manual Tasks Creating a manual task follows the same steps as creating a one-time task. The key difference with manual tasks is that they don't require setting an execution time, as they are intended to be triggered manually when needed. Here's how you can create and manage manual tasks: Begin by creating a task configuration, just as you did for the one-time task. In the task configuration, set the execution mode to \"Manual.\" Complete the rest of the task configuration details and save the configuration. Once the manual task is configured, it will be available for execution by authorized users. An administrator needs to review and approve the manual task, just like with other tasks, before it can be executed. To execute a manual task: Navigate to the \"Task Management\" menu and select \"Task Scheduling.\" Locate the manual task you want to execute. Click the \"Execute Now\" button next to the task. This will trigger the immediate execution of the task. Manual tasks are particularly useful when you want to perform specific data archiving or synchronization tasks on-demand, giving you full control over when these tasks are executed. "},"whalealData/use cases/CreateLoopTask.html":{"url":"whalealData/use cases/CreateLoopTask.html","title":"CreateLoopTask","keywords":"","body":"Creating Recurring Tasks Creating a recurring task is similar to creating a one-time task. The key difference is that you need to set the execution interval for the recurring task, which determines when the task will be executed again in the future. Here's how you can create and manage recurring tasks: Follow the same steps for creating a task configuration, just like you did for the one-time task. In the task configuration, set the execution mode to \"Recurring.\" Specify the execution interval, which determines how often the task will be repeated. Fill in the rest of the task configuration details and save the configuration. After configuring the recurring task, an administrator needs to review and approve it, just like with one-time tasks. Once the recurring task is approved, you can find it in the \"Task Scheduling\" section. To execute a recurring task: Navigate to the \"Task Management\" menu and select \"Task Scheduling.\" Locate the recurring task you want to execute. Click the \"Execute Now\" button next to the task. This will trigger the immediate execution of the task according to the predefined execution interval. By creating recurring tasks, you can automate data archiving and synchronization at regular intervals, ensuring that your data remains up-to-date and consistent over time. "},"whalealData/use cases/TaskExecutionMonitoring.html":{"url":"whalealData/use cases/TaskExecutionMonitoring.html","title":"TaskExecutionMonitoring","keywords":"","body":"Task Execution Monitoring After clicking \"Execute Now\" on the task scheduling page or when the scheduled execution time is reached, different types of tasks will appear in different sections of the task monitoring page under the \"Task Management\" menu. For example, warm data tasks will appear in the \"Task Monitoring (Warm)\" section, cold data tasks will appear in the \"Task Monitoring (Cold)\" section, and so on. Each section provides information about the task's status, including \"Completed,\" \"In Progress,\" and \"Exception.\" You can monitor the execution status of your tasks in the respective monitoring sections for warm, cold, and S3 data tasks. "},"whalealData/use cases/RetryAbnormalTask.html":{"url":"whalealData/use cases/RetryAbnormalTask.html","title":"RetryAbnormalTask","keywords":"","body":"Re-Execution of Failed Tasks Under the \"Task Management\" menu, there are three sections for task monitoring: \"Completed,\" \"In Progress,\" and \"Exception.\" For tasks that have encountered exceptions or failures, you have the option to manually trigger a re-execution of these tasks. Here's how you can re-execute a failed task: Go to the \"Task Monitoring\" section. Click on the \"Exception\" tab to view tasks that encountered exceptions. Find the specific task you want to re-execute and click on it to view its details. Within the task details view, you'll find an option to \"Re-Execute.\" Clicking this option will initiate the re-execution process. The task will first be rolled back to its previous state, and then it will be re-executed. This functionality provides a way to address and resolve exceptions that may have occurred during task execution, allowing you to retry the task and ensure its successful completion. "},"whalealData/use cases/AbnormalTaskFeedback.html":{"url":"whalealData/use cases/AbnormalTaskFeedback.html","title":"AbnormalTaskFeedback","keywords":"","body":"Handling Exceptional Tasks When encountering exceptions during cold data archiving, you can follow these steps to address the issue: Navigate to the \"Archive Management (Cold Data)\" menu and select \"Log Query (Cold)\". This page will display the archived cold data tasks. Click on \"File Rewriting\" to perform a rollback of the archived files into the database. This process allows you to manage and recover from any anomalies that might occur during the cold data archiving process. "},"whalealData/use cases/SystemDeleteSourceData.html":{"url":"whalealData/use cases/SystemDeleteSourceData.html","title":"SystemDeleteSourceData","keywords":"","body":"System Deletion of Source Data When configuring table jobs in the \"Table Job Configuration\" section under the \"Configuration Management\" menu, you have the option to choose the data processing method. If you select \"System Deletion,\" you will need to specify the batch value in the provided field. After the data archiving is completed, the system will automatically delete the source data based on the specified batch. Please exercise caution when using system deletion, as it will permanently remove source data based on the batch value you provide. Make sure to verify your configuration settings before proceeding. "},"whalealData/use cases/ManuallyDeleteSourceData.html":{"url":"whalealData/use cases/ManuallyDeleteSourceData.html","title":"ManuallyDeleteSourceData","keywords":"","body":"Manual Deletion of Source Data When configuring table jobs in the \"Table Job Configuration\" section under the \"Configuration Management\" menu, there's an option for \"Data Processing Method.\" By selecting \"Manual Deletion,\" along with providing a batch value, you can control how the source data is managed after archiving. If you choose \"Manual Deletion\" and set a batch value, the source data won't be automatically deleted after archiving is completed. Instead, it will be marked as archived with the specified batch value. To manually delete the source data after archiving: Go to the \"Task Monitoring (Cold)\" section. Find the task for which you want to delete the source data. Click on the task to view its details. Click \"Modify Source Table Data Status\" to change the status of the source table data. Alternatively, click \"Manual Deletion of Source Table Data\" to manually delete the source table data. This provides you with control over when and how the source data is deleted after archiving, allowing you to manage your data archiving process according to your specific needs. "},"whalealData/use cases/ColdDataWriteBack.html":{"url":"whalealData/use cases/ColdDataWriteBack.html","title":"ColdDataWriteBack","keywords":"","body":"Cold Data Rollback If there are issues with cold data archiving, you can perform a rollback operation. Here's how: Go to the \"Archive Management\" menu and select \"Log Query (Cold)\" under \"Cold Data\". This page displays the archived cold data tasks. Click on the \"File Rollback\" button to perform a rollback operation for a specific task. This action will roll back the archived file into the database. After clicking the task, you will see a yellow \"File Rollback\" button next to it. Click this button to initiate the rollback process. Creating a Rollback Task Clicking the \"File Rollback\" button will open a form. Fill in the required information and click \"Confirm\" to create the rollback task. By following these steps, you can initiate a rollback operation for archived cold data files that need to be brought back into the database. This ensures that the data remains accessible and consistent within your system. "},"whalealData/use cases/ColdDataFullTextSearch.html":{"url":"whalealData/use cases/ColdDataFullTextSearch.html","title":"ColdDataFullTextSearch","keywords":"","body":"Cold Data Full-Text Search To perform a full-text search on archived cold data, follow these steps: Click on the \"Archive Management\" menu and select \"File Full-Text Search\". This will display all file full-text search log information. Click the \"Create Search Task\" button to create a file full-text search task. The system will execute a global search on the files. For the files that match the search criteria, you can perform a file rollback operation. Searching The green button on the top is the search button. The left-side filter options can be filled in as needed. After filling in the conditions, click the search button to filter the desired search content. Creating a Search Task Click the blue button to create a search task. This will open a form where you can enter the necessary details. After filling in the required information, click \"Confirm\". By following these steps, you can search for archived cold data using the full-text search feature in the Whaleal Data platform. You can filter and retrieve specific files based on your search criteria and even perform file rollback operations as needed. "},"whalealData/use cases/SMTPConfig.html":{"url":"whalealData/use cases/SMTPConfig.html","title":"SMTPConfig","keywords":"","body":"SMTP Email Configuration To configure SMTP email settings, follow these steps: Click on the \"System Management\" menu and select \"System Settings.\" This page is only accessible to the administrator user. The page displays SMTP configuration options. Fill in the required information for the SMTP configuration, including SMTP server, port, username, password, sender email, and recipient email. After filling in the basic configuration, click the \"Test Connection\" button. If the test is successful and you receive a test email, the \"Save\" button will appear. Click on \"Save\" to save the SMTP configuration. Once the SMTP configuration is saved, all task configuration alerts will be sent using the email settings provided in this configuration. Please note that proper SMTP configuration is essential for receiving alerts and notifications from the system. Make sure to verify the accuracy of the configuration before saving it. "},"whalealData/use cases/WarmJobDemo.html":{"url":"whalealData/use cases/WarmJobDemo.html","title":"WarmJobDemo","keywords":"","body":"Warm Job Demo 1. Add Data Source and Target Source Add Data Source Click on the \"Data Source Management\" page under the \"Configuration Management\" menu. On this page, click the blue \"Add\" button and fill in the relevant information for the data source. After passing the test, click \"Confirm\" to save this data source. Add Target Source Click on the \"Target Source Management\" page under the \"Configuration Management\" menu. On this page, click the blue \"Add\" button and fill in the relevant information for the target source. After passing the test, click \"Confirm\" to save this target source. 2. Permission Assignment After adding the data source and target source, the administrator user can assign database permissions to users on the \"User Management\" page under the \"System Management\" menu. 3. Create Warm Job Click on the \"Table Job Configuration\" under the \"Configuration Management\" menu. On the warm data table job page, click the blue \"Add\" button to open the following form. Choose the data source and target source you want to synchronize. Table jobs have consistency verification functionality. After selecting \"Yes,\" you can enter the desired verification percentage. After synchronization, the platform will perform consistency checks on the synchronized data. For MySQL-related jobs, you can choose the isolation level. When synchronizing from MongoDB to MongoDB, you can choose whether to synchronize Gridfs. If you choose \"No,\" the platform will only synchronize ordinary documents. The data processing mode can be selected as manual deletion or system deletion. This feature will delete the source table by batch after synchronization is completed. 4. Create Task Add Task Configuration Click on the \"Task Configuration\" under the \"Task Configuration\" menu. This page displays all tasks. Click the blue \"Add\" button to open the form shown in the second image. Fill in the task configuration according to your needs. If there are many table jobs in this task, you can set the execution mode, task timeout, and failure retry times according to your needs. After selecting a reminder strategy, you can add email addresses for receiving alert notifications. After the task is completed, alerts will be sent via email according to the selected strategy. Add Table Jobs to the Task Click on \"Add Job\" to open the data configuration shown in the third image. Check the desired job and click \"OK\" to bind this job to the newly created task. A task can be bound to multiple table jobs. Administrator Approval After configuring the task, the administrator user can review the task configuration. After approval, the task can be executed. 5. Task Scheduling Click on \"Task Scheduling\" under the \"Task Management\" menu to view the status of the task. Click \"Execute Now\" to immediately run the task. 6. Task Monitoring After clicking \"Execute Now\" on the task scheduling page or when the task execution time arrives, you can view the execution status of the task in the task monitoring (Warm) section under the \"Task Management\" menu. "},"whalealData/use cases/ColdWorkDemo.html":{"url":"whalealData/use cases/ColdWorkDemo.html","title":"ColdWorkDemo","keywords":"","body":"Cold Job Demo Here's a step-by-step demonstration of how to create and manage a cold data archiving job in the platform: 1. Add Data Source and File Source Add Data Source Go to the \"Configuration Management\" menu and select \"Data Source Management\". Click the blue \"Add\" button and fill in the required information for the data source. Test the connection and save it if the test is successful. Add File Source Go to the \"Configuration Management\" menu and select \"Target Source Management\". Click on \"Cold Data File Source\" and then click the blue \"Add\" button. Fill in the necessary information related to cold data archiving and save the file source. 2. Assign Permissions After adding the data source and file source, administrators need to assign database permissions in the \"User Management\" page under \"System Management\". 3. Create a Cold Job Go to the \"Configuration Management\" menu and select \"Table Job Configuration\". Click the blue \"Add\" button in the \"Cold Data Table Job\" section. Fill in the required information, including selecting the data source and file source. Optionally, configure data consistency validation and choose the data handling method after synchronization. Save the job configuration. 4. Create a Task Click on the \"Task Configuration\" menu and then \"Cold Data Task Configuration\". Click the blue \"Add\" button to create a new task configuration. Fill in the necessary details according to your requirements. Optionally, configure execution mode, task timeout, retry attempts, and notification settings. Choose the notification strategy and add email addresses for alerts. Save the task configuration. Add Table Jobs to the Task Click \"Add Job\" to associate table jobs with the created task. Select the desired table jobs and click \"Confirm\" to add them to the task. Admin Approval After configuring the task, an administrator can review and approve it if needed. The task is ready for execution after approval. 5. Task Scheduling Navigate to the \"Task Management\" menu and select \"Task Scheduling\". Monitor the status of the task in this section. Click \"Execute Now\" to immediately execute the task. 6. Task Monitoring In the task scheduling section, you can monitor the execution status of the task. Visit the \"Task Monitoring (Cold)\" page to view the detailed execution status of the task. By following these steps, you can successfully create, configure, and manage cold data archiving jobs in the platform. This allows you to archive data from various sources and monitor the execution of tasks for data consistency and reliability. "},"whalealData/use cases/S3JobDemo.html":{"url":"whalealData/use cases/S3JobDemo.html","title":"S3JobDemo","keywords":"","body":"S3 Job Demo 1. Adding Data Source and Target S3 Adding Data Source Navigate to the \"Configuration Management\" menu and select \"Data Source Management.\" Click the blue \"Add\" button and provide the necessary information for the data source. After testing and confirmation, save the data source. Adding Target S3 In the \"Configuration Management\" menu, select \"Target Source Management.\" Click on the S3 section, then click the blue \"Add\" button to provide the relevant information for the target S3. After successful testing, confirm and save the target S3. 2. Permission Assignment After adding the data source and target S3, the administrator can assign database permissions to users through the \"User Management\" page under the \"System Management\" menu. 3. Creating an S3 Job Navigate to the \"Configuration Management\" menu and select \"Table Job Configuration.\" On the S3 job configuration page, click the blue \"Add\" button to open the form. Choose the data source and target S3 you want to sync. The S3 job includes consistency verification, where you can set the required verification percentage. After synchronization, the platform will perform consistency checks on the synced data. You can also select the data handling method as either manual deletion or system deletion. After completion, the source table will be deleted according to batch numbers. 4. Creating a Task Adding a Task Configuration Navigate to the \"Task Configuration\" menu and click the blue \"Add\" button to open the form. Fill in the required details based on your needs. If there are multiple jobs within the task, you can customize the execution mode, task timeout, and retry count. Select a notification strategy and add email addresses for notifications. After task completion, notifications will be sent based on the selected strategy. Adding Table Jobs to the Task Click the \"Add Job\" button to select and add table jobs to the task. Once added, click \"Confirm\" to bind the jobs to the task. Multiple table jobs can be added to a single task. Administrator Approval After configuring the task, administrators can review and approve the task. Once approved, the task can be executed. 5. Task Scheduling Navigate to the \"Task Scheduling\" menu and check the status of the tasks. Click \"Execute Now\" to initiate immediate execution of a task. 6. Task Monitoring After executing or reaching the scheduled execution time, go to the \"Task Monitoring\" (S3) section under \"Task Management\" to view the status of the task. "},"documentDataTransfer/":{"url":"documentDataTransfer/","title":"Document Data Transfer","keywords":"","body":"Introduction to DDT (Document Data Transfer) Part 1: DDT Overview DDT is a next-generation MongoDB database migration and synchronization tool developed by Shanghai Jinmu Information Technology Co., Ltd. (referred to as \"Jinmu Information\"). It is designed to meet various customer needs and leverages Jinmu Information's years of experience in MongoDB services and research and development. DDT is a versatile data transfer software developed in JAVA that offers high robustness, high transferability, and high availability. It allows for fast and stable data migration, helping users with tasks such as data backup, real-time migration, disaster recovery, and more. Users can also customize configuration parameters to achieve efficient data transfer for different scenarios. Given the limitations of the built-in primary-secondary synchronization in MongoDB replica sets for certain business scenarios, Jinmu Information developed the DDT synchronization tool. DDT can be used for instance-level, data center-level, and cross-data center replication, catering to disaster recovery and multi-active requirements. Traditional MongoDB data synchronization is limited to data transfers between similar architectures. However, DDT supports data transfers between three types of architectures: standalone nodes, replica sets, and sharded clusters. This flexibility enables data synchronization between different types of architectures, such as from a standalone node to a sharded cluster or from a sharded cluster to a standalone node. The core of DDT's real-time synchronization lies in its efficient parsing and application of the OPLOG log, allowing for high-performance and secure real-time synchronization. The source MongoDB can be a standalone instance, a replica set, or a sharded cluster, while the target can be a mongod or mongos instance. For replica sets, it's recommended to source data from secondary/hidden nodes to reduce the load on the primary node. For sharded clusters, each shard should connect to DDT. Part 2: Features DDT is characterized by its simplicity, security, versatility, multiple functionalities, and high performance. 2.1 High Performance Efficient Data Validation Ensures consistent data volume. Ensures consistent data information. Ensures consistent data indexes. Ensures consistent data structure. Multiple Synchronization Scenarios Full data replication. Real-time data synchronization. Incremental data synchronization. Customizable synchronization scope. Composite data synchronization scenarios. High-Speed Synchronization Mechanism Utilizes 100% of available bandwidth. Controlled CPU utilization. Configurable memory usage. Supports parallel synchronization of multiple tables. Compact, Stable, and Efficient Compact in size. Supports seamless resume in case of interruption. Supports synchronization across multiple MongoDB versions. 2.2 Synchronization Modes Synchronization Modes: Full, Real-time, Full and Incremental, Full and Real-time. Incremental synchronization refers to real-time synchronization with a specified time range for the Oplog. Full Synchronization: Splits source MongoDB collections for querying, and multithreadedly writes the queried data into the target MongoDB collections. In this mode, higher resource availability generally leads to higher QPS. Real-time Synchronization: Replicates data from the source MongoDB to another MongoDB to create redundant copies. It captures the oplog from the source MongoDB and replays it in the target MongoDB. 2.3 Resumable Transfer In case of an unexpected source MongoDB shutdown, DDT can still synchronize data seamlessly upon restart. When DDT is unexpectedly closed, it can automatically resume from the last checkpoint and continue with the data transfer. 2.4 Multi-Version Support DDT currently supports MongoDB versions 3.2 to 6.0. It reliably supports the synchronization of tables and bucket collections in newer versions. 2.5 DDL Operations During real-time synchronization, users can customize the synchronization of certain DDL operations. Additionally, these DDL operations are recorded in logs for auditing purposes. 2.6 Oplog Delay Oplog delay synchronization allows for easy failover in case of issues. 2.7 Synchronization Scope In real-time synchronization, users can set the start and end times for synchronizing the Oplog within a specified time range. There are additional features such as filtering the list of synchronized tables, data validation, and more. Part 3: Company Overview Shanghai Jinmu Information Technology Co., Ltd. is a professional IT data consulting and service provider. The company is committed to delivering high-quality information products, consulting, and services to users. Established in 2015 in Shanghai, Jinmu Information has branches in Beijing, Shenzhen, and Guangzhou. Jinmu Information is a core partner for MongoDB in the Greater China region and a core partner for Akamai and Vonage in China. The company provides professional technical services, consulting, and application development to clients. As a technology-driven IT service provider prioritizing innovation and customer needs, Jinmu Information's products and services have gained recognition from leading domestic enterprises. The company has over 50 core clients and offers premium services and innovative product solutions in industries such as finance, insurance, securities, gaming, and e-commerce, covering mainland China and Hong Kong. Jinmu Information Website: www.jinmuinfo.com Consultation Email: support@jinmuinfo.com Contact Numbers: 021-58870038, 021-66696778 "},"documentDataTransfer/Introduction/Architecture.html":{"url":"documentDataTransfer/Introduction/Architecture.html","title":"Architecture","keywords":"","body":"Introduction to DDT Architecture Background The need for full migration and real-time synchronization of MongoDB databases led to the development of the new data migration project at our company, resulting in the DDT project. Project Overview Name: DDT (Document Data Transfer) Language: Developed purely in Java Purpose: Full migration and real-time synchronization of MongoDB databases Functionality Overview DDT is a MongoDB data synchronization component. Migration features include: Synchronization Modes Full Synchronization: Syncs all data in tables. Only data from tables existing before the program startup is synchronized. Real-time Synchronization: Real-time sync of oplogs (operation logs) generated by the source. Full + Incremental Synchronization: After full synchronization, only operations on source tables during the sync period are synchronized. The start time of real-time sync is the start time of full synchronization, and the end time of real-time sync is the end time of full synchronization. Full + Real-time Synchronization: After full synchronization, real-time sync begins. The start time of real-time sync is the start time of full synchronization. Additional Features Delayed Synchronization: During real-time sync, the reading of oplogs can be delayed. Syncing DDL Operations: During real-time sync, users can customize the synchronization of certain DDL operations. Source Table Indexes: During full synchronization, users can specify whether to sync the source table indexes list after 60% of the total data is synced. Multi-Table Parallelism: During full synchronization, choose between synchronizing multiple source tables simultaneously or synchronizing them one by one. Sync Source Table List: Use regular expressions to specify the desired list of tables to sync. Time-Interval Real-time Sync: During real-time sync, you can set to sync oplogs within a certain time interval. MongoDB Versions Supported Versions: DDT supports MongoDB versions 3.2, 3.4, 3.6, 4.0, 4.4, 5.0, and 6.0. Architecture Explanation: A JVM container corresponds to multiple instances, with each instance corresponding to a migration program. Each instance comprises three parts: a. Source (extracts data from the source database, supports full/real-time implementation) b. Cache (caches data from the source according to target requirements) c. Target (updates data to the target database, supports full/real-time/comparison implementation) DDT Process Diagram Real-Time Migration Note: Use CAS mechanism to ensure that only one thread accesses each table's buckets at a time. Use CAS mechanism to ensure that only one thread writes data to a bucket of a table at a time. When splitting buckets for a table, if a DDL operation is encountered, the data before that DDL is written, followed by executing the DDL. Full Migration Note: Use CAS mechanism to ensure that only one thread writes/reads bucket queues in the same partition at the same time. Data from a source table can be placed in any partition. A target table can retrieve data from any partition. However, only one thread can operate on a partition at a time. "},"documentDataTransfer/Introduction/CustomerCase.html":{"url":"documentDataTransfer/Introduction/CustomerCase.html","title":"CustomerCase","keywords":"","body":"DDT Application Scenarios Let's introduce some use cases of users employing DDT, including business scenarios, durations, and performance comparisons. Case 1: Securities Company Benefits of Disaster Recovery: In addition to local backups in the production center, business operations can also be backed up in the disaster recovery center. In a dual-active architecture, support for dual-center mutual backup enhances business resilience, providing a double insurance for the business. By utilizing the DDT synchronization tool, remote data is written to the target center in real time. Case 2: Airline Company There is a need for a cross-major-version upgrade of a MongoDB replica set cluster, upgrading from version 3.2 to 4.4. Due to the need for rapid upgrade changes on the application side, the traditional MongoDB replica set would require step-by-step version upgrades, which is time-consuming. Also, in case of anomalies, the transition back to the correct state might not be timely. Our solution for the airline company is to set up a new 4.4 version database, using DDT to migrate old data to the new cluster in real time. When both new and old clusters have no delay, the application-side database address is switched. In this case, the original data size is 700GB, with a real-time data rate of 10,000 records per second, including intermittent DDL operations like table creation and deletion. DDT took a total of 6 hours to complete the transfer, with 5 hours for the full migration and 1 hour for real-time migration. "},"documentDataTransfer/Install/Requirements.html":{"url":"documentDataTransfer/Install/Requirements.html","title":"Requirements","keywords":"","body":"DDT System Requirements Hardware Requirements Operating System: Linux distribution (such as Ubuntu, CentOS). Recommended Configuration: 8 cores, 16GB RAM. Storage Space: At least 100GB of available disk space. Network Adapter: Wired or wireless network adapter. Network Requirements Network Access Requirements Bandwidth: Gigabit or Fast Ethernet. Network Policy: Configure network policies to allow connectivity between the source and target MongoDB instances. Port Requirements Open the specified ports (e.g., port 27017 for MongoDB communication, ports used by the program at startup, or custom ports for source and target MongoDB instances). Software Requirements Operating System Requirements Supported on Linux CentOS 7 and above. "},"documentDataTransfer/Install/Installation.html":{"url":"documentDataTransfer/Install/Installation.html","title":"Installation","keywords":"","body":"Installation and Deployment Deploying DDT on CentOS JDK Installation Download JDK 11 tgz package: wget https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.9_linux-x64_bin.tar.gz Extract the downloaded tar package: tar -zxvf openjdk-11.0.9_linux-x64_bin.tar.gz Move the extracted directory: mv jdk-11.0.9 /usr/local/jdk11 Configure environment variables: vi /etc/profile export JAVA_HOME=/usr/local/jdk11 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin Refresh the environment: source /etc/profile Verify the installation: java --version Running DDT Prerequisites Ensure that the installation and configuration files are in place and the Java environment is correctly set up before starting the DDT process. Starting the Service Navigate to the bin directory. Run the startup script: ./start-DDT.sh. This starts the data transfer functionality. Run the startup script: ./start-monitor.sh. This starts the web monitoring functionality. Stopping the Service Navigate to the bin directory. Run the shutdown script: ./stop-DDT.sh. This stops the data transfer functionality. Run the shutdown script: ./stop-monitor.sh. This stops the web monitoring functionality. DDT Features DDT supports full, real-time, full + incremental, and full + real-time synchronization modes. Incremental mode refers to real-time synchronization with a time range restriction on the Oplog. DDT currently supports MongoDB versions 3.2 to 6.0. Newer version features such as time-series tables and bucket tables are fully supported for synchronization. During real-time synchronization, users can customize the synchronization of specific DDL operations. Additionally, DDL operations are logged for auditing purposes. Currently, synchronizing data with the same version has no impact. When synchronizing from a higher version to a lower version, new types introduced in the higher version cannot be synchronized to the lower version. Similarly, when synchronizing from a lower version to a higher version, certain types removed in the higher version cannot be synchronized. For example, deleting an index in version 3.2 or adding a time-series table in version 5.0. "},"documentDataTransfer/Install/QuickStart.html":{"url":"documentDataTransfer/Install/QuickStart.html","title":"QuickStart","keywords":"","body":"QuickStart Startup Steps 1. Download DDT Visit https://github.com/whaleal/DocumentDataTransfer/releases Download the latest version of DDT.tar.gz. 2. Extract mkdir DDT tar -zxvf DDT.tar.gz -C DDT 3. Modify Configuration Files Introduction to Configuration cd DDT/config vi DDT.properties 4. Prepare to Start cd bin ./start-all.sh 5. Check the Running Status Access the web monitoring page: http://bind_ip:58000/DDT_WEB/#/home 6. Check the Data Consistency of the Target Use the built-in validation tool of MongoDB (may lock the database): use xxx db.runCommand({ dbHash: 1 }) Manually validate the data: java -jar checkData.jar /path/to/configuration/DDT.properties Please replace /path/to/configuration/DDT.properties with the actual path to your DDT configuration file. "},"documentDataTransfer/Install/Configuring.html":{"url":"documentDataTransfer/Install/Configuring.html","title":"Configuring","keywords":"","body":"Function Operation Instructions 1. Parameter Meanings When configuring a MongoDB data synchronization task, here is the detailed meaning of each parameter: workName: Meaning: Task name Description: Used to identify the name of the data synchronization task. If not provided, it defaults to \"workNameDefault\". sourceDsUrl: Meaning: Source MongoDB connection URL Description: Specifies the connection URL of the source MongoDB database, which can be a single node, a replica set, or a sharded cluster. targetDsUrl: Meaning: Target MongoDB connection URL Description: Specifies the connection URL of the target MongoDB database, which can be a single node, a replica set, or a sharded cluster. syncMode: Meaning: Synchronization mode Description: Specifies the mode of data synchronization, which can be one of the following options: \"all\": Full mode, sync all tables, excluding operations on source tables during synchronization. \"allAndRealTime\": Full plus real-time mode, performs full sync first and then starts real-time sync. \"allAndIncrement\": Full plus incremental mode, performs full sync first and then syncs only operations on source tables during synchronization. \"realTime\": Real-time mode, syncs based on configured start and end times. realTimeType: Meaning: Real-time task type Description: Selects the type of real-time task, which can be \"oplog\" or \"changestream\". Additional Information: \"oplog\": Uses MongoDB's oplog for real-time synchronization, suitable for source replica sets, supports DDL operations, and is faster. \"changestream\": Uses MongoDB's changestream for real-time synchronization, suitable for source replica sets or mongos, does not support DDL operations, and has moderate speed. fullType: Meaning: Full task type Description: Selects the type of full task, which can be \"sync\" or \"reactive\". Additional Information: \"sync\": Uses a stable transmission method for full synchronization. \"reactive\": Uses a faster transmission method for full synchronization. dbTableWhite: Meaning: Tables to synchronize Description: Specifies tables to synchronize using regular expressions. For example, to sync all tables under the mongodb database: mongodb\\..+, the default is to sync all tables. ddlFilterSet: Meaning: DDL operations to synchronize Description: Specifies DDL operations to synchronize, separated by commas. The default is *, meaning sync all DDL operations. sourceThreadNum: Meaning: Source task thread number (full mode) Description: Specifies the number of threads to read source tasks in full synchronization. targetThreadNum: Meaning: Target task thread number (full mode) Description: Specifies the number of threads to write target tasks in full synchronization. ... (Continues with the rest of the parameter explanations) 2. Parameter Usage Scope | Parameter | Real-Time Task | Full Task | Full + Increment Task | Full + Real-Time Task | |--------------------|--------------|----------|----------------------|-----------------------| | workName | ✔️ | ✔️ | ✔️ | ✔️ | | sourceDsUrl | ✔️ | ✔️ | ✔️ | ✔️ | | targetDsUrl | ✔️ | ✔️ | ✔️ | ✔️ | | syncMode | ✔️ | ✔️ | ✔️ | ✔️ | | realTimeType | ✔️ | | ✔️ | ✔️ | | fullType | | ✔️ | ✔️ | ✔️ | | dbTableWhite | ✔️ | ✔️ | ✔️ | ✔️ | | ddlFilterSet | ✔️ | | ✔️ | ✔️ | | batchSize | ✔️ | ✔️ | ✔️ | ✔️ | | bucketNum | ✔️ | ✔️ | ✔️ | ✔️ | | bucketSize | ✔️ | ✔️ | ✔️ | ✔️ | | startOplogTime | ✔️ | | | | | endOplogTime | ✔️ | | ✔️ | ✔️ | | delayTime | ✔️ | | | | | nsBucketThreadNum | ✔️ | | | | | writeThreadNum | ✔️ | | | | | ddlWait | ✔️ | ✔️ | ✔️ | ✔️ | | clusterInfoSet | ✔️ | ✔️ | ✔️ | ✔️ | | bind_ip | ✔️ | ✔️ | ✔️ | ✔️ | 3. Data Validation # Data validation script # 0: Multi-threaded validation: Configure 1-8 validation methods after 0, which can be processed concurrently # 1: Estimate count validation for libraries and tables (may be inaccurate) # 2: Accurate count validation for libraries and tables # 3: Library and table dbHash validation (locks the library, use with caution) # 4: Validate 100 randomly selected data from libraries and tables, source side randomly selects 100 data, check if they exist on the target side # 5: Validate 100 data of each data type from libraries and tables, extract 100 data of each data type for _id (first 50 and last 50), check if they exist on the target side # 6: Check missing index information in libraries and tables # 7: Check missing index information in libraries and tables and create missing indexes # 8: Library dbHash validation (locks the library, use with caution) # 9: Output detailed validation log information. When not specified, the log only records abnormal validation information # Can be used in combination, e.g., 123456 123457 1237. If not specified, the default is combination 16 checkData=12456 "},"documentDataTransfer/Usecase/FunctionalTest.html":{"url":"documentDataTransfer/Usecase/FunctionalTest.html","title":"FunctionalTest","keywords":"","body":"Full Data Transfer 1. Start Preparation use photon Create ten tables. Each table has no indexes other than _id. Insert approximately 50 million records into each table. 2. Source-side Data Insertion Single record: { \"_id\": ObjectId(\"61bad4f68a27d20b123ed7e8\"), \"BsonTimestamp1\": Timestamp(1639634166, 78), \"String\": \"str\", \"Doc\": { \"1\": 1 }, \"javaInt\": 71916, \"bytes\": BinData(0, \"AQ==\"), \"Array\": [], \"Binary data\": BinData(0, \"AQID\"), \"ObjectId\": ObjectId(\"61bad4f68a27d20b123ed7e6\"), \"Boolean\": false, \"Date\": ISODate(\"2021-12-16T05:56:06.688Z\"), \"Null\": null, \"Regular Expression\": /lhp.*/, \"DBPointer\": DBPointer(\"1\", ObjectId(\"61bad4f68a27d20b123ed7e7\")), \"Undefined\": undefined, \"JavaScript\": { \"code\": \"var i=0\" }, \"Symbol\": \"var i=0\", \"BsonStr\": \"var i=0\", \"BsonJavaScriptWithScope\": { \"code\": \"var i=0\", \"scope\": {} }, \"32integer\": 12, \"Timestamp\": ISODate(\"2021-12-16T05:56:06.688Z\"), \"64int\": NumberLong(123), \"Min key\": { \"$minKey\": 1 }, \"Max key\": { \"$maxKey\": 1 }, \"BsonTimestamp\": Timestamp(1639634166, 457) } 3. Source-side Data Volume Calculation show dbs; Disk usage on source: photon 35.885GB db.stats() { \"db\": \"photon\", \"collections\": 10, \"views\": 0, \"objects\": 474281344, // Estimated total number of records \"avgObjSize\": 132.06465577958498, // Average size per record in bytes \"dataSize\": 57890360946, \"storageSize\": 14807171072, \"freeStorageSize\": 4571136, \"indexes\": 20, \"indexSize\": 23723704320, \"indexFreeStorageSize\": 14454784, \"totalSize\": 38530875392, \"totalFreeStorageSize\": 19025920, \"scaleFactor\": 1, \"fsUsedSize\": 587772825600, \"fsTotalSize\": 11939478503424, \"ok\": 1, \"$clusterTime\": { \"clusterTime\": Timestamp(1640065750, 1), \"signature\": { \"hash\": BinData(0, \"v3ySiE7Zub+VPOJpQ/K3IaCJBxM=\"), \"keyId\": NumberLong(\"7025843880893349893\") } }, \"operationTime\": Timestamp(1640065750, 1) } 4. Start DDT Refer to QuickStart Test environment using the following parameters: # DDT.properties Configuration File # Task name. If not specified, defaults to workNameDefault. workName = mongoTask # Source-side MongoDB URL, required. Can be a URL for a single node, replica set, or sharded cluster. sourceDsUrl = mongodb://192.168.12.200:24578 # sourceDsUrl = mongodb://192.168.12.100:3999 # Target-side MongoDB URL, required. Can be a URL for a single node, replica set, or sharded cluster. targetDsUrl = mongodb://192.168.12.100:24578 # Synchronization mode, default is all. # all: Full data transfer, synchronizes tables while ignoring operations on the source during synchronization. syncMode = all # During full data transfer, choose between sync or reactive. # sync: Stable data transfer. # reactive: Faster data transfer. fullType = reactive # Tables to be synchronized, using regular expressions. Default is to synchronize all tables: .+ dbTableWhite = .+ # Number of threads for reading source data during full data transfer, minimum is 2, maximum is 100. Default is system-calculated value. sourceThreadNum = 10 # Number of threads for writing data to the target during full data transfer, minimum is 4, maximum is 100. Default is system-calculated value. It's recommended that targetThreadNum is three times sourceThreadNum. targetThreadNum = 20 # Number of threads for concurrent index creation during full data transfer, minimum is 1, maximum is 100. Default is system-calculated value. createIndexThreadNum = 15 # The following three parameters, bucketSize, bucketNum, and batchSize, collectively determine the number of data records cached in memory during full data transfer. Be cautious about potential memory overflow. # Default batchSize is 128. batchSize = 128 # Default bucketNum is 20. bucketNum = 20 # Default bucketSize is 20. bucketSize = 20 # Maximum time allowed for each DDL operation during synchronization, in seconds. ddlWait = 1200 # During full data transfer: # Before data transmission, pre-process: synchronize DDL information in the cluster. # 0: Whether to delete existing tables on the target. # 1: Print all user information in the cluster. # 2: Synchronize table structure. # 3: Synchronize table index information. # 4: Enable sharding for all databases. # 5: Synchronize shard key for tables. # 6: Synchronize config.setting table. # 7: Pre-split chunk for tables. # Combine numbers using commas. For example: 1,2,3,4,5,6. clusterInfoSet = 0,1,2,3,4,5,6,7 # When monitor is enabled, configure the IP address of the local machine. bind_ip = 192.168.12.190 Real-time 1. Start DDT Refer to QuickStart Test environment using the following parameters: # DDT.properties Configuration File # Task name. If not specified, defaults to workNameDefault. workName = mongoTask # Source-side MongoDB URL, required. Can be a URL for a single node, replica set, or sharded cluster. sourceDsUrl = mongodb://192.168.12.200:24578 # sourceDsUrl = mongodb://192.168.12.100:3999 # Target-side MongoDB URL, required. Can be a URL for a single node, replica set, or sharded cluster. targetDsUrl = mongodb:// 192.168.12.100:24578 # Synchronization mode, default is all. # realTime: Real-time synchronization. startOplogTime and endOplogTime can be configured. syncMode = realTime # Choose between oplog and changestream for real-time or incremental tasks. # Choose oplog for advantages such as faster synchronization speed when the source is a replica set and support for DDL operations. # Choose changestream for sources that are replica sets or mongos, but it doesn't support DDL operations and is generally slower. realTimeType = changestream # Tables to be synchronized, using regular expressions. Default is to synchronize all tables: .+ dbTableWhite = .+ # In real-time synchronization, you can specify which DDL operations to synchronize: drop, create, createIndexes, dropIndexes, renameCollection, convertToCapped, dropDatabase, modify, shardCollection. # Default is *, which means all DDL operations are synchronized. ddlFilterSet = * # The following three parameters, bucketSize, bucketNum, and batchSize, collectively determine the number of data records cached in memory during real-time synchronization. Be cautious about potential memory overflow. # Default batchSize is 128. batchSize = 128 # Default bucketNum is 20. bucketNum = 20 # Default bucketSize is 20. bucketSize = 20 # When using real-time synchronization, set the start time to read oplog. Default is the 10-digit timestamp when the program starts. startOplogTime = 1692843646 # When using real-time synchronization, set the end time to read oplog. Default is 0, meaning no end time. Use a 10-digit timestamp if needed. endOplogTime = 1692847246 # When using real-time synchronization, set the delay time for reading oplog. Default is 0, meaning no delay time. delayTime = 0 # Number of threads for parsing namespaces (buckets) during real-time synchronization. Minimum is 8, maximum is 100. Default is system-calculated value. nsBucketThreadNum = 15 # Number of threads for writing data during real-time synchronization. Minimum is 8, maximum is 100. Default is system-calculated value. writeThreadNum = 15 # Maximum time allowed for each DDL operation during synchronization, in seconds. ddlWait = 1200 # When monitor is enabled, configure the IP address of the local machine. bind_ip = 192.168.12.190 2. Source-side Data Insertion Use the source-side script for CRUD operations. The script performs CRUD operations on 10 tables. Single insert data model: { \"_id\": ObjectId(\"61bad4f68a27d20b123ed7e8\"), \"BsonTimestamp1\": Timestamp(1639634166, 78), \"String\": \"str\", \"Doc\": { \"1\": 1 }, \"javaInt\": 71916, \"bytes\": BinData(0, \"AQ==\"), \"Array\": [], \"Binary data\": BinData(0, \"AQID\"), \"ObjectId\": ObjectId(\"61bad4f68a27d20b123ed7e6\"), \"Boolean\": false, \"Date\": ISODate(\"2021-12-16T05:56:06.688Z\"), \"Null\": null, \"Regular Expression\": /lhp.*/, \"DBPointer\": DBPointer(\"1\", ObjectId(\"61bad4f68a27d20b123ed7e7\")), \"Undefined\": undefined, \"JavaScript\": { \"code\": \"var i=0\" }, \"Symbol\": \"var i=0\", \"BsonStr\": \"var i=0\", \"BsonJavaScriptWithScope\": { \"code\": \"var i=0\", \"scope\": {} }, \"32integer\": 12, \"Timestamp\": ISODate(\"2021-12-16T05:56:06.688Z\"), \"64int\": NumberLong(123), \"Min key\": { \"$minKey\": 1 }, \"Max key\": { \"$maxKey\": 1 }, \"BsonTimestamp\": Timestamp(1639634166, 457) } Source-side CRUD concurrency is 100,000/s. 3. Conclusion During real-time synchronization, the source-side CRUD concurrency is 100,000/s. The target-side executes an average of 58,000 data records per second. When the data volume of source-side CRUD operations is large, it may cause DDT to be unable to synchronize the source oplog in a timely manner. Observe the 'Reading oplog delay xxxs' data and avoid missing the sliding window time for reading oplog. "},"documentDataTransfer/Usecase/FullTesting.html":{"url":"documentDataTransfer/Usecase/FullTesting.html","title":"FullTesting","keywords":"","body":"DDT Full-scale Testing Test Environment Hardware Resources Configuration: CPU: 40 cores, Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz Memory: 4*32GB Network Card: 1Gbps Operating System: Linux x86_64 MongoDB Version: 0.1 Disk: SSD Test Conditions The test data covers the following dimensions: Latency, QPS (Queries Per Second), CPU Usage, Memory Usage. All values are provided as the average over 10 seconds. QPS is derived from log outputs on the data platform, with OPLOG write counts per second recorded. CPU and memory usage information is also provided. Test Results When cacheBucketSize=32, cacheBucketNum=32, dataBatchSize=128: Test One Configuration Information Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize 30GBTarget MongoDB: Single-node replica set, cacheSize 30GB Data Volume One database with 10 collections, each document has 7 columns, total size of each OPLOG document is around 140 bytes Source Read Threads 5 Target Write Threads 15 Cache Settings cacheBucketSize=32cacheBucketNum=32dataBatchSize=128 Test Results: Measurement Description QPS 145062 CPU Usage 400% Memory Usage 13631MB Test Two Configuration Information Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize 30GBTarget MongoDB: Single-node replica set, cacheSize 30GB Data Volume One database with 10 collections, each document has 7 columns, total size of each document is around 140 bytes Source Read Threads 6 Target Write Threads 20 Cache Settings cacheBucketSize=32cacheBucketNum=32dataBatchSize=128 Test Results Measurement Description QPS 160837 CPU Usage 440% Memory Usage 16384MB Test Three Configuration Information Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize 30GBTarget MongoDB: Single-node replica set, cacheSize 30GB Data Volume One database with 10 collections, each document has 7 columns, total size of each document is around 140 bytes Source Read Threads 6 Target Write Threads 24 Cache Settings cacheBucketSize=32cacheBucketNum=32dataBatchSize=128 Test Results Description Measurement QPS 155232 CPU Usage 440% Memory Usage 15860MB Summary Cache Settings cacheSize Source Read Threads Target Write Threads QPS Memory Usage CPU Usage cacheBucketSize=32 cacheBucketNum=32 dataBatchSize=128 30GB 5 15 145062 13631MB 400% 6 20 160837 16384MB 440% 6 24 155232 15860MB 440% Summary: When cacheBucketSize=32, cacheBucketNum=32, and dataBatchSize=128, it can be observed that increasing the number of threads: (1) Does not improve QPS, as the read volume is lower than the write volume; (2) Does not increase memory usage significantly due to cache size limitations. When cacheBucketSize=48, cacheBucketNum=48, and dataBatchSize=128: Test One Configuration Information Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize 30GB; Target MongoDB: Single-node replica set, cacheSize 30GB Data Volume One database with 10 collections, each document has 7 columns, total size of each document is around 140 bytes Source Read Threads 5 Target Write Threads 15 Cache Settings cacheBucketSize=48 cacheBucketNum=48 dataBatchSize=128 Test Results Measurement Description QPS 315702 CPU Usage 400% Memory Usage 31326MB Test Two Configuration Information Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize 30GB; Target MongoDB: Single-node replica set, cacheSize 30GB Data Volume One database with 10 collections, each document has 7 columns, total size of each document is around 140 bytes Source Read Threads 6 Target Write Threads 20 Cache Settings cacheBucketSize=48 cacheBucketNum=48 dataBatchSize=128 Test Results Measurement Description QPS 340716 CPU Usage 800% Memory Usage 24773MB Test Three Configuration Information Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize 30GB; Target MongoDB: Single-node replica set, cacheSize 30GB Data Volume One database with 10 collections, each document has 7 columns, total size of each document is around 140 bytes Source Read Threads 6 Target Write Threads 24 Cache Settings cacheBucketSize=48 cacheBucketNum=48 dataBatchSize=128 Test Results Measurement Description QPS 367178 CPU Usage 880% Memory Usage 23986MB Test Four Configuration Information Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize 30GB; Target MongoDB: Single-node replica set, cacheSize 30GB Data Volume One database with 10 collections, each document has 7 columns, total size of each document is around 140 bytes Source Read Threads 8 Target Write Threads 24 Cache Settings cacheBucketSize=48 cacheBucketNum=48 dataBatchSize=128 Test Results Measurement Description QPS 371528 CPU Usage 1120% Memory Usage 27132MB Summary Cache Settings cacheSize Source Read Threads Target Write Threads QPS Memory Usage CPU Usage cacheBucketSize=48 cacheBucketNum=48 dataBatchSize=128 30GB 5 15 315702 31326MB 400% 6 20 340716 24773MB 800% 6 24 367178 23986MB 880% 8 24 371528 27132MB 1120% Summary: When cacheBucketSize=48, cacheBucketNum=48, and dataBatchSize=128, it can be observed that increasing the number of threads: (1) Increases QPS. When cacheBucketSize=64, cacheBucketNum=64, and dataBatchSize=128: Test One Configuration Information Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize 30GB; Target MongoDB: Single-node replica set, cacheSize 30GB Data Volume One database with 10 collections, each document has 7 columns, total size of each document is around 140 bytes Source Read Threads 5 Target Write Threads 15 Cache Settings cacheBucketSize=64 cacheBucketNum=64 dataBatchSize=128 Test Results Measurement Description QPS 370042 CPU Usage 812% Memory Usage 25159MB Test Two Configuration Information Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize 30GB; Target MongoDB: Single-node replica set, cacheSize 30GB Data Volume One database with 10 collections, each document has 7 columns, total size of each document is around 140 bytes Source Read Threads 6 Target Write Threads 20 Cache Settings cacheBucketSize=64 cacheBucketNum=64 dataBatchSize=128 Test Results Measurement Description QPS 390000 CPU Usage 1080% Memory Usage 26522MB Test Three Configuration Information Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize 30GB; Target MongoDB: Single-node replica set, cacheSize 30GB Data Volume One database with 10 collections, each document has 7 columns, total size of each document is around 140 bytes Source Read Threads 6 Target Write Threads 24 Cache Settings cacheBucketSize=64 cacheBucketNum=64 dataBatchSize=128 Test Results Measurement Description QPS 400138 CPU Usage 1160% Memory Usage 26655MB Test Four Configuration Information Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize 30GB; Target MongoDB: Single-node replica set, cacheSize 30GB Data Volume One database with 10 collections, each document has 7 columns, total size of each document is around 140 bytes Source Read Threads 8 Target Write Threads 24 Cache Settings cacheBucketSize=64 cacheBucketNum=64 dataBatchSize=128 Test Results Measurement Description QPS 360209 CPU Usage 1120% Memory Usage 25252MB Summary: Cache Settings cacheSize Source Read Threads Target Write Threads QPS Memory Usage CPU Usage cacheBucketSize=64 cacheBucketNum=64 dataBatchSize=128 30GB 5 15 370042 25159MB 812% 6 20 390000 26522MB 1080% 6 24 400138 26655MB 1160% 8 24 360209 25252MB 1120% Summary: (1) CPU and QPS: (2) Memory Usage and QPS: "},"documentDataTransfer/Usecase/RealTimeTest.html":{"url":"documentDataTransfer/Usecase/RealTimeTest.html","title":"RealTimeTest","keywords":"","body":"DDT Real-time Testing Test Environment Hardware Configuration: CPU: 40 cores, Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz Memory: 4*32GB Network Card: 1Gbps Operating System: Linux x86_64 MongoDB Version: 0.1 Disk: SSD Test Conditions The test data covers the following dimensions: Latency, QPS (Queries Per Second), CPU Usage, Memory Usage. All values are given as the average over a 10-second period. QPS is obtained from the data platform's log output, which counts the number of OPLOG writes per second. We also provide CPU and memory usage information. Test Results When cacheBucketSize=16, cacheBucketNum=16, dataBatchSize=128: Test 1 Configuration Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize30GB Target MongoDB: Single-node replica set, cacheSize30GB Data Volume One database with 10 collections, each document contains 7 columns, and the total size of each document is approximately 140 bytes Real-time Sync Threads {oplogNS=1, oplogWrite=6, oplogRead=1, oplogNsBucket=2} Cache Area cacheBucketSize=16, cacheBucketNum=16, dataBatchSize=128 Test Results Measurement Description QPS 72398 CPU Usage 280% Memory Usage 8258MB Test 2 Configuration Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize30GB Target MongoDB: Single-node replica set, cacheSize30GB Data Volume One database with 10 collections, each document contains 7 columns, and the total size of each document is approximately 140 bytes Real-time Sync Threads {oplogNS=1, oplogWrite=9, oplogRead=1, oplogNsBucket=3} Cache Area cacheBucketSize=16, cacheBucketNum=16, dataBatchSize=128 Test Results Measurement Description QPS 80385 CPU Usage 240% Memory Usage 14418MB Test 3 Configuration Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize30GB Target MongoDB: Single-node replica set, cacheSize30GB Data Volume One database with 10 collections, each document contains 7 columns, and the total size of each document is approximately 140 bytes Real-time Sync Threads {oplogNS=1, oplogWrite=12, oplogRead=1, oplogNsBucket=4} Cache Area cacheBucketSize=16, cacheBucketNum=16, dataBatchSize=128 Test Results Measurement Description QPS 79365 CPU Usage 280% Memory Usage 15728MB Test 4 Configuration Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize30GB Target MongoDB: Single-node replica set, cacheSize30GB Data Volume One database with 10 collections, each document contains 7 columns, and the total size of each document is approximately 140 bytes Real-time Sync Threads {oplogNS=1, oplogWrite=15, oplogRead=1, oplogNsBucket=5} Cache Area cacheBucketSize=16, cacheBucketNum=16, dataBatchSize=128 Test Results Measurement Description QPS 75388 CPU Usage 280% Memory Usage 14025MB Summary Cache Area oplogNS oplogWrite oplogRead oplogNsBucket QPS CPU Usage Memory Usage cacheBucketSize=16 cacheBucketNum=16 dataBatchSize=128 1 6 1 2 72398 280% 8258MB 1 9 1 3 80385 240% 14418MB 1 12 1 4 79365 280% 15728MB 1 15 1 5 75388 280% 14025MB Summary: When cacheBucketSize=16, cacheBucketNum=16, dataBatchSize=128, it can be observed that increasing the number of threads does not increase QPS, due to the limitation of the cache area size. When cacheBucketSize=32, cacheBucketNum=32, dataBatchSize=128: Test 1 Configuration Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize30GB Target MongoDB: Single-node replica set, cacheSize30GB Data Volume One database with 10 collections, each document contains 7 columns, and the total size of each document is approximately 140 bytes | | Real-time Sync Threads | {oplogNS=1, oplogWrite=6, oplogRead=1, oplogNsBucket=2} | | Cache Area | cacheBucketSize=32, cacheBucketNum=32, dataBatchSize=128 | Test Results Measurement Description QPS 87719 CPU Usage 240% Memory Usage 13107MB Test 2 Configuration Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize30GB Target MongoDB: Single-node replica set, cacheSize30GB Data Volume One database with 10 collections, each document contains 7 columns, and the total size of each document is approximately 140 bytes Real-time Sync Threads {oplogNS=1, oplogWrite=9, oplogRead=1, oplogNsBucket=3} Cache Area cacheBucketSize=32, cacheBucketNum=32, dataBatchSize=128 Test Results Measurement Description QPS 100000 CPU Usage 320% Memory Usage 11534MB Test 3 Configuration Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize30GB Target MongoDB: Single-node replica set, cacheSize30GB Data Volume One database with 10 collections, each document contains 7 columns, and the total size of each document is approximately 140 bytes Real-time Sync Threads {oplogNS=1, oplogWrite=12, oplogRead=1, oplogNsBucket=4} Cache Area cacheBucketSize=32, cacheBucketNum=32, dataBatchSize=128 Test Results Measurement Description QPS 112370 CPU Usage 320% Memory Usage 11796MB Test 4 Configuration Parameter Description MongoDB Type Source MongoDB: Single-node replica set, cacheSize30GB Target MongoDB: Single-node replica set, cacheSize30GB Data Volume One database with 10 collections, each document contains 7 columns, and the total size of each document is approximately 140 bytes Real-time Sync Threads {oplogNS=1, oplogWrite=15, oplogRead=1, oplogNsBucket=5} Cache Area cacheBucketSize=32, cacheBucketNum=32, dataBatchSize=128 Test Results Measurement Description QPS 120030 CPU Usage 360% Memory Usage 12845MB Summary Cache Area oplogNS oplogWrite oplogRead oplogNsBucket QPS CPU Usage Memory Usage cacheBucketSize=32 cacheBucketNum=32 dataBatchSize=128 1 6 1 2 87719 240% 13107MB 1 9 1 3 100000 320% 11534MB 1 12 1 4 112370 320% 11796MB 1 15 1 5 120030 360% 12845MB Summary: When cacheBucketSize=32, cacheBucketNum=32, dataBatchSize=128, it can be observed that increasing the number of threads increases QPS, due to the limitation of the Oplog read rate. Conclusion: (1) CPU vs. QPS: (2) Memory Usage vs. QPS: Make the necessary translation adjustments and ensure that the formatting and image paths are not modified. "}}